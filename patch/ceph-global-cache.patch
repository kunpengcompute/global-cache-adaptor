diff --git a/src/CMakeLists.txt b/src/CMakeLists.txt
index 28ec983..bf4aa6e 100644
--- a/src/CMakeLists.txt
+++ b/src/CMakeLists.txt
@@ -103,6 +103,11 @@ if(HAVE_INTEL)
     HAVE_BETTER_YASM_ELF64)
 endif()
 
+if(WITH_GLOBAL_CACHE)
+set(third_part_dir /opt/gcache_adaptor_compile/third_part)
+message(STATUS "third part directory --> " ${third_part_dir})
+link_directories(${third_part_dir}/lib)
+endif()
 
 # require c++17
 if(CMAKE_VERSION VERSION_LESS "3.8")
@@ -393,6 +398,10 @@ if(WITH_DPDK)
   list(APPEND ceph_common_deps common_async_dpdk)
 endif()
 
+if(WITH_GLOBAL_CACHE)
+  list(APPEND ceph_common_deps ceph_client_adaptor_plugin)
+endif()
+
 add_library(common STATIC ${ceph_common_objs})
 target_link_libraries(common ${ceph_common_deps})
 
@@ -560,6 +569,12 @@ add_subdirectory(dmclock)
 
 add_subdirectory(compressor)
 
+# Client adaptor
+if(WITH_GLOBAL_CACHE)
+message(STATUS "Client adaptor cmake executing...")
+add_subdirectory(client_adaptor)
+endif()
+
 add_subdirectory(tools)
 
 if(WITH_TESTS)
diff --git a/src/client_adaptor/CMakeLists.txt b/src/client_adaptor/CMakeLists.txt
new file mode 100644
index 0000000..aba70f7
--- /dev/null
+++ b/src/client_adaptor/CMakeLists.txt
@@ -0,0 +1,17 @@
+set(client_adaptor_srcs
+  ClientAdaptorMsg.cc
+  ClientAdaptorMgr.cc
+  ClientAdaptorPerf.cc
+  ClientAdaptorPlugin.cc
+)
+
+add_library(ceph_client_adaptor_plugin SHARED ${client_adaptor_srcs})
+message(STATUS "In cliend adaptor third part directory --> " ${third_part_dir})
+target_link_libraries(ceph_client_adaptor_plugin osdc agent_client_lib das)
+
+
+set(client_adaptor_dir ${CEPH_INSTALL_PKGLIBDIR})
+install(TARGETS ceph_client_adaptor_plugin DESTINATION ${client_adaptor_dir})
+
+message(STATUS "Global Cache client-adaptor cmake executing...")
+
diff --git a/src/client_adaptor/ClientAdaptorMgr.cc b/src/client_adaptor/ClientAdaptorMgr.cc
new file mode 100644
index 0000000..900be66
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorMgr.cc
@@ -0,0 +1,256 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include <iostream>
+#include "ClientAdaptorMgr.h"
+
+void ClientAdaptorMgr::set_init_flag(bool flag){
+    init_flag = flag;
+    return;
+}
+
+const bool ClientAdaptorMgr::is_init_succeed(){
+    if(init_flag){
+        return true;
+    } else {
+        return false;
+    }
+}
+
+int32_t CcmPtChangeNotify(PTViewPtEntry *entry, uint32_t entryNum, void *ctx)
+{
+    if (entryNum == 0) {
+        return RET_OK;
+    }
+    std::vector<uint32_t> normal_pt;
+    for (uint32_t i = 0; i < entryNum; i++) {
+        if (entry[i].state == CCM_PT_STATE_OK) {
+            normal_pt.push_back(entry[i].ptId);
+        }
+    }
+    if (normal_pt.size() == 0) {
+        return RET_OK;
+    }
+    Objecter *obj = static_cast<Objecter*>(ctx);
+    obj->retry_op_submit(normal_pt);
+    return RET_OK;
+}
+
+int32_t CcmNodeChangeNotify(int32_t clusterId, NodeInfo *nodeList, uint32_t nodeNum, void *ctx) {
+    if (nodeNum == 0) {
+        return RET_OK;
+    }
+    std::set<uint32_t> available_nodes;
+    for (uint32_t i = 0; i < nodeNum; i++) {
+        if (nodeList[i].state == NODE_STATE_UP) {
+            available_nodes.insert(nodeList[i].nodeId);
+        }
+    }
+    if (nodeNum - available_nodes.size() == 0) {
+        return RET_OK;
+    }
+    Objecter *obj = static_cast<Objecter *>(ctx);
+    obj->nodeview_change_retry_op_submit(clusterId, available_nodes);
+    return RET_OK;
+}
+
+void ClientAdaptorCcm::ccm_deregister(Objecter *obj)
+{
+    if (register_objs.count(obj) > 0) {
+        OpenDeregisterViewChangeNotifyChain(register_objs[obj]);
+        OpenDeregisterNodeViewChangeNotifyChain(register_node_change_objs[obj]);
+        delete register_objs[obj];
+        delete register_node_change_objs[obj];
+        register_objs.erase(obj);
+        register_node_change_objs.erase(obj);
+    }
+    if (register_objs.empty() && is_init_succeed()) {
+        set_init_flag(false);
+    }
+}
+
+bool ClientAdaptorCcm::ccm_callback_register(Objecter *obj)
+{
+    PTViewChangeOpHandle *ccmCallback = new PTViewChangeOpHandle();
+    NodeViewChangeOpHandle *ccmNodeChangeCallback = new NodeViewChangeOpHandle();
+    register_objs[obj] = ccmCallback;
+    register_node_change_objs[obj] = ccmNodeChangeCallback;
+    ccmCallback->notifyPtChange = CcmPtChangeNotify;
+    ccmNodeChangeCallback->notifyNodeChange = CcmNodeChangeNotify;
+    ccmCallback->ctx = (void *)obj;
+    ccmNodeChangeCallback->ctx = (void *)obj;
+    if (OpenRegisterViewChangeNotifyChain(ccmCallback) || OpenRegisterNodeViewChangeNotifyChain(ccmNodeChangeCallback)) {
+        std::cout << __func__ << " Client Adaptor: CCM agent register failed" << std::endl;
+        return false;
+    }
+    return true;
+}
+
+/*
+ * Init manager ,  failed to return !0
+*/
+int32_t ClientAdaptorCcm::init_mgr(Objecter *obj){
+    int32_t ret = 0;
+    //Already init
+    if (is_init_succeed()){
+        if (!ccm_callback_register(obj)) {
+            return RET_CCM_REGISTER_ERROR;
+        }
+        return RET_OK;
+    }
+
+    ret = OpenBcmInit();
+    if (ret != 0) {
+        std::cout << __func__ << " ccm agint init failed, ret=" << ret << std::endl;
+	    return RET_CCM_AGENT_INIT_ERROR;
+    }
+
+    if (!ccm_callback_register(obj)){
+    	return RET_CCM_REGISTER_ERROR;
+    }
+    return RET_OK;
+}
+
+int32_t ClientAdaptorCcm::get_pt_num(int32_t clusterId, uint32_t& num){
+    num = OpenGetTotalPtNum(clusterId);
+    return RET_OK;
+}
+
+int32_t ClientAdaptorCcm::get_pt_entry(int32_t clusterId, uint32_t pt_index, PTViewPtEntry* entry){
+    if (OpenGetPtEntry(clusterId, pt_index, entry)){
+        std::cout << __func__ << " Client Adaptor: Get PT entry failed" << std::endl;
+	    return RET_CCM_PT_ENTRY_ERROR;
+    }
+    return RET_OK;
+}
+
+int32_t ClientAdaptorCcm::get_node_info(int32_t clusterId, uint32_t node_id, NodeInfo* node_info){
+    using namespace std::chrono;
+    int32_t ret = 0;
+    seconds timeout {50 * 60};
+
+    steady_clock::time_point begin = steady_clock::now();
+    while (true) {
+        ret = OpenAgentGetNodeInfo(clusterId, node_id, node_info);
+        if (ret == 0) {
+            break;
+        }
+
+        steady_clock::time_point end = steady_clock::now();
+        seconds time_span = duration_cast<seconds>(end - begin);
+
+        if (time_span > timeout) {
+            std::cout << __func__ << " Client Adaptor: get node info failed because of timeout"
+                                  << ", timeout=" << timeout << std::endl;
+            break;
+        }
+        sleep(1);
+    }
+    if (ret){
+        std::cout << __func__ << " Client Adaptor: Get node infomation failed" << std::endl;
+        return RET_CCM_NODE_INFO_ERROR;
+    }
+
+    return RET_OK;
+}
+
+bool ClientAdaptorCcm::get_pt_status(int32_t clusterId, uint32_t pt_id) {
+    bool ret = true;
+    PTViewPtEntry pt_entry = { 0 };
+    if (get_pt_entry(clusterId, pt_id, &pt_entry)) {
+        std::cout << __func__ << " Client Adaptor: Get PT entry failed" << std::endl;
+        return false;
+    }
+
+    if (pt_entry.state != CCM_PT_STATE_OK) {
+        return false;
+    }
+    return ret;
+}
+
+int32_t ClientAdaptorCcm::add_snap_to_gc(int64_t md_pool_id,
+                                         int64_t data_pool_id,
+                                         const std::string &image_id,
+                                         uint64_t snap_id) {
+    int32_t ret = OpenCreateSnapshot(md_pool_id, data_pool_id, image_id.c_str(), snap_id);
+    if (ret < 0) {
+        std::cout << __func__ << " Client Adaptor: Add to gc failed, snap_id=" << snap_id
+                 << " ret="<< ret << std::endl;
+        return ret;
+    }
+    return 0;
+}
+
+int32_t ClientAdaptorCcm::remove_snap_from_gc(int64_t data_pool_id,
+                                              const std::string &name_space,
+                                              const std::string &image_id,
+                                              uint64_t snap_id) {
+    int32_t ret = OpenDeleteSnapshot(data_pool_id, name_space.c_str(), image_id.c_str(), snap_id);
+    if (ret < 0) {
+        std::cout << __func__ << " Client Adaptor: Remove from gc failed, snap_name=" << data_pool_id << "/" << image_id
+                 << "@" << snap_id << " ret="<< ret << std::endl;
+        return ret;
+    }
+    return 0;
+}
+
+int32_t ClientAdaptorCcm::remove_gc_image_resource(int64_t data_pool_id, const std::string image_id) {
+    int32_t ret = OpenReleaseImageResource(data_pool_id, image_id.c_str());
+    if (ret < 0) {
+        std::cout << __func__ << " Client Adaptor: Remove image resource failed, image_id=" << image_id
+                 << " ret="<< ret << std::endl;
+        return ret;
+    }
+    return 0;
+}
+
+int32_t ClientAdaptorCcm::get_node_from_ma(const int64_t pool_id, int32_t *nodeId) {
+    return OpenAgentInit(pool_id, nodeId);
+}
+
+int32_t ClientAdaptorCcm::rollback_gc_snap(int64_t md_pool_id, int64_t data_pool_id,
+                                         const std::string image_id, uint64_t num_objs,
+                                         uint64_t snap_seq, uint64_t rb_snap_id,
+                                         uint64_t tp_snap_id1, uint64_t tp_snap_id2) {
+    RollbackInfo info;
+    info.mdPoolId = md_pool_id;
+    info.dataPoolId = data_pool_id;
+    info.imageId = image_id.c_str();
+    info.numObjs = num_objs;
+    info.snapId = rb_snap_id;
+    info.oldHeadSnapId = tp_snap_id1;
+    info.rollbackSnapId = tp_snap_id2;
+    info.snapSeq = snap_seq;
+    int ret = OpenRollbackSnapshot(&info);
+    if (ret < 0) {
+        std::cout << __func__ << "Client Adaptor: register rollback list failed, snap="
+                            << md_pool_id << "-" << data_pool_id << "/" << image_id
+                            << "@" << rb_snap_id << " ret="<< ret << std::endl;
+        return ret;
+    }
+    return 0;
+}
+
+int32_t ClientAdaptorCcm::gc_is_rollbacking(int64_t md_pool_id, int64_t data_pool_id, const std::string image_id)
+{
+    int ret = OpenImageBusy(data_pool_id, image_id.c_str());
+    if (ret < 0) {
+        std::cout << __func__ << "Client Adaptor: query gc rollbacking failed, image=" << md_pool_id
+                            << "(" << data_pool_id << ")/" <<image_id << std::endl;
+    }
+    return ret;
+}
+
+int32_t ClientAdaptorCcm::gc_snap_is_rollbacking(int64_t data_pool_id, const std::string image_id,
+                                                int64_t snap_id)
+{
+    int ret = OpenSnapshotBusy(data_pool_id, image_id.c_str(), snap_id);
+    if (ret < 0) {
+        std::cout << __func__ << "Client Adaptor: query gc snap rollbacking failed, image=" << data_pool_id
+                            << "/" <<image_id << "@" << snap_id << ", ret=" << ret << std::endl;
+    }
+    return ret;
+}
diff --git a/src/client_adaptor/ClientAdaptorMgr.h b/src/client_adaptor/ClientAdaptorMgr.h
new file mode 100644
index 0000000..7ea053f
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorMgr.h
@@ -0,0 +1,189 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#ifndef CLIENT_ADAPTOR_MGR_H
+#define CLIENT_ADAPTOR_MGR_H
+
+#include <string>
+#include <set>
+#include <osdc/Objecter.h>
+extern "C"
+{
+#include "open_ccm.h"
+}
+
+enum {
+  RET_OK = 0,
+  RET_PARAM_ERROR,
+  RET_CCM_PT_NUM_ERROR,
+  RET_CCM_PT_ENTRY_ERROR,
+  RET_CCM_NODE_INFO_ERROR,
+  RET_CCM_AGENT_INIT_ERROR,
+  RET_CONF_PARSER_ERROR,
+  RET_CCM_PORT_NUM_ERROR,
+  RET_CCM_IP_ERROR,
+  RET_CCM_REGISTER_ERROR,
+  RET_CCM_GET_NODE_ERROR,
+  RET_CCM_PARAM_ERROR
+};
+
+class ClientAdaptorMgr {
+public:
+  ClientAdaptorMgr(){}
+  virtual ~ClientAdaptorMgr(){}
+
+  virtual int32_t init_mgr(Objecter *obj) = 0;
+
+  virtual int32_t get_pt_num(int32_t clusterId, uint32_t& num) = 0;
+
+  virtual int32_t get_pt_entry(int32_t clusterId, uint32_t pt_index, PTViewPtEntry* entry) = 0;
+
+  virtual int32_t get_node_info(int32_t clusterId, uint32_t node_id, NodeInfo* node_info) = 0;
+
+  virtual int32_t add_snap_to_gc(int64_t md_pool_id, int64_t data_pool_id, const std::string &image_id,
+                                uint64_t snap_id) = 0;
+
+  virtual int32_t remove_snap_from_gc(int64_t data_pool_id, const std::string &name_space,
+                                    const std::string &image_id, uint64_t snap_id) = 0;
+
+  virtual int32_t remove_gc_image_resource(const int64_t pool_id, const std::string image_id) = 0;
+
+  virtual int32_t get_node_from_ma(const int64_t pool_id, int32_t *nodeId) = 0;
+
+  virtual int32_t rollback_gc_snap(int64_t pool_id, int64_t data_pool_id, const std::string image_id,
+                                    uint64_t num_objs, uint64_t snap_seq,
+                                    uint64_t rb_snap_id, uint64_t tp_snap_id1, uint64_t tp_snap_id2) = 0;
+
+  virtual int32_t gc_is_rollbacking(int64_t md_pool_id,
+                                    int64_t data_pool_id, const std::string image_id) = 0;
+
+  virtual int32_t gc_snap_is_rollbacking(int64_t data_pool_id, const std::string image_id, int64_t snap_id) = 0;
+
+  virtual const std::string name(){
+    return "ClientAdaptorMgr";
+  }
+
+  const bool is_init_succeed();
+
+  void set_init_flag(bool flag);
+
+  virtual bool get_pt_status(int32_t clusterId, uint32_t pt_id) = 0;
+  virtual void ccm_deregister(Objecter *obj) = 0;
+private:
+  bool init_flag = false;
+};
+
+class ClientAdaptorCcm : public ClientAdaptorMgr {
+public:
+  ClientAdaptorCcm(){}
+  ~ClientAdaptorCcm() override {}
+
+  int32_t init_mgr(Objecter *obj);  
+
+  int32_t get_pt_num(int32_t clusterId, uint32_t& num);
+
+  int32_t get_pt_entry(int32_t clusterId, uint32_t pt_index, PTViewPtEntry* entry);
+
+  int32_t get_node_info(int32_t clusterId, uint32_t node_id, NodeInfo* node_info);
+
+  int32_t add_snap_to_gc(int64_t md_pool_id, int64_t data_pool_id, const std::string &image_id, uint64_t snap_id);
+
+  int32_t remove_snap_from_gc(int64_t data_pool_id, const std::string &name_space,
+                            const std::string &image_id, uint64_t snap_id);
+
+  int32_t remove_gc_image_resource(const int64_t pool_id, const std::string image_id);
+
+  int32_t get_node_from_ma(const int64_t pool_id, int32_t *nodeId);
+
+  int32_t rollback_gc_snap(int64_t pool_id, int64_t data_pool_id,
+                            const std::string image_id, uint64_t num_objs, uint64_t snap_seq,
+                            uint64_t rb_snap_id, uint64_t tp_snap_id1, uint64_t tp_snap_id2);
+
+  int32_t gc_is_rollbacking(int64_t md_pool_id, int64_t data_pool_id, const std::string image_id);
+
+  int32_t gc_snap_is_rollbacking(int64_t data_pool_id, const std::string image_id, int64_t snap_id);
+
+  const std::string name() override {
+    return "ClientAdaptorCcm";
+  }
+
+  bool get_pt_status(int32_t clusterId, uint32_t pt_id);
+  void ccm_deregister(Objecter *obj);
+
+private:
+  std::map<Objecter*, PTViewChangeOpHandle* > register_objs;
+  std::map<Objecter*, NodeViewChangeOpHandle* > register_node_change_objs;
+  bool ccm_callback_register(Objecter *obj);
+}; 
+
+class ClientAdaptorLocal : public ClientAdaptorMgr {
+public:
+  ClientAdaptorLocal(){}
+  ~ClientAdaptorLocal() override {}
+ 
+  int32_t init_mgr(Objecter *obj){
+    return 0;
+  }  
+  
+  int32_t get_pt_num(int32_t clusterId, uint32_t& num){
+    num = 10;
+    return 0;
+  }
+  
+  int32_t get_pt_entry(int32_t clusterId, uint32_t pt_index, PTViewPtEntry* entry){
+    entry->curNodeInfo.nodeId = pt_index % 3;
+    return 0;
+  }
+
+  int32_t get_node_info(int32_t clusterId, uint32_t node_id, NodeInfo* node_info){
+    strcpy(node_info->publicAddrStr, "localhost");
+    node_info->ports[0] = 1234;
+    node_info->portNum = 1;
+    return 0;
+  }
+
+  int32_t add_snap_to_gc(int64_t md_pool_id, int64_t data_pool_id, const std::string &image_id, uint64_t snap_id) {
+    return 0;
+  }
+
+  int32_t remove_snap_from_gc(int64_t data_pool_id, const std::string &name_space,
+                            const std::string &image_id, uint64_t snap_id)  {
+    return 0;
+  }
+
+  int32_t remove_gc_image_resource(const int64_t pool_id, const std::string image_id)  {
+    return 0;
+  }
+  int32_t get_node_from_ma(const int64_t pool_id, int32_t *nodeId) {
+    return 0;
+  }
+  int32_t rollback_gc_snap(int64_t pool_id, int64_t data_pool_id,
+                            const std::string image_id, uint64_t num_objs, uint64_t snap_seq,
+                            uint64_t rb_snap_id, uint64_t tp_snap_id1, uint64_t tp_snap_id2) {
+      return 0;
+  }
+
+  int32_t gc_is_rollbacking(int64_t md_pool_id, int64_t data_pool_id, const std::string image_id) {
+      return 0;
+  }
+
+  int32_t gc_snap_is_rollbacking(int64_t data_pool_id, const std::string image_id, int64_t snap_id) {
+      return 0;
+  }
+
+  const std::string name() override {
+    return "ClientAdaptorLocal";
+  }
+  bool get_pt_status(int32_t clusterId, uint32_t pt_id) {
+    return true;
+  }
+  void ccm_deregister(Objecter *obj) {}
+
+private:
+};
+
+
+#endif
diff --git a/src/client_adaptor/ClientAdaptorMsg.cc b/src/client_adaptor/ClientAdaptorMsg.cc
new file mode 100644
index 0000000..ee19bbd
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorMsg.cc
@@ -0,0 +1,348 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include <iostream>
+#include <regex>
+#include <bits/stdc++.h>
+#include "ClientAdaptorMsg.h"
+#include "ClientAdaptorMgr.h"
+
+namespace {
+const int RBD_DATA_OBJECT_NAME_FILTER_LEN = 8;  // rbd_data(8 bits)
+const int RBD_DATA_OBJECT_NAME_LEN = 27;   // rbd_data(8 bits).image_id(n bits).object_index(16 bits)
+const string RBD_DATA_OBJECT_NAME = "rbd_data";
+const int RGW_BUCKET_ID_LEN = 36;    // bucket_id(36 bits)
+const int RGW_OBJECT_NAME_LEN = 38;      // bucket_id(36 bits)_Object_Name(n bits)
+const uint64_t SEGMENT_SIZE = 4194304;
+const uint64_t SEGMENT_MASK = 0x3FFFFF;
+const int OBJECT_ID_LEN = 16;
+const int GC_PORT_MIN = 7880;
+const int GC_PORT_MAX = 7889;
+const string GC_SNAP_PREFIX = "gc_";
+}
+
+ClientAdaptorMsg::ClientAdaptorMsg(ClientAdaptorMgr* mgr) : mgr_ref(mgr){
+}
+
+void ClientAdaptorMsg::push_strategy(Objecter *objecter, uint64_t pool_id, int32_t node_id, std::string oid_name, bufferlist &indata)
+{
+    if (objecter == NULL) {
+        std::cout << __func__ << " Objecter null " << std::endl;
+        return;
+    }
+    if (oid_name.size() == 0) {
+        std::cout << __func__ << " oid_name size 0 " << std::endl;
+        return;
+    }
+    if (node_id < 0) {
+        std::cout << __func__ << " node id < 0 " << std::endl;
+        return;
+    }
+    const char *cls = "rpc";
+    const char *method = "das_prefetch";
+    vector<OSDOp> nops(1);
+    OSDOp &op = nops[0];
+    op.op.op = CEPH_OSD_OP_CALL;
+    op.op.cls.class_len = strlen(cls);
+    op.op.cls.method_len = strlen(method);
+    op.op.cls.indata_len = indata.length();
+    op.indata.append(cls, op.op.cls.class_len);
+    op.indata.append(method, op.op.cls.method_len);
+    op.indata.append(indata);
+    Objecter::Op *objecter_op = 
+	    new Objecter::Op(object_t(oid_name), object_locator_t(), nops, CEPH_OSD_FLAG_EXEC, NULL, NULL, NULL, nullptr);
+    objecter_op->target.osd = node_id;
+    objecter_op->target.base_oloc.pool = pool_id;
+    objecter_op->target.base_oid.name = oid_name;
+    objecter_op->target.flags = CEPH_OSD_FLAG_READ | CEPH_OSD_FLAG_WRITE;
+
+    objecter->op_submit(objecter_op);
+}
+
+/**
+ * Maximum string length of the RBD block object name prefix (not including
+ * null termination).
+ */
+bool ClientAdaptorMsg::filter_msg(Objecter::op_target_t *t){
+  string obj_name = t->base_oid.name;	
+  if (obj_name.size() < RBD_DATA_OBJECT_NAME_LEN) {
+    return false;
+  }
+
+  if (obj_name.compare(0, RBD_DATA_OBJECT_NAME_FILTER_LEN, RBD_DATA_OBJECT_NAME) == 0){
+    return true;
+  }
+
+  return false;
+}
+
+bool ClientAdaptorMsg::filter_msg_by_op(Objecter::Op *op){
+  return filter_msg(&op->target);
+}
+
+
+bool ClientAdaptorMsg::is_node(uint32_t index){
+  if (index >> FLAG_OFFSET_BIT){
+    return true;
+  }
+  return false;
+}
+
+int32_t ClientAdaptorMsg::get_node_id(int32_t clusterId, string obj_name, int64_t pool_id, uint32_t& pt_id){
+  if (obj_name.length() == 0){
+    std::cout << __func__ << " Client Adaptor: input parameter invalid!" << std::endl;
+    return -RET_CCM_PARAM_ERROR;
+  }
+
+  string obj_id = to_string(pool_id);
+  obj_id += '_';
+  obj_id += obj_name;
+  hash<string> hash_str;
+  uint32_t obj_hashed_id = hash_str(obj_id);
+
+  uint32_t pt_num = 0;
+  NodeInfo info = {0};
+
+  if (mgr_ref->get_pt_num(clusterId, pt_num)){
+    std::cout << __func__ << " Client Adaptor: Get PT number failed" << std::endl;
+    return -RET_CCM_PT_NUM_ERROR;
+  }
+
+  if (pt_num == 0) {
+    std::cout << __func__ << " Client Adaptor: Get PT number zero" << std::endl;
+    return -RET_CCM_PT_NUM_ERROR;
+  }
+
+  pt_id = obj_hashed_id % pt_num;
+
+  PTViewPtEntry pt_entry = {0};
+  if (mgr_ref->get_pt_entry(clusterId, pt_id, &pt_entry)){
+    std::cout << __func__ << " Client Adaptor: Get PT entry failed" << std::endl;
+    return -RET_CCM_PT_ENTRY_ERROR;
+  }
+  uint32_t node_id = pt_entry.curNodeInfo.nodeId << NODE_ID_OFFSET_BIT;
+  node_id += 0x1 << FLAG_OFFSET_BIT;
+  node_id += clusterId << CLUSTER_ID_OFFSET;
+
+  if (mgr_ref->get_node_info(clusterId, pt_entry.curNodeInfo.nodeId, &info)){
+    std::cout << __func__ << " Client Adaptor: Get node info failed. node id " << pt_entry.curNodeInfo.nodeId  << std::endl;
+    return -RET_CCM_NODE_INFO_ERROR;
+  }
+  if (info.portNum > PORT_SUPPORT_MAX || info.portNum == 0) {
+    std::cout << __func__ << " Client Adaptor: Port number invalid. Port Number: " << info.portNum  << std::endl;
+    return -RET_CCM_PORT_NUM_ERROR;
+  }
+  uint32_t pt_index = pt_entry.indexInNode;
+
+  node_id += pt_index % info.portNum;
+
+  return node_id;
+}
+
+bool ClientAdaptorMsg::valid_ip(string ip_addr)
+{
+    string regStr = "^((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]\\d|[1-9])"\
+                    "(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]\\d|\\d)){3})|(0.0.0.0)$";
+    regex regIp(regStr);
+    bool matchValue = regex_match(ip_addr, regIp);
+    return matchValue;
+}
+
+string ClientAdaptorMsg::gc_snap_prefix()
+{
+    return GC_SNAP_PREFIX;
+}
+
+bool ClientAdaptorMsg::is_gc_snap(string snap_name)
+{
+    int n = snap_name.find(GC_SNAP_PREFIX);
+    if (n == 0) {
+        return true;
+    }
+    return false;
+}
+
+void ClientAdaptorMsg::gen_random_gc_snap(uint64_t snap_id, int num, string &rd_snap_name)
+{
+    std::chrono::system_clock::time_point now = std::chrono::system_clock::now();
+    std::chrono::microseconds ms = std::chrono::duration_cast<std::chrono::microseconds>(now.time_since_epoch());
+    long timepoint = ms.count();
+    std::mt19937 generator{std::random_device{}()};
+    std::uniform_int_distribution<uint32_t> distribution{0, 0xFFFF};
+    uint32_t extra = distribution(generator);
+    rd_snap_name.clear();
+    rd_snap_name.append(GC_SNAP_PREFIX);
+    rd_snap_name.append(to_string(extra));
+    rd_snap_name.append("_");
+    rd_snap_name.append(to_string(snap_id));
+    rd_snap_name.append("_");
+    rd_snap_name.append(to_string(num));
+    rd_snap_name.append("_");
+    rd_snap_name.append(to_string(timepoint));
+}
+
+int32_t ClientAdaptorMsg::get_node_ip(int32_t clusterId, uint32_t node_index, string& node_ip){
+  uint32_t node_id = (node_index & NODE_ID_MASK) >> NODE_ID_OFFSET_BIT;
+  uint32_t port_index = node_index & PORT_INDEX_MASK;
+  NodeInfo info = {0};
+  if (mgr_ref->get_node_info(clusterId, node_id, &info)){
+    std::cout << __func__ << " Client Adaptor: Get node info failed." << std::endl;
+    return RET_CCM_NODE_INFO_ERROR;
+  }
+  uint32_t port = info.ports[port_index];
+  if (port < GC_PORT_MIN || port > GC_PORT_MAX) {
+      return RET_CCM_PORT_NUM_ERROR;
+  }
+
+  string server_ip = info.publicAddrStr;
+
+  if (!valid_ip(server_ip)) {
+      return RET_CCM_IP_ERROR;
+  }
+
+  string addr_str = "tcp://";
+  addr_str += server_ip;
+  addr_str += ":";
+  addr_str += to_string(port);
+  node_ip = addr_str;
+  return RET_OK;
+}
+
+int32_t ClientAdaptorMsg::get_node_raw_ip(int32_t clusterId, uint32_t node_index, string& node_ip) {
+    uint32_t node_id = (node_index & NODE_ID_MASK) >> NODE_ID_OFFSET_BIT;
+    uint32_t port_index = node_index & PORT_INDEX_MASK;
+    NodeInfo info = {0};
+    if (mgr_ref->get_node_info(clusterId, node_id, &info)){
+        std::cout << __func__ << " Client Adaptor: Get node info failed." << std::endl;
+        return RET_CCM_NODE_INFO_ERROR;
+    }
+    uint32_t port = info.ports[port_index];
+    if (port < GC_PORT_MIN || port > GC_PORT_MAX) {
+        return RET_CCM_PORT_NUM_ERROR;
+    }
+
+    node_ip = info.publicAddrStr;
+
+    if (!valid_ip(node_ip)) {
+        return RET_CCM_IP_ERROR;
+    }
+
+    return RET_OK;
+}
+
+void ClientAdaptorMsg::set_mgr(ClientAdaptorMgr* mgr){
+  mgr_ref = mgr;
+  return;
+}
+
+ClientAdaptorMgr* ClientAdaptorMsg::get_mgr(){
+  return mgr_ref;
+}
+
+void das_req_prefetch(DasKvParam *params)
+{
+    if (params == NULL) {
+        return;
+    }
+    ClientAdaptorMsg *msg_ref = static_cast<ClientAdaptorMsg *>(params->handle);
+    Objecter *obj = static_cast<Objecter *>(params->ctx);
+    if (msg_ref->is_valid_object(obj) ==false) {
+      return;
+    }
+    uint64_t offset = params->offset & SEGMENT_MASK;
+    int id = params->objId;
+    uint64_t left = params->len;
+    while(left) {
+        uint64_t max = std::min<uint64_t>(SEGMENT_SIZE - offset, left);
+
+        char buff[params->imageIdLen+OBJECT_ID_LEN + 1];
+        snprintf(buff, params->imageIdLen + 1, "%s", params->imageIdBuf);
+        snprintf(buff+params->imageIdLen, OBJECT_ID_LEN+1, "%016x", id);
+        std::string oid_name(buff);
+        uint32_t pt_id;
+        int64_t pool_id = params->cephPoolId;
+        int32_t node_id = msg_ref->get_node_id(params->clusterId, oid_name, pool_id, pt_id);
+        if (node_id < 0) {
+            ceph_abort();
+        }
+        bufferlist indata;
+        encode(offset, indata);
+        encode(max, indata);
+        msg_ref->push_strategy(obj, params->cephPoolId, node_id, oid_name, indata);
+
+        left -= max;
+        offset = 0;
+        id++;
+    };
+}
+
+int32_t ClientAdaptorMsg::das_init(Objecter *obj)
+{
+    int32_t rc;
+    das_objs.insert(obj);
+    if (initialized)
+      return 0;
+    DasModuleParam *dasInstanceParam = new DasModuleParam();
+    DasOPS *regOps = new DasOPS();
+    regOps->SubmitDasPrefetch = das_req_prefetch;
+    dasInstanceParam->ops = regOps;
+
+    rc = OpenRcacheCeateDasModule(this, dasInstanceParam);
+    if (rc) {
+      return -1;
+    }
+    initialized = true;
+    return 0;
+}
+
+int32_t ClientAdaptorMsg::das_update_info(int32_t clusterId, Objecter *obj, Objecter::Op *op)
+{
+    if (!initialized)
+      return 0;
+    DasKvParam *params[op->ops.size()];
+
+    if((op->target.flags & CEPH_OSD_FLAG_WRITE) == CEPH_OSD_FLAG_WRITE)
+      return 0;
+    string obj_name = op->target.base_oid.name;
+    if (obj_name.compare(0, RBD_DATA_OBJECT_NAME_FILTER_LEN, RBD_DATA_OBJECT_NAME))
+      return 0;
+    std::size_t found = obj_name.find_last_of('.');
+    if(found == std::string::npos)
+      return -EINVAL;
+    uint64_t objId = std::stol(obj_name.substr(found+1, OBJECT_ID_LEN), nullptr, 16);
+    uint64_t ns = ceph_clock_now().to_nsec();
+    int i = 0;
+    for(vector<OSDOp>::iterator p = op->ops.begin(); p != op->ops.end(); ++p) {
+    	if (p->op.op == CEPH_OSD_OP_READ || p->op.op == CEPH_OSD_OP_SPARSE_READ || p->op.op == CEPH_OSD_OP_SYNC_READ) {
+            params[i] = reinterpret_cast<DasKvParam*>(new char[sizeof(DasKvParam) + found + 1]);
+            params[i]->offset = p->op.extent.offset;
+            params[i]->len = p->op.extent.length;
+            params[i]->opcode = 0;
+            params[i]->timeStamp = ns;
+            params[i]->cephPoolId = op->target.base_oloc.pool;
+            params[i]->algType = DAS_ALG_SEQ;
+            params[i]->objId = objId;
+            params[i]->imageIdLen =found + 1;
+            params[i]->clusterId = clusterId;
+            memcpy(params[i]->imageIdBuf, obj_name.c_str(), params[i]->imageIdLen);
+            params[i]->handle = this;
+            params[i]->ctx = obj;
+            i++;
+        }
+    }
+
+    if (i) {
+      int rc = OpenRcachePutDasInfo(params, i);
+      if (rc)
+	 return -1;
+    }
+    return 0;
+}
+
+int32_t ClientAdaptorMsg::get_node_pool(const int64_t pool_id, int32_t *nodeId)
+{
+    return mgr_ref->get_node_from_ma(pool_id, nodeId);
+}
diff --git a/src/client_adaptor/ClientAdaptorMsg.h b/src/client_adaptor/ClientAdaptorMsg.h
new file mode 100644
index 0000000..80afa78
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorMsg.h
@@ -0,0 +1,87 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#ifndef CLIENT_ADAPTOR_MSG_H
+#define CLIENT_ADAPTOR_MSG_H
+
+#include <string>
+#include <map>
+#include <stdint.h>
+#include "open_das.h"
+
+#include "osdc/Objecter.h"
+#include "ClientAdaptorMgr.h"
+
+class ClientAdaptorMsg {
+public:
+  ClientAdaptorMsg(ClientAdaptorMgr* mgr); 
+  ~ClientAdaptorMsg() {}; 
+
+  void push_strategy(Objecter *objecter, uint64_t pool_id, int32_t node_id, std::string oid_name, bufferlist &indata);
+
+  int32_t das_init(Objecter *obj);
+
+  void das_remove(Objecter *obj) {
+    das_objs.erase(obj);
+    if (das_objs.empty() && initialized) {
+      initialized = false;
+      OpenRcacheExitDasModule(this);
+    }
+  }
+
+  bool filter_msg(Objecter::op_target_t *t);
+
+  bool filter_msg_by_op(Objecter::Op *op);
+
+  const string name() {return "ClientAdaptorMsg";}
+
+  bool is_node(uint32_t index);
+
+  int32_t get_node_id(int32_t clusterId, string obj_name, int64_t pool_id, uint32_t& pt_index);
+
+  int32_t get_node_ip(int32_t clusterId, uint32_t node_index, string& node_ip);
+
+  int32_t get_node_raw_ip(int32_t clusterId, uint32_t node_index, string& node_ip);
+
+  void set_mgr(ClientAdaptorMgr* mgr);
+
+  ClientAdaptorMgr* get_mgr(void);
+
+  bool is_valid_object(Objecter *obj) {
+    auto it = das_objs.find(obj);
+    if (it != das_objs.end())
+      return true;
+    return false;
+  }
+
+  int32_t das_update_info(int32_t clusterId, Objecter *obj, Objecter::Op *op);
+
+  int32_t get_node_pool(const int64_t pool_id, int32_t *nodeId);
+
+  string gc_snap_prefix();
+  bool is_gc_snap(string snap_name);
+  void gen_random_gc_snap(uint64_t snap_id, int num, string &rd_snap_name);
+
+protected:
+  ClientAdaptorMgr* mgr_ref;
+
+  const int FLAG_OFFSET_BIT = 20;
+  const int CLUSTER_ID_OFFSET = 16;
+  const int PORT_INDEX_MASK = 0xf;
+  const int NODE_ID_OFFSET_BIT = 4;
+  const int NODE_ID_MASK = 0xfff0;
+  const int PORT_SUPPORT_MAX = 16;
+private:
+  bool initialized = false;
+  std::set<Objecter *> das_objs;
+  bool valid_ip(string ip_addr);
+public:
+  std::unordered_set<void *> connections;
+  mutable std::shared_mutex connlock;
+};
+
+
+#endif
diff --git a/src/client_adaptor/ClientAdaptorPerf.cc b/src/client_adaptor/ClientAdaptorPerf.cc
new file mode 100644
index 0000000..cea7835
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorPerf.cc
@@ -0,0 +1,85 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include "ClientAdaptorPerf.h"
+#include "ClientAdaptorPlugin.h"
+using namespace std;
+
+#include <sys/syscall.h>
+#define gettid() syscall(__NR_gettid)
+
+void ClientAdaptorPerf::start_tick(Objecter::Op *op) {
+  gettimeofday(&(op->perf_tick.start), NULL);
+  return;
+}
+
+void ClientAdaptorPerf::end_tick(Objecter::Op *op) {
+  gettimeofday(&(op->perf_tick.end), NULL);
+  return;
+}
+  
+void ClientAdaptorPerf::record_op(Objecter::Op *op) {
+  for (vector<OSDOp>::iterator p = op->ops.begin(); p != op->ops.end(); ++p) {
+    if (p->op.op == CEPH_OSD_OP_READ || p->op.op == CEPH_OSD_OP_SPARSE_READ || p->op.op == CEPH_OSD_OP_SYNC_READ) {
+      read.op_count++;
+      read.time_cost += (op->perf_tick.end.tv_sec - op->perf_tick.start.tv_sec) * 1000 * 1000 + \
+             (op->perf_tick.end.tv_usec - op->perf_tick.start.tv_usec);
+    } else if (p->op.op == CEPH_OSD_OP_WRITE || p->op.op == CEPH_OSD_OP_WRITEFULL) {
+      write.op_count++;
+      write.time_cost += (op->perf_tick.end.tv_sec - op->perf_tick.start.tv_sec) * 1000 * 1000 + \
+	     (op->perf_tick.end.tv_usec-op->perf_tick.start.tv_usec);
+    }
+  }
+  return;
+}
+
+std::function<void ()> ClientAdaptorPerf::create_thread(const ClientAdaptorPlugin* in) {
+  const ClientAdaptorPlugin* plugin = in;
+  return [this, plugin](){
+    char thread_name[16];
+    sprintf(thread_name, "ca-perf-tick");
+    pthread_setname_np(pthread_self(), thread_name);
+    uint64_t read_cnt = 0;
+    uint64_t read_cost = 0;
+    uint64_t write_cnt = 0;
+    uint64_t write_cost = 0;
+    uint64_t read_lat = 0xff;
+    uint64_t write_lat = 0xff;
+    float avg_flight = 0;
+
+    while (!tick_done) {
+	    sleep (3);
+	    read_cnt = plugin->perf_ref->read.op_count;
+	    read_cost = plugin->perf_ref->read.time_cost;
+	    write_cnt = plugin->perf_ref->write.op_count;
+	    write_cost = plugin->perf_ref->write.time_cost;
+	    if (read_cnt != 0) {
+		    read_lat = read_cost/read_cnt;
+	    }
+	    if (write_cnt != 0) {
+		    write_lat = write_cost/write_cnt;
+	    }
+	    if ((read_cnt + write_cnt) != 0 ) {
+		    avg_flight = (float)total_in_flight / (float)(read_cnt + write_cnt);
+	    }
+	    outfile << "*************************************************************************" << std::endl;
+	    outfile << "PID: " << getpid() << "    TID: " << gettid() << std::endl;
+	    outfile << "          total_count     avg_latency(us)" << std::endl;
+	    outfile << "read " << setw(16) << read_cnt << "    " << setw(16) << read_lat << std::endl;
+	    outfile << "write" << setw(16) << write_cnt << "    " << setw(16) << write_lat << std::endl;
+	    outfile << "          total_count      average" << std::endl;
+	    outfile << "in-flight" << setw(12) << total_in_flight
+		    <<  "   " << setw(16) << fixed << setprecision(1) << avg_flight << std::endl;
+    }
+   };
+}
+
+void ClientAdaptorPerf::start_record(ClientAdaptorPlugin* plugin) {
+	std::function<void ()> perf_thread = create_thread(plugin);
+	threads.push_back(std::thread(perf_thread));
+	outfile.open("/var/log/ceph/perf_tick.log", ios::out | ios::app);
+	return;
+}
diff --git a/src/client_adaptor/ClientAdaptorPerf.h b/src/client_adaptor/ClientAdaptorPerf.h
new file mode 100644
index 0000000..f7ee5be
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorPerf.h
@@ -0,0 +1,55 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#ifndef CLIENT_ADAPTOR_PERF_H
+#define CLIENT_ADAPTOR_PERF_H
+
+#include <stdint.h>
+#include <sys/time.h>
+#include <thread>
+#include <iomanip>
+#include <sched.h>
+#include <vector>
+#include <iostream>
+#include <fstream>
+
+#include "osdc/Objecter.h"
+
+class ClientAdaptorPlugin;
+
+class ClientAdaptorPerf {
+public:
+  ClientAdaptorPerf(){}
+  ~ClientAdaptorPerf(){}
+
+
+  struct op_perf_t {
+    std::atomic<uint64_t> op_count{0};
+    std::atomic<uint64_t> time_cost{0};
+  };
+
+void start_tick(Objecter::Op *op);
+
+void end_tick(Objecter::Op *op);
+  
+void record_op(Objecter::Op *op); 
+
+void start_record(ClientAdaptorPlugin* plugin); 
+
+std::function<void ()> create_thread(const ClientAdaptorPlugin* plugin);
+
+const string name() {return "ClientAdaptorPerf";}
+vector<std::thread> threads;
+bool tick_done{false};
+std::ofstream outfile;
+std::atomic<uint64_t> total_in_flight{0};
+private:
+  struct op_perf_t read;
+  struct op_perf_t write;
+
+};
+
+#endif
diff --git a/src/client_adaptor/ClientAdaptorPlugin.cc b/src/client_adaptor/ClientAdaptorPlugin.cc
new file mode 100644
index 0000000..c44e348
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorPlugin.cc
@@ -0,0 +1,45 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include <iostream>
+#include "ClientAdaptorPlugin.h"
+#include "ceph_ver.h"
+#include "ClientAdaptorMsg.h"
+#include "ClientAdaptorMgr.h"
+#include "ClientAdaptorPerf.h"
+
+
+
+
+ClientAdaptorPlugin::~ClientAdaptorPlugin() {
+    delete mgr_ref;
+    delete msg_ref;
+    delete perf_ref;
+}
+
+const char *__ceph_plugin_version()
+{
+  return CEPH_GIT_NICE_VER;
+}
+
+
+int __ceph_plugin_init(CephContext *cct,
+		       const std::string& type,
+		       const std::string& name)
+{
+  PluginRegistry *instance = cct->get_plugin_registry();
+  if (cct->_conf.get_val<bool>("global_cache_debug_mode")){
+    ClientAdaptorLocal* ccm = new ClientAdaptorLocal();
+    ClientAdaptorMsg* msg = new ClientAdaptorMsg(ccm);
+    ClientAdaptorPerf* perf = new ClientAdaptorPerf();
+    return instance->add(type, name, new ClientAdaptorPlugin(cct, msg, ccm, perf));
+  } else {
+    ClientAdaptorCcm* ccm = new ClientAdaptorCcm();
+    ClientAdaptorMsg* msg = new ClientAdaptorMsg(ccm);
+    ClientAdaptorPerf* perf = new ClientAdaptorPerf();
+    return instance->add(type, name, new ClientAdaptorPlugin(cct, msg, ccm, perf));
+  }
+}
diff --git a/src/client_adaptor/ClientAdaptorPlugin.h b/src/client_adaptor/ClientAdaptorPlugin.h
new file mode 100644
index 0000000..fa7d71d
--- /dev/null
+++ b/src/client_adaptor/ClientAdaptorPlugin.h
@@ -0,0 +1,39 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#ifndef CLIENT_ADAPTOR_PLUGIN_H
+#define CLIENT_ADAPTOR_PLUGIN_H
+#include <unistd.h>
+
+//#include "ceph_ver.h"
+#include "common/PluginRegistry.h"
+#include "common/ceph_context.h"
+//#include "acconfig.h"
+
+
+class ClientAdaptorMsg;
+class ClientAdaptorMgr;
+class ClientAdaptorPerf;
+
+class ClientAdaptorPlugin : public Plugin {
+public:
+  ClientAdaptorPlugin(CephContext* cct, ClientAdaptorMsg* msg, ClientAdaptorMgr* mgr,
+      ClientAdaptorPerf* perf) : Plugin(cct), msg_ref(msg), mgr_ref(mgr), perf_ref(perf)
+  {
+  }
+  
+  ~ClientAdaptorPlugin();
+
+  ClientAdaptorMsg* msg_ref;
+  ClientAdaptorMgr* mgr_ref;
+  ClientAdaptorPerf* perf_ref;
+
+  const string name() {
+    return "ClientAdaptorPlugin";
+  }
+};
+
+#endif
diff --git a/src/client_adaptor/open_ccm.h b/src/client_adaptor/open_ccm.h
new file mode 100644
index 0000000..2fefc7e
--- /dev/null
+++ b/src/client_adaptor/open_ccm.h
@@ -0,0 +1,209 @@
+/* 
+ * Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+*/
+#ifndef __CCM_INTERFACE_H__
+#define __CCM_INTERFACE_H__
+
+#include <stdint.h>
+#include <stdbool.h>
+#include <time.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define PT_VIEW_NODE_MAX_DOMAIN     32
+#define PT_VIEW_MAX_POOL            256
+#define MAX_CCM_CTRL_NODE_NUM       128
+
+#define MAX_PT_ENTRY          1024
+
+#define MAX_SERVER_NUM        MAX_CCM_CTRL_NODE_NUM
+
+#define CCM_MAX_DISK_NUM      1024
+#define CCM_VNODE_NUM_PER_NODE   8
+#define MAX_DISK_NUM_PER_NODE   16
+#define IP_ADDR_LEN           (16)
+#define DISK_NAME_LEN         (64)
+#define DISK_SN_LEN           (64)
+
+#define MAX_POOL_NAME_LEN     (256)
+#define CCM_MAX_POOL_NUM      (4096)
+#define MAX_PORT_NUM          (8)
+
+#define ZK_IP_ADDR_LEN        (16)
+#define ZK_DISK_NAME_LEN      (64)
+#define ZK_DISK_SN_LEN        (64)
+
+#define CCM_VERSION_0 0
+#define CCM_VERSION_1 1
+#define CCM_CURRENT_VERSION CCM_VERSION_0
+#define CCM_Upgrade_Embedding_LEN 64
+
+typedef enum {
+    NODE_STATE_INVALID  = 0,
+    NODE_STATE_UP       = 1,
+    NODE_STATE_STARTING = 2,
+    NODE_STATE_RUNNING  = 3,
+    NODE_STATE_UNWORK   = 4,
+    NODE_STATE_DOWN     = 5,
+    NODE_STATE_BUTT
+} NodeState;
+
+typedef enum {
+    VDISK_STATE_DOWN = 0,
+    VDISK_STATE_UP   = 1,
+    VIDSK_STATE_BUTT
+} VdiskState;
+
+typedef struct {
+    uint32_t NodeId;
+    uint32_t ptNum;
+    uint32_t (*ptMap)[2];
+} NodePtInfo;
+
+typedef struct {
+    uint32_t nodeId;
+    uint32_t diskId;
+    uint32_t localDiskId;
+    char diskName[DISK_NAME_LEN];
+    char sn[DISK_SN_LEN];
+    uint32_t capacity;
+    uint32_t usedCap;
+    VdiskState state;
+    bool isFirstFormat;
+} VdiskInfo;
+
+typedef struct {
+    uint32_t rackId;
+    uint32_t nodeId;
+    NodeState state;
+    uint32_t ipv4addr;
+    char ipv4AddrStr[IP_ADDR_LEN];
+    char publicAddrStr[IP_ADDR_LEN];
+    char clusterAddrStr[IP_ADDR_LEN];
+    int32_t portNum;
+    uint32_t ports[MAX_PORT_NUM];
+    uint32_t diskNum;
+    VdiskInfo diskList[MAX_DISK_NUM_PER_NODE];
+    uint64_t version;
+    NodeState inOutState;
+    NodeState runningState;
+} NodeInfo;
+
+typedef enum {
+    CCM_PT_STATE_INIT=0,
+    CCM_PT_STATE_OK,
+    CCM_PT_STATE_TRIM,
+    CCM_PT_STATE_REPLAY,
+    CCM_PT_STATE_FAULT,
+} PtState;
+
+typedef struct {
+    uint32_t nodeId;
+    uint32_t diskId;
+    uint32_t vnodeId;
+} PtNodeInfo;
+
+typedef struct {
+    uint32_t version;
+    bool ptChange;
+    uint32_t birthVersion;
+    uint32_t ptId;
+    uint32_t indexInNode;
+    PtState state;
+    PtNodeInfo curNodeInfo;
+    PtNodeInfo srcNodeInfo;
+    char reserved[CCM_Upgrade_Embedding_LEN];
+} PtInfo;
+
+typedef struct _PtView {
+    uint32_t version;
+    uint32_t globalVersion;
+    uint32_t ptNum;
+    char reserved[CCM_Upgrade_Embedding_LEN];
+    PtInfo ptInfo[0];
+} PtView;
+
+typedef PtInfo PTViewPtEntry;
+
+/* NodeView callback */
+typedef struct {
+    void *ctx;
+    int32_t (*notifyNodeChange)(int32_t clusterId, NodeInfo *nodeList, uint32_t nodeNum, void *ctx);
+} NodeViewChangeOpHandle;
+
+/* PTView callback */
+typedef struct {
+    void *ctx;
+    int32_t (*notifyPtChange)(PTViewPtEntry *entry, uint32_t entryNum, void *ctx);
+} PTViewChangeOpHandle;
+
+typedef enum {
+    CCM_MODULE_INFRAS = 0,
+    CCM_MODULE_PLOG,
+    CCM_MODULE_INDEX,
+    CCM_MODULE_CACHE,
+    CCM_MODULE_CLIENT,
+    CCM_MODULE_CEPH,
+    CCM_MODULE_BUTT,
+} ModuleType;
+
+typedef struct {
+    int64_t mdPoolId;
+    int64_t dataPoolId;
+    const char *imageId;
+    uint64_t numObjs;
+    uint64_t snapId;
+    uint64_t oldHeadSnapId;
+    uint64_t rollbackSnapId;
+    uint64_t snapSeq;
+} RollbackInfo;
+
+int32_t OpenAgentInit(int64_t poolId, int32_t *nodeId);
+
+int32_t OpenBcmInit(void);
+
+int32_t OpenGetPtEntry(int32_t clusterId, uint32_t ptId, PTViewPtEntry *entry);
+
+uint32_t OpenGetTotalPtNum(int32_t clusterId);
+
+int32_t OpenAgentGetNodeInfo(int32_t clusterId, uint32_t nodeId, NodeInfo *nodeInfo);
+
+int32_t OpenRegisterViewChangeNotifyChain(PTViewChangeOpHandle *handle);
+
+int32_t OpenRegisterNodeViewChangeNotifyChain(NodeViewChangeOpHandle *handle);
+
+void OpenDeregisterViewChangeNotifyChain(PTViewChangeOpHandle *handle);
+
+void OpenDeregisterNodeViewChangeNotifyChain(NodeViewChangeOpHandle *handle);
+
+int32_t OpenCreateSnapshot(int64_t mdPoolId, int64_t dataPoolId, const char *imageId, uint64_t snapId);
+
+int32_t OpenDeleteSnapshot(int64_t dataPoolId, const char *nameSpace, const char *imageId, uint64_t snapId);
+
+int32_t OpenReleaseImageResource(int64_t poolId, const char *imageId);
+
+int32_t OpenRollbackSnapshot(RollbackInfo *info);
+
+int32_t OpenImageBusy(int64_t dataPoolId, const char *imageId);
+
+int32_t OpenSnapshotBusy(int64_t dataPoolId, const char *imageId, int64_t snapId);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif // __CCM_INTERFACE_H__
\ No newline at end of file
diff --git a/src/client_adaptor/open_das.h b/src/client_adaptor/open_das.h
new file mode 100644
index 0000000..6eac9f7
--- /dev/null
+++ b/src/client_adaptor/open_das.h
@@ -0,0 +1,67 @@
+/* 
+ * Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+*/
+#ifndef OPEN_DAS_H
+#define OPEN_DAS_H
+
+#include <cstdio>
+#include <cstdint>
+#include <cstdlib>
+#include <string>
+
+typedef enum EnumDasResult {
+    RETURN_DAS_FULL = -2,
+    RETURN_DAS_ERROR = -1,
+    RETURN_DAS_OK = 0,
+    RETURN_DAS_EMPTY = 1,
+    RETURN_DAS_DELETING = 2,
+} DAS_RESULT;
+
+typedef enum TagDasAlgType {
+    DAS_ALG_SEQ = 0,
+    DAS_ALG_REVERSE_SEQ,
+    DAS_ALG_STRIDE,
+    DAS_ALG_BUTT,
+} DasAlgType;
+
+typedef struct TagDasKvParam {
+    uint64_t offset;
+    uint64_t len;
+    uint8_t opcode;
+    uint64_t timeStamp;
+    int64_t cephPoolId;
+    DasAlgType algType;
+    uint64_t objId;
+    uint32_t imageIdLen;
+    int32_t clusterId;
+    void *ctx;
+    void *handle;
+    char imageIdBuf[0];
+} DasKvParam;
+
+typedef struct TagDasOPS {
+    void (*SubmitDasPrefetch)(DasKvParam* params);
+} DasOPS;
+
+typedef struct TagDasModuleParam {
+    DasOPS *ops;
+} DasModuleParam;
+
+int32_t OpenRcacheCeateDasModule(void *handle, DasModuleParam *createInstanceParam);
+
+int32_t OpenRcachePutDasInfo(DasKvParam *params[], uint32_t keyNum);
+
+void OpenRcacheExitDasModule(void *handle);
+#endif // OPEN_DAS_H
\ No newline at end of file
diff --git a/src/common/options.cc b/src/common/options.cc
index 8135ea8..9d3bb78 100644
--- a/src/common/options.cc
+++ b/src/common/options.cc
@@ -1015,7 +1015,7 @@ std::vector<Option> get_global_options() {
     .set_description("Induce a crash/exit on various bugs (for testing purposes)"),
 
     Option("ms_dispatch_throttle_bytes", Option::TYPE_SIZE, Option::LEVEL_ADVANCED)
-    .set_default(100_M)
+    .set_default(2048_M)
     .set_description("Limit messages that are read off the network but still being processed"),
 
     Option("ms_bind_ipv4", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
@@ -2308,6 +2308,15 @@ std::vector<Option> get_global_options() {
     .set_default(10.0)
     .set_description("Seconds before in-flight op is considered 'laggy' and we query mon for the latest OSDMap"),
 
+#ifdef WITH_GLOBAL_CACHE
+    Option("objecter_inflight_op_bytes", Option::TYPE_SIZE, Option::LEVEL_ADVANCED)
+    .set_default(0)
+    .set_description("Max in-flight data in bytes (both directions)"),
+
+    Option("objecter_inflight_ops", Option::TYPE_UINT, Option::LEVEL_ADVANCED)
+    .set_default(0)
+    .set_description("Max in-flight operations"),
+#else
     Option("objecter_inflight_op_bytes", Option::TYPE_SIZE, Option::LEVEL_ADVANCED)
     .set_default(100_M)
     .set_description("Max in-flight data in bytes (both directions)"),
@@ -2315,6 +2324,7 @@ std::vector<Option> get_global_options() {
     Option("objecter_inflight_ops", Option::TYPE_UINT, Option::LEVEL_ADVANCED)
     .set_default(1024)
     .set_description("Max in-flight operations"),
+#endif
 
     Option("objecter_completion_locks_per_session", Option::TYPE_UINT, Option::LEVEL_DEV)
     .set_default(32)
@@ -5587,6 +5597,14 @@ std::vector<Option> get_global_options() {
     Option("debug_heartbeat_testing_span", Option::TYPE_INT, Option::LEVEL_DEV)
     .set_default(0)
     .set_description("Override 60 second periods for testing only"),
+#ifdef WITH_GLOBAL_CACHE
+    Option("global_cache_debug_mode", Option::TYPE_BOOL, Option::LEVEL_DEV)
+    .set_default(false)
+    .set_description("Global Cache client adaptor local debug mode switch"),
+    Option("global_cache_tick", Option::TYPE_BOOL, Option::LEVEL_DEV)
+    .set_default(false)
+    .set_description("Global Cache client adaptor performance tick switch"),
+#endif
   });
 }
 
@@ -7222,11 +7240,15 @@ static std::vector<Option> get_rbd_options() {
     Option("rbd_non_blocking_aio", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
     .set_default(true)
     .set_description("process AIO ops from a dispatch thread to prevent blocking"),
-
+#ifdef WITH_GLOBAL_CACHE
+    Option("rbd_cache", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
+    .set_default(false)
+    .set_description("whether to enable caching (writeback unless rbd_cache_max_dirty is 0)"),
+#else
     Option("rbd_cache", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
     .set_default(true)
     .set_description("whether to enable caching (writeback unless rbd_cache_max_dirty is 0)"),
-
+#endif
     Option("rbd_cache_writethrough_until_flush", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
     .set_default(true)
     .set_description("whether to make writeback caching writethrough until "
@@ -7537,6 +7559,15 @@ static std::vector<Option> get_rbd_options() {
     .set_default(60)
     .set_min(0)
     .set_description("RBD Image access timestamp refresh interval. Set to 0 to disable access timestamp update."),
+#ifdef WITH_GLOBAL_CACHE
+    Option("global_cache", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
+    .set_default(true)
+    .set_description("whether to enable global cache"),
+
+    Option("gc_perf", Option::TYPE_BOOL, Option::LEVEL_ADVANCED)
+    .set_default(true)
+    .set_description("whether to enable global cache perf"),
+#endif
   });
 }
 
diff --git a/src/include/config-h.in.cmake b/src/include/config-h.in.cmake
index d83a59b..12dfdf5 100644
--- a/src/include/config-h.in.cmake
+++ b/src/include/config-h.in.cmake
@@ -187,6 +187,9 @@
 /* Define if you want to use Babeltrace */
 #cmakedefine WITH_BABELTRACE
 
+/* Define if you want to use Global Cache */
+#cmakedefine WITH_GLOBAL_CACHE
+
 /* Define to 1 if you have the <babeltrace/babeltrace.h> header file. */
 #cmakedefine HAVE_BABELTRACE_BABELTRACE_H 1
 
diff --git a/src/include/rados/librados.hpp b/src/include/rados/librados.hpp
index 0c047c4..2b0ce9f 100644
--- a/src/include/rados/librados.hpp
+++ b/src/include/rados/librados.hpp
@@ -1247,6 +1247,9 @@ inline namespace v14_2_0 {
     std::string get_namespace() const;
 
     int64_t get_id();
+#ifdef WITH_GLOBAL_CACHE
+    uint64_t get_snap_seq();
+#endif
 
     // deprecated versions
     uint32_t get_object_hash_position(const std::string& oid)
@@ -1276,6 +1279,11 @@ inline namespace v14_2_0 {
                                     const std::string &key);
     int application_metadata_list(const std::string& app_name,
                                   std::map<std::string, std::string> *values);
+#ifdef WITH_GLOBAL_CACHE
+    bool check_acc();
+    int32_t get_cluster_id();
+
+#endif
 
   private:
     /* You can only get IoCtx instances from Rados */
diff --git a/src/include/rbd/librbd.h b/src/include/rbd/librbd.h
index 83b5735..8874286 100644
--- a/src/include/rbd/librbd.h
+++ b/src/include/rbd/librbd.h
@@ -934,6 +934,16 @@ CEPH_RBD_API int64_t rbd_read_iterate(rbd_image_t image, uint64_t ofs, size_t le
 CEPH_RBD_API int rbd_read_iterate2(rbd_image_t image, uint64_t ofs, uint64_t len,
 		                   int (*cb)(uint64_t, size_t, const char *, void *),
                                    void *arg);
+#ifdef WITH_GLOBAL_CACHE
+CEPH_RBD_API void rbd_get_date_dts_addr(rbd_image_t image, uint64_t offset, uint64_t *objOffset,
+                    char *objAddr, uint32_t *objId);
+
+CEPH_RBD_API int rbd_client_image_read(rbd_image_t image, uint32_t objId, uint32_t objOffset,
+                    char* buf, size_t len);
+
+CEPH_RBD_API int rbd_client_image_aio_read(rbd_image_t image, uint32_t objId, uint32_t objOffset,
+                char *buf, size_t len, rbd_completion_t c);
+#endif
 /**
  * get difference between two versions of an image
  *
diff --git a/src/include/rbd/librbd.hpp b/src/include/rbd/librbd.hpp
index 15eb9f5..10e82f4 100644
--- a/src/include/rbd/librbd.hpp
+++ b/src/include/rbd/librbd.hpp
@@ -542,6 +542,14 @@ public:
 		       int (*cb)(uint64_t, size_t, const char *, void *), void *arg);
   int read_iterate2(uint64_t ofs, uint64_t len,
 		    int (*cb)(uint64_t, size_t, const char *, void *), void *arg);
+
+  int get_data_dst_addr(uint64_t off, uint64_t *obj_offset,
+                        char *obj_addr, uint32_t *obj_id);
+
+  ssize_t client_image_read(uint32_t objId, uint32_t objOffset,
+                            char* buf, size_t len);
+  ssize_t client_image_aio_read(uint32_t objId, uint32_t objOffset,
+                            char* buf, size_t len, RBD::AioCompletion *c);
   /**
    * get difference between two versions of an image
    *
@@ -621,7 +629,6 @@ public:
    * @returns 0 on success, negative error code on failure
    */
   int aio_flush(RBD::AioCompletion *c);
-
   /**
    * Drop any cached data for this image
    *
diff --git a/src/librados/RadosClient.cc b/src/librados/RadosClient.cc
index 902c2c6..6d86530 100644
--- a/src/librados/RadosClient.cc
+++ b/src/librados/RadosClient.cc
@@ -52,6 +52,11 @@
 #include "include/ceph_assert.h"
 #include "common/EventTrace.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#endif
+
 #define dout_subsys ceph_subsys_rados
 #undef dout_prefix
 #define dout_prefix *_dout << "librados: "
@@ -495,7 +500,23 @@ int librados::RadosClient::create_ioctx(const char *name, IoCtxImpl **io)
   if (poolid < 0) {
     return (int)poolid;
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  int32_t node_pool = -1;
+  int32_t ret = plugin->msg_ref->get_node_pool(poolid, &node_pool);
+  if (ret != 0) {
+    lderr(cct) << __func__ << " get node pool from ma failed" << dendl;
+    return -EHOSTUNREACH;
+  }
+  if (node_pool <= 4 && node_pool >= 1) {
+    ldout(cct, 10) << __func__ << " poolid " << poolid << " belong to node " << node_pool << " is acc pool" << dendl;
+    objecter->set_acc_pool_set(poolid, node_pool);
+  } else {
+    ldout(cct, 10) << __func__ << " poolid " << poolid << " is nor pool" << dendl;
+  }
+#endif
   *io = new librados::IoCtxImpl(this, objecter, poolid, CEPH_NOSNAP);
   return 0;
 }
@@ -506,6 +527,24 @@ int librados::RadosClient::create_ioctx(int64_t pool_id, IoCtxImpl **io)
   int r = pool_get_name(pool_id, &pool_name, true);
   if (r < 0)
     return r;
+
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  int32_t node_pool = -1;
+  int32_t ret = plugin->msg_ref->get_node_pool(pool_id, &node_pool);
+  if (ret != 0) {
+    lderr(cct) << __func__ << " get node pool from ma failed" << dendl;
+    return -EHOSTUNREACH;
+  }
+  if (node_pool <= 4 && node_pool >= 1) {
+    ldout(cct, 10) << __func__ << " poolid " << pool_id << " belong to node " << node_pool << " is acc pool" << dendl;
+    objecter->set_acc_pool_set(pool_id, node_pool);
+  } else {
+    ldout(cct, 10) << __func__ << " poolid " << pool_id << " is nor pool" << dendl;
+  }
+#endif
   *io = new librados::IoCtxImpl(this, objecter, pool_id, CEPH_NOSNAP);
   return 0;
 }
diff --git a/src/librados/librados_cxx.cc b/src/librados/librados_cxx.cc
index bc3f65f..69f0252 100644
--- a/src/librados/librados_cxx.cc
+++ b/src/librados/librados_cxx.cc
@@ -2976,3 +2976,18 @@ int librados::IoCtx::application_metadata_list(const std::string& app_name,
 {
   return io_ctx_impl->application_metadata_list(app_name, values);
 }
+
+#ifdef WITH_GLOBAL_CACHE
+bool librados::IoCtx::check_acc()
+{
+  int64_t poolid = io_ctx_impl->get_id();
+  return io_ctx_impl->objecter->get_acc_pool_set(poolid);
+}
+
+int32_t librados::IoCtx::get_cluster_id()
+{
+  int64_t poolid = io_ctx_impl->get_id();
+  return io_ctx_impl->objecter->get_cluster_id(poolid);
+}
+
+#endif
diff --git a/src/librbd/ImageCtx.cc b/src/librbd/ImageCtx.cc
index 8375d1a..f79941e 100644
--- a/src/librbd/ImageCtx.cc
+++ b/src/librbd/ImageCtx.cc
@@ -270,6 +270,18 @@ public:
                         "wb", perf_prio, unit_t(UNIT_BYTES));
     plb.add_time_avg(l_librbd_wr_latency, "wr_latency", "Write latency",
                      "wl", perf_prio);
+#ifdef WITH_GLOBAL_CACHE
+    plb.add_time_avg(l_librbd_rd_before_queue_op_lat, "rd_before_queue_latency", "before queue latency",
+		    "rbql", perf_prio);
+    plb.add_time_avg(l_librbd_wr_before_queue_op_lat, "wr_before_queue_latency", "before queue latency",
+		    "wbql", perf_prio);
+    plb.add_time_avg(l_librbd_after_dequeue_op_lat, "after_dequeue_latency", "after dequeue latency",
+		    "adl", perf_prio);
+    plb.add_time_avg(l_librbd_send_lat, "send_latency", "send latency",
+		    "send", perf_prio);
+    plb.add_u64(l_librbd_rd_queue, "rqueue", "q", "q", PerfCountersBuilder::PRIO_USEFUL);
+    plb.add_u64(l_librbd_wr_queue, "wqueue", "q", "q", PerfCountersBuilder::PRIO_USEFUL);
+#endif
     plb.add_u64_counter(l_librbd_discard, "discard", "Discards");
     plb.add_u64_counter(l_librbd_discard_bytes, "discard_bytes", "Discarded data", NULL, 0, unit_t(UNIT_BYTES));
     plb.add_time_avg(l_librbd_discard_latency, "discard_latency", "Discard latency");
@@ -778,9 +790,7 @@ public:
 
     bool skip_partial_discard = true;
     ASSIGN_OPTION(non_blocking_aio, bool);
-    ASSIGN_OPTION(cache, bool);
     ASSIGN_OPTION(cache_writethrough_until_flush, bool);
-    ASSIGN_OPTION(cache_max_dirty, Option::size_t);
     ASSIGN_OPTION(sparse_read_threshold_bytes, Option::size_t);
     ASSIGN_OPTION(readahead_max_bytes, Option::size_t);
     ASSIGN_OPTION(readahead_disable_after_bytes, Option::size_t);
diff --git a/src/librbd/ImageCtx.h b/src/librbd/ImageCtx.h
index df5271e..2e8d7f0 100644
--- a/src/librbd/ImageCtx.h
+++ b/src/librbd/ImageCtx.h
@@ -174,9 +174,9 @@ namespace librbd {
 
     /// Cached latency-sensitive configuration settings
     bool non_blocking_aio;
-    bool cache;
+    bool cache = false; // bypass rbd cache feature
     bool cache_writethrough_until_flush;
-    uint64_t cache_max_dirty;
+    uint64_t cache_max_dirty = 0;
     uint64_t sparse_read_threshold_bytes;
     uint64_t readahead_max_bytes;
     uint64_t readahead_disable_after_bytes;
diff --git a/src/librbd/Operations.cc b/src/librbd/Operations.cc
index ffb8f5c..58fbe1d 100644
--- a/src/librbd/Operations.cc
+++ b/src/librbd/Operations.cc
@@ -41,6 +41,12 @@
 #include <boost/bind.hpp>
 #include <boost/scope_exit.hpp>
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#endif
+
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
 #define dout_prefix *_dout << "librbd::Operations: "
@@ -245,8 +251,21 @@ struct C_InvokeAsyncRequest : public Context {
       return;
     }
 
+#ifdef WITH_GLOBAL_CACHE
+    if (name.compare("snap_create") != 0) {
+      send_remote_request();
+    }
+#else
     send_remote_request();
+#endif
     owner_lock.put_read();
+#ifdef WITH_GLOBAL_CACHE
+    if (name.compare("snap_create") == 0) {
+      ldout(cct, 20) << __func__ << " " << name << " can not send remote request, go on to refresh image" << dendl;
+      usleep(500000);
+      send_refresh_image();
+    }
+#endif
   }
 
   void send_remote_request() {
@@ -733,6 +752,26 @@ void Operations<I>::snap_create(const cls::rbd::SnapshotNamespace &snap_namespac
   }
   m_image_ctx.snap_lock.put_read();
 
+#ifdef WITH_GLOBAL_CACHE
+  if (m_image_ctx.md_ctx.check_acc()) {
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+
+    if (!plugin || !plugin->msg_ref) {
+      lderr(cct) << "global_cache plugin not loaded, plugin=" << plugin << dendl;
+      on_finish->complete(-EINVAL);
+      return;
+    }
+
+    if (plugin->msg_ref->is_gc_snap(snap_name)) {
+      lderr(cct) << "create " << snap_name << " with prefix " << plugin->msg_ref->gc_snap_prefix()
+          << " which be used by gc internal, not allowed" << dendl;
+      on_finish->complete(-EBUSY);
+      return;
+    }
+  }
+#endif
+
   C_InvokeAsyncRequest<I> *req = new C_InvokeAsyncRequest<I>(
     m_image_ctx, "snap_create", exclusive_lock::OPERATION_REQUEST_TYPE_GENERAL,
     true,
@@ -811,6 +850,39 @@ int Operations<I>::snap_rollback(const cls::rbd::SnapshotNamespace& snap_namespa
       }
     }
 
+#ifdef WITH_GLOBAL_CACHE
+    if (m_image_ctx.md_ctx.check_acc()) {
+      PluginRegistry *reg = cct->get_plugin_registry();
+      auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+      int mgr_ret = -ELIBACC;
+      if (plugin && plugin->mgr_ref && plugin->msg_ref) {
+        int64_t md_pool_id;
+        int64_t data_pool_id;
+        md_pool_id = m_image_ctx.md_ctx.get_id();
+        if (!m_image_ctx.data_ctx.is_valid()) {
+          data_pool_id = md_pool_id;
+        } else {
+          data_pool_id = m_image_ctx.data_ctx.get_id();
+        }
+        if (plugin->msg_ref->is_gc_snap(snap_name)) {
+          lderr(cct) << snap_name << "is gc internal snap, rollback not allowed" << dendl;
+          return -EBUSY;
+        }
+        ldout(cct, 10) << " check if rollback " << md_pool_id << "/" << data_pool_id << "/" << m_image_ctx.id << dendl;
+        mgr_ret = plugin->mgr_ref->gc_is_rollbacking(md_pool_id, data_pool_id, m_image_ctx.id);
+        if (mgr_ret < 0) {
+          return mgr_ret;
+        } else if (mgr_ret == 1) {
+          lderr(cct) << " Client Adaptor: " << md_pool_id << "(" << data_pool_id << ")/" << m_image_ctx.id
+            << " GC backend is rollbacking, image another rollback not allowed, please wait." << dendl;
+          return -EBUSY;
+        }
+      } else {
+        return mgr_ret;
+      }
+    }
+#endif
+
     r = prepare_image_update(exclusive_lock::OPERATION_REQUEST_TYPE_GENERAL,
                              false);
     if (r < 0) {
@@ -913,6 +985,80 @@ void Operations<I>::snap_remove(const cls::rbd::SnapshotNamespace& snap_namespac
                    (m_image_ctx.features & RBD_FEATURE_JOURNALING) != 0);
   m_image_ctx.snap_lock.put_read();
 
+#ifdef WITH_GLOBAL_CACHE
+  if (m_image_ctx.md_ctx.check_acc()) {
+  ldout(cct, 10) << "snap_remove acc_pool" << dendl;
+
+  m_image_ctx.snap_lock.get_read();
+  uint64_t snap_id = m_image_ctx.get_snap_id(cls::rbd::UserSnapshotNamespace(), snap_name);
+  m_image_ctx.snap_lock.put_read();
+  ceph_assert(snap_id != CEPH_NOSNAP);
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  int mgr_ret = -ELIBACC;
+
+  if (plugin && plugin->mgr_ref) {
+      int64_t pool_id;
+      string name_space;
+      if (!m_image_ctx.data_ctx.is_valid()) {
+        pool_id = m_image_ctx.md_ctx.get_id();
+        name_space = m_image_ctx.md_ctx.get_namespace();
+        ldout(cct, 10) << "data ctx invalid poolId=" << pool_id << dendl;
+      } else {
+        pool_id = m_image_ctx.data_ctx.get_id();
+        name_space = m_image_ctx.data_ctx.get_namespace();
+        ldout(cct, 10) << "data ctx valid poolId=" << pool_id << dendl;
+      }
+
+      if (plugin->msg_ref->is_gc_snap(snap_name)) {
+        lderr(cct) << snap_name << "is gc internal snap, remove not allowed" << dendl;
+        on_finish->complete(-EBUSY);
+        return;
+      }
+
+      mgr_ret = plugin->mgr_ref->gc_snap_is_rollbacking(pool_id, m_image_ctx.id, snap_id);
+      if (mgr_ret < 0) {
+        on_finish->complete(mgr_ret);
+        return;
+      } else if (mgr_ret == 1) {
+        lderr(cct) << " Client Adaptor: " << pool_id << "/" << m_image_ctx.id << "@" << snap_id
+          << " GC backend is rollbacking, this snap delete not allowed, please wait." << dendl;
+        on_finish->complete(-EAGAIN);
+        return;
+      }
+
+      mgr_ret = plugin->mgr_ref->remove_snap_from_gc(pool_id, name_space, m_image_ctx.id, snap_id);
+      if (mgr_ret < 0) {
+          lderr(cct) << " " << __func__ << "remove snap failed, agent return " << mgr_ret << dendl;
+      } else {
+          bool exist = false;
+          int t = 0;
+          do {
+            if (exist) {
+                ldout(cct, 20) << "snap still exists snap_name= " << snap_name << dendl;
+                usleep(500000);   // 0.5s
+            }
+            if (t < 10) {
+              mgr_ret = snap_exists(&m_image_ctx, cls::rbd::UserSnapshotNamespace(),
+                snap_name.c_str(), &exist);
+              t++;
+            } else {
+              mgr_ret = snap_exists_with_refresh(&m_image_ctx, cls::rbd::UserSnapshotNamespace(),
+                snap_name.c_str(), &exist);
+              t = 0;
+            }
+            if (mgr_ret < 0) {
+                lderr(cct) << " " << __func__ << ": refresh failed, ret=" << mgr_ret << dendl;
+            }
+          } while(exist);
+      }
+  } else {
+      lderr(cct) << " " << __func__ << ": load plugin failed! plugin=" << plugin << dendl;
+  }
+    on_finish->complete(mgr_ret);
+    return;
+  }
+#endif
   if (proxy_op) {
     auto request_type = exclusive_lock::OPERATION_REQUEST_TYPE_GENERAL;
     if (cls::rbd::get_snap_namespace_type(snap_namespace) ==
@@ -1010,6 +1156,28 @@ int Operations<I>::snap_rename(const char *srcname, const char *dstname) {
     }
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  if (m_image_ctx.md_ctx.check_acc()) {
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+
+    if (!plugin || !plugin->msg_ref) {
+      lderr(cct) << "global_cache plugin not loaded, plugin=" << plugin << dendl;
+      return -EINVAL;
+    }
+
+    if (plugin->msg_ref->is_gc_snap(srcname)) {
+      lderr(cct) << srcname << "is gc internal snap, rename not allowed" << dendl;
+      return -EBUSY;
+    }
+
+    if (plugin->msg_ref->is_gc_snap(dstname)) {
+      lderr(cct) << dstname << "is gc internal snap, rename not allowed" << dendl;
+      return -EBUSY;
+    }
+  }
+#endif
+
   if (m_image_ctx.test_features(RBD_FEATURE_JOURNALING)) {
     r = invoke_async_request("snap_rename",
                              exclusive_lock::OPERATION_REQUEST_TYPE_GENERAL,
@@ -1111,6 +1279,23 @@ int Operations<I>::snap_protect(const cls::rbd::SnapshotNamespace& snap_namespac
     }
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  if (m_image_ctx.md_ctx.check_acc()) {
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+
+    if (!plugin || !plugin->msg_ref) {
+      lderr(cct) << "global_cache plugin not loaded, plugin=" << plugin << dendl;
+      return -EINVAL;
+    }
+
+    if (plugin->msg_ref->is_gc_snap(snap_name)) {
+      lderr(cct) << snap_name << "is gc internal snap, protect not allowed" << dendl;
+      return -EBUSY;
+    }
+  }
+#endif
+
   if (m_image_ctx.test_features(RBD_FEATURE_JOURNALING)) {
     r = invoke_async_request("snap_protect",
                              exclusive_lock::OPERATION_REQUEST_TYPE_GENERAL,
diff --git a/src/librbd/Types.h b/src/librbd/Types.h
index 3f11044..e33023f 100644
--- a/src/librbd/Types.h
+++ b/src/librbd/Types.h
@@ -22,6 +22,14 @@ enum {
   l_librbd_wr,
   l_librbd_wr_bytes,
   l_librbd_wr_latency,
+#ifdef WITH_GLOBAL_CACHE
+  l_librbd_rd_before_queue_op_lat,
+  l_librbd_wr_before_queue_op_lat,
+  l_librbd_after_dequeue_op_lat,
+  l_librbd_send_lat,
+  l_librbd_wr_queue,
+  l_librbd_rd_queue,
+#endif
   l_librbd_discard,
   l_librbd_discard_bytes,
   l_librbd_discard_latency,
diff --git a/src/librbd/api/Image.cc b/src/librbd/api/Image.cc
index 8f526f4..5f9aa77 100644
--- a/src/librbd/api/Image.cc
+++ b/src/librbd/api/Image.cc
@@ -20,6 +20,11 @@
 #include "librbd/image/PreRemoveRequest.h"
 #include <boost/scope_exit.hpp>
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#endif
+
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
 #define dout_prefix *_dout << "librbd::api::Image: " << __func__ << ": "
@@ -578,6 +583,35 @@ int Image<I>::deep_copy(I *src, librados::IoCtx& dest_md_ctx,
     return -ENOSYS;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+    if (src->md_ctx.check_acc()) {
+        PluginRegistry *reg = src->cct->get_plugin_registry();
+        auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+        int mgr_ret = -ELIBACC;
+        if (plugin && plugin->mgr_ref) {
+            int64_t md_pool_id;
+            int64_t data_pool_id;
+            md_pool_id = src->md_ctx.get_id();
+            if (!src->data_ctx.is_valid()) {
+                data_pool_id = md_pool_id;
+            } else {
+                data_pool_id = src->data_ctx.get_id();
+            }
+            ldout(src->cct, 10) << " check if rollback " << md_pool_id << "(" << data_pool_id << ")/" << src->id <<dendl;
+            mgr_ret = plugin->mgr_ref->gc_is_rollbacking(md_pool_id, data_pool_id, src->id);
+            if (mgr_ret < 0) {
+                return mgr_ret;
+            } else if (mgr_ret == 1) {
+                lderr(src->cct) << " Client Adaptor: " << md_pool_id << "(" << data_pool_id << ")/" << src->id
+                    << " GC backend is rollbacking, deep copy not allowed, please wait." << dendl;
+                return -EBUSY;
+            }
+        } else {
+            return mgr_ret;
+        }
+    }
+#endif
+
   uint64_t flatten = 0;
   if (opts.get(RBD_IMAGE_OPTION_FLATTEN, &flatten) == 0) {
     opts.unset(RBD_IMAGE_OPTION_FLATTEN);
diff --git a/src/librbd/image/PreRemoveRequest.cc b/src/librbd/image/PreRemoveRequest.cc
index 3326c14..09a6d40 100644
--- a/src/librbd/image/PreRemoveRequest.cc
+++ b/src/librbd/image/PreRemoveRequest.cc
@@ -11,6 +11,11 @@
 #include "librbd/journal/DisabledPolicy.h"
 #include "librbd/operation/SnapshotRemoveRequest.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#endif
+
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
 #define dout_prefix *_dout << "librbd::image::PreRemoveRequest: " << this \
@@ -22,6 +27,7 @@ namespace image {
 namespace {
 
 bool auto_delete_snapshot(const SnapInfo& snap_info) {
+
   auto snap_namespace_type = cls::rbd::get_snap_namespace_type(
     snap_info.snap_namespace);
   switch (snap_namespace_type) {
@@ -31,6 +37,18 @@ bool auto_delete_snapshot(const SnapInfo& snap_info) {
     return false;
   }
 }
+#ifdef WITH_GLOBAL_CACHE
+bool is_gc_snapshot(CephContext *cct, const SnapInfo& snap_info) {
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+
+  if (!plugin || !plugin->msg_ref) {
+      return false;
+  }
+
+  return plugin->msg_ref->is_gc_snap(snap_info.name);
+}
+#endif
 
 } // anonymous namespace
 
@@ -154,7 +172,11 @@ void PreRemoveRequest<I>::check_image_snaps() {
 
   m_image_ctx->snap_lock.get_read();
   for (auto& snap_info : m_image_ctx->snap_info) {
+#ifdef WITH_GLOBAL_CACHE
+    if (auto_delete_snapshot(snap_info.second) || is_gc_snapshot(cct, snap_info.second)) {
+#else
     if (auto_delete_snapshot(snap_info.second)) {
+#endif
       m_snap_infos.insert(snap_info);
     } else {
       m_image_ctx->snap_lock.put_read();
diff --git a/src/librbd/image/RemoveRequest.cc b/src/librbd/image/RemoveRequest.cc
index 8e029f8..3073a01 100644
--- a/src/librbd/image/RemoveRequest.cc
+++ b/src/librbd/image/RemoveRequest.cc
@@ -14,6 +14,10 @@
 #include "librbd/journal/RemoveRequest.h"
 #include "librbd/mirror/DisableRequest.h"
 #include "librbd/operation/TrimRequest.h"
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#endif
 
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
@@ -58,6 +62,15 @@ template<typename I>
 void RemoveRequest<I>::send() {
   ldout(m_cct, 20) << dendl;
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = m_cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  if (plugin == NULL) {
+    ldout(m_cct, 5) << "global_cache plugin not loaded" << dendl;
+    finish(-ELIBACC);
+    return;
+  }
+#endif
   open_image();
 }
 
@@ -127,10 +140,17 @@ void RemoveRequest<I>::handle_pre_remove_image(int r) {
   }
 
   if (!m_image_ctx->data_ctx.is_valid()) {
+#ifdef WITH_GLOBAL_CACHE
+    m_data_pool_id = m_image_ctx->md_ctx.get_id();
+    ldout(m_cct, 20) << " data pool not valid " << m_data_pool_id << dendl;
+#endif
     detach_child();
     return;
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  m_data_pool_id = m_image_ctx->data_ctx.get_id();
+  ldout(m_cct, 20) << " data pool valid " << m_data_pool_id << dendl;
+#endif
   trim_image();
 }
 
@@ -393,6 +413,11 @@ void RemoveRequest<I>::handle_mirror_image_remove(int r) {
   if (m_from_trash_remove) {
     // both the id object and the directory entry have been removed in
     // a previous call to trash_move.
+#ifdef WITH_GLOBAL_CACHE
+if (m_ioctx.check_acc()) {  
+    remove_from_gc();
+  }
+#endif
     finish(0);
     return;
   }
@@ -433,7 +458,9 @@ void RemoveRequest<I>::handle_remove_v1_image(int r) {
       lderr(m_cct) << "error removing image from v1 directory: "
                    << cpp_strerror(r) << dendl;
     }
-
+#ifdef WITH_GLOBAL_CACHE
+    remove_from_gc();
+#endif
     m_on_finish->complete(r);
     delete this;
     return;
@@ -587,10 +614,34 @@ void RemoveRequest<I>::handle_dir_remove_image(int r) {
     lderr(m_cct) << "error removing image from v2 directory: "
                  << cpp_strerror(r) << dendl;
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  if (m_ioctx.check_acc()) {
+    remove_from_gc();
+  }
+#endif
   finish(r);
 }
 
+#ifdef WITH_GLOBAL_CACHE
+template<typename I>
+void RemoveRequest<I>::remove_from_gc() {
+
+    ldout(m_cct, 20) << "remove_from_gc pool_id=" << m_data_pool_id
+                    << " image_id=" << m_image_id
+                     << " image_name=" << m_image_name
+                     << dendl;
+    PluginRegistry *reg = m_cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    int64_t ret = plugin->mgr_ref->remove_gc_image_resource(m_data_pool_id, m_image_id);
+    if (ret == 0) {
+        ldout(m_cct, 20) << "remove image from gc successfully, image_id=" << m_image_id << dendl;
+    } else {
+        ldout(m_cct, 5) <<"remove image from gc failed, image_id=" << m_image_id << " ret=" << ret << dendl;
+    }
+}
+#endif
+
 template<typename I>
 void RemoveRequest<I>::finish(int r) {
   ldout(m_cct, 20) << "r=" << r << dendl;
diff --git a/src/librbd/image/RemoveRequest.h b/src/librbd/image/RemoveRequest.h
index 98d5976..5b67635 100644
--- a/src/librbd/image/RemoveRequest.h
+++ b/src/librbd/image/RemoveRequest.h
@@ -96,6 +96,10 @@ private:
    * |               |                /  |
    * |               \-------<-------/   |
    * |                                   v
+   * |                            REMOVE GC SNAP INFO
+   * |                                   |
+   * |                                   |
+   * |                                   v
    * \------------------>------------<finish>
    *
    * @endverbatim
@@ -113,6 +117,9 @@ private:
   librados::IoCtx &m_ioctx;
   std::string m_image_name;
   std::string m_image_id;
+#ifdef WITH_GLOBAL_CACHE
+  int64_t m_data_pool_id;
+#endif
   ImageCtxT *m_image_ctx = nullptr;
   bool m_force;
   bool m_from_trash_remove;
@@ -187,6 +194,10 @@ private:
   void dir_remove_image();
   void handle_dir_remove_image(int r);
 
+#ifdef WITH_GLOBAL_CACHE
+  void remove_from_gc();
+#endif
+
   void finish(int r);
 };
 
diff --git a/src/librbd/internal.cc b/src/librbd/internal.cc
index 89b538d..6c0f0ba 100644
--- a/src/librbd/internal.cc
+++ b/src/librbd/internal.cc
@@ -56,6 +56,11 @@
 #include <boost/variant.hpp>
 #include "include/ceph_assert.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#endif
+
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
 #define dout_prefix *_dout << "librbd: "
@@ -1239,6 +1244,22 @@ int validate_pool(IoCtx &io_ctx, CephContext *cct) {
     return 0;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  int snap_exists_with_refresh(ImageCtx *ictx, const cls::rbd::SnapshotNamespace& snap_namespace,
+		  const char *snap_name, bool *exists)
+  {
+    ldout(ictx->cct, 20) << "snap_exists_with_refresh " << ictx << " " << snap_name << dendl;
+
+    int r = ictx->state->refresh();
+    if (r < 0)
+      return r;
+
+    RWLock::RLocker l(ictx->snap_lock);
+    *exists = ictx->get_snap_id(snap_namespace, snap_name) != CEPH_NOSNAP;
+    return 0;
+  }
+#endif
+
   int snap_remove(ImageCtx *ictx, const char *snap_name, uint32_t flags,
 		  ProgressContext& pctx)
   {
@@ -1580,7 +1601,62 @@ int validate_pool(IoCtx &io_ctx, CephContext *cct) {
       prog_ctx.update_progress(src_size, src_size);
     return r;
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  int internal_get_data_dst_addr(ImageCtx *image_ctx,
+                            uint64_t off,
+                            uint64_t *obj_offset,
+                            char *obj_addr,
+                            uint32_t *obj_id)
+  {
+    CephContext *cct = image_ctx->cct;
+    ldout(cct, 20) << " get dst data: " << image_ctx->name
+                  << " off: " << off
+                  << " format: " << image_ctx->format_string
+                  << " layout: " << image_ctx->layout << dendl;
+    std::vector<ObjectExtent> object_extents;
+    Striper::file_to_extents(cct, image_ctx->format_string, &image_ctx->layout,
+                              off, 1, 0, object_extents);
+    ceph_assert(object_extents.size() == 1);
+    string obj_name;
+    for (std::vector<ObjectExtent>::const_iterator p = object_extents.begin();
+          p != object_extents.end(); ++p) {
+      ldout(cct, 20) << "oid " << p->oid << " " << p->offset << "~" << p->length
+        << " from " << p->buffer_extents << dendl;
+      *obj_offset = p->offset;
+      obj_name = p->oid.name;
+    }
+
+    int ret = sscanf(obj_name.c_str(), image_ctx->format_string, obj_id);
+    if (ret != 1) {
+      lderr(cct) << __func__ << " parse format string error: " << ret << dendl;
+      return ret;
+    }
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    int mgr_ret = -ELIBACC;
+    if (plugin && plugin->msg_ref) {
+      uint32_t pt_id;
+      uint64_t pool_id = image_ctx->md_ctx.get_id();
+      int32_t clusterId = image_ctx->md_ctx.get_cluster_id();
+      int32_t node_id = plugin->msg_ref->get_node_id(clusterId, obj_name, pool_id, pt_id);
+      if (node_id < 0) {
+        lderr(cct) << __func__ << " Get node id failed ret " << node_id << dendl;
+        return mgr_ret;
+      }
+      string node_ip = "";
+      uint32_t mgr_ret = plugin->msg_ref->get_node_raw_ip(clusterId, node_id, node_ip);
+      if (mgr_ret) {
+        lderr(cct) << __func__ << " Get node ip failed, ret " << mgr_ret << dendl;
+        return mgr_ret;
+      }
+      ldout(cct, 20) << "node_ip" << node_ip << dendl;
+      strncpy(obj_addr, node_ip.c_str(), node_ip.length() + 1);
+    } else {
+      lderr(cct) << __func__ <<  " load plugin failed! plugin=" << plugin << dendl;
+    }
+    return mgr_ret;
+  }
+#endif
   int list_lockers(ImageCtx *ictx,
 		   std::list<locker_t> *lockers,
 		   bool *exclusive,
diff --git a/src/librbd/internal.h b/src/librbd/internal.h
index 1e1864f..b61c43f 100644
--- a/src/librbd/internal.h
+++ b/src/librbd/internal.h
@@ -98,6 +98,10 @@ namespace librbd {
   int snap_list(ImageCtx *ictx, std::vector<snap_info_t>& snaps);
   int snap_exists(ImageCtx *ictx, const cls::rbd::SnapshotNamespace& snap_namespace,
 		  const char *snap_name, bool *exists);
+#ifdef WITH_GLOBAL_CACHE
+  int snap_exists_with_refresh(ImageCtx *ictx, const cls::rbd::SnapshotNamespace& snap_namespace,
+		  const char *snap_name, bool *exists);
+#endif
   int snap_get_limit(ImageCtx *ictx, uint64_t *limit);
   int snap_set_limit(ImageCtx *ictx, uint64_t limit);
   int snap_get_timestamp(ImageCtx *ictx, uint64_t snap_id, struct timespec *timestamp);
@@ -139,6 +143,10 @@ namespace librbd {
   int64_t read_iterate(ImageCtx *ictx, uint64_t off, uint64_t len,
 		       int (*cb)(uint64_t, size_t, const char *, void *),
 		       void *arg);
+#ifdef WITH_GLOBAL_CACHE
+  int internal_get_data_dst_addr(ImageCtx *ictx, uint64_t off, uint64_t *obj_offset,
+                            char *obj_addr, uint32_t *obj_id);
+#endif
   void readahead(ImageCtx *ictx,
                  const vector<pair<uint64_t,uint64_t> >& image_extents);
 
diff --git a/src/librbd/io/ImageDispatchSpec.h b/src/librbd/io/ImageDispatchSpec.h
index 93c53a0..7551fb0 100644
--- a/src/librbd/io/ImageDispatchSpec.h
+++ b/src/librbd/io/ImageDispatchSpec.h
@@ -121,7 +121,11 @@ public:
     return new ImageDispatchSpec(image_ctx, aio_comp, {}, Flush{flush_source},
                                  0, parent_trace);
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  AioCompletion* get_completion() {
+    return m_aio_comp;
+  }
+#endif
   void send();
   void fail(int r);
 
diff --git a/src/librbd/io/ImageRequestWQ.cc b/src/librbd/io/ImageRequestWQ.cc
index 34f2c2c..cae0341 100644
--- a/src/librbd/io/ImageRequestWQ.cc
+++ b/src/librbd/io/ImageRequestWQ.cc
@@ -286,7 +286,13 @@ void ImageRequestWQ<I>::aio_read(AioCompletion *c, uint64_t off, uint64_t len,
   RWLock::RLocker owner_locker(m_image_ctx.owner_lock);
   if (m_image_ctx.non_blocking_aio || writes_blocked() || !writes_empty() ||
       require_lock_on_read()) {
+#ifdef WITH_GLOBAL_CACHE
+    ceph::timespan elapsed = coarse_mono_clock::now() - c->start_time;
+    m_image_ctx.perfcounter->tinc(l_librbd_rd_before_queue_op_lat, elapsed);
     queue(ImageDispatchSpec<I>::create_read_request(
+#else
+    queue(ImageDispatchSpec<I>::create_read_request(
+#endif
             m_image_ctx, c, {{off, len}}, std::move(read_result), op_flags,
             trace));
   } else {
@@ -325,7 +331,13 @@ void ImageRequestWQ<I>::aio_write(AioCompletion *c, uint64_t off, uint64_t len,
 
   RWLock::RLocker owner_locker(m_image_ctx.owner_lock);
   if (m_image_ctx.non_blocking_aio || writes_blocked()) {
+#ifdef WITH_GLOBAL_CACHE
+    ceph::timespan elapsed = coarse_mono_clock::now() - c->start_time;
+    m_image_ctx.perfcounter->tinc(l_librbd_wr_before_queue_op_lat, elapsed);
+    queue(ImageDispatchSpec<I>::create_write_request(
+#else
     queue(ImageDispatchSpec<I>::create_write_request(
+#endif
             m_image_ctx, c, {{off, len}}, std::move(bl), op_flags, trace));
   } else {
     c->start_op();
@@ -790,8 +802,16 @@ void ImageRequestWQ<I>::process(ImageDispatchSpec<I> *req) {
   CephContext *cct = m_image_ctx.cct;
   ldout(cct, 20) << "ictx=" << &m_image_ctx << ", "
                  << "req=" << req << dendl;
-
+#ifdef WITH_GLOBAL_CACHE
+  ceph::timespan elapsed = coarse_mono_clock::now() - req->get_completion()->start_time;
+  m_image_ctx.perfcounter->tinc(l_librbd_after_dequeue_op_lat, elapsed);
+  coarse_mono_time before_send = coarse_mono_clock::now();
+#endif
   req->send();
+#ifdef WITH_GLOBAL_CACHE
+  ceph::timespan send_time = coarse_mono_clock::now() - before_send;
+  m_image_ctx.perfcounter->tinc(l_librbd_send_lat, send_time);
+#endif
 
   finish_queued_io(req);
   if (req->is_write_op()) {
@@ -905,6 +925,10 @@ void ImageRequestWQ<I>::queue(ImageDispatchSpec<I> *req) {
   } else {
     m_queued_reads++;
   }
+#ifdef WITH_GLOBAL_CACHE
+  m_image_ctx.perfcounter->set(l_librbd_rd_queue, m_queued_reads);
+  m_image_ctx.perfcounter->set(l_librbd_wr_queue, m_queued_writes);
+#endif
 
   ThreadPool::PointerWQ<ImageDispatchSpec<I> >::queue(req);
 }
diff --git a/src/librbd/librbd.cc b/src/librbd/librbd.cc
index 749f5cb..179ef7d 100644
--- a/src/librbd/librbd.cc
+++ b/src/librbd/librbd.cc
@@ -2154,6 +2154,38 @@ namespace librbd {
     tracepoint(librbd, read_iterate2_exit, r);
     return (int)r;
   }
+#ifdef WITH_GLOBAL_CACHE
+  int Image::get_data_dst_addr(uint64_t off, uint64_t *obj_offset,
+                            char *obj_addr, uint32_t *obj_id)
+  {
+    ImageCtx *ictx = (ImageCtx *)ctx;
+    int64_t r = librbd::internal_get_data_dst_addr(ictx, off, obj_offset, obj_addr, obj_id);
+    return (int)r;
+  }
+  ssize_t Image::client_image_read(uint32_t objId, uint32_t objOffset,
+                            char *buf, size_t len)
+  {
+    ImageCtx *ictx = (ImageCtx *)ctx;
+    uint64_t ofs = (uint64_t)objId * ictx->layout.object_size + objOffset;
+    tracepoint(librbd, read_enter, ictx, ictx->name.c_str(), ictx->snap_name.c_str(), ictx->read_only, ofs, len);
+    uint32_t r = ictx->io_work_queue->read(ofs, len, librbd::io::ReadResult{buf, len},
+                        0);
+    tracepoint(librbd, read_exit, r);
+    return r;
+  }
+  ssize_t Image::client_image_aio_read(uint32_t objId, uint32_t objOffset,
+                            char *buf, size_t len, RBD::AioCompletion *c)
+  {
+    ImageCtx *ictx = (ImageCtx *)ctx;
+    uint64_t off = (uint64_t)objId * ictx->layout.object_size + objOffset;
+    tracepoint(librbd, aio_read_enter, ictx, ictx->name.c_str(), ictx->snap_name.c_str(), ictx->read_only, off, len, buf, c->pc);
+
+    ictx->io_work_queue->aio_read(get_aio_completion(c), off, len,
+                                  librbd::io::ReadResult{buf, len}, 0);
+    tracepoint(librbd, aio_read_exit, 0);
+    return 0;
+  }
+#endif
 
   int Image::diff_iterate(const char *fromsnapname,
 			  uint64_t ofs, uint64_t len,
@@ -5129,6 +5161,38 @@ extern "C" int rbd_read_iterate2(rbd_image_t image, uint64_t ofs, uint64_t len,
   tracepoint(librbd, read_iterate2_exit, r);
   return (int)r;
 }
+#ifdef WITH_GLOBAL_CACHE
+extern "C" void rbd_get_date_dts_addr(rbd_image_t image, uint64_t offset, uint64_t *objOffset,
+                          char *objAddr, uint32_t *objId)
+{
+  librbd::ImageCtx *ictx = (librbd::ImageCtx *)image;
+  librbd::internal_get_data_dst_addr(ictx, offset, objOffset, objAddr, objId);
+}
+extern "C" int rbd_client_image_read(rbd_image_t image, uint32_t objId, uint32_t objOffset,
+                          char *buf, size_t len)
+{
+  librbd::ImageCtx *ictx = (librbd::ImageCtx *)image;
+  uint64_t ofs = (uint64_t)objId * ictx->layout.object_size + objOffset;
+  tracepoint(librbd, read_enter, ictx, ictx->name.c_str(), ictx->snap_name.c_str(), ictx->read_only, ofs, len);
+  int r = ictx->io_work_queue->read(ofs, len, librbd::io::ReadResult{buf, len},
+                      0);
+  tracepoint(librbd, read_exit, r);
+  return r;
+}
+extern "C" int rbd_client_image_aio_read(rbd_image_t image, uint32_t objId, uint32_t objOffset,
+                          char *buf, size_t len, rbd_completion_t c)
+{
+  librbd::ImageCtx *ictx = (librbd::ImageCtx *)image;
+  librbd::RBD::AioCompletion *comp = (librbd::RBD::AioCompletion *)c;
+  uint64_t off = (uint64_t)objId * ictx->layout.object_size + objOffset;
+  tracepoint(librbd, aio_read_enter, ictx, ictx->name.c_str(), ictx->snap_name.c_str(), ictx->read_only, off, len, buf, comp->pc);
+
+  ictx->io_work_queue->aio_read(get_aio_completion(comp), off, len,
+                                librbd::io::ReadResult{buf, len}, 0);
+  tracepoint(librbd, aio_read_exit, 0);
+  return 0;
+}
+#endif
 
 extern "C" int rbd_diff_iterate(rbd_image_t image,
 				const char *fromsnapname,
diff --git a/src/librbd/operation/SnapshotCreateRequest.cc b/src/librbd/operation/SnapshotCreateRequest.cc
index 6629360..60250b2 100644
--- a/src/librbd/operation/SnapshotCreateRequest.cc
+++ b/src/librbd/operation/SnapshotCreateRequest.cc
@@ -11,6 +11,11 @@
 #include "librbd/Utils.h"
 #include "librbd/io/ImageRequestWQ.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#endif
+
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
 #define dout_prefix *_dout << "librbd::SnapshotCreateRequest: "
@@ -45,6 +50,16 @@ void SnapshotCreateRequest<I>::send_op() {
     return;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  if (plugin == NULL) {
+      lderr(cct) << "client adaptor plugin not loaded" << dendl;
+      this->async_complete(-ELIBACC);
+      return;
+  }
+#endif
+
   send_suspend_requests();
 }
 
@@ -104,7 +119,11 @@ void SnapshotCreateRequest<I>::send_append_op_event() {
   if (!this->template append_op_event<
         SnapshotCreateRequest<I>,
         &SnapshotCreateRequest<I>::handle_append_op_event>(this)) {
+#ifdef WITH_GLOBAL_CACHE
+    send_allocate_fake_snap_id();
+#else
     send_allocate_snap_id();
+#endif
     return;
   }
 
@@ -124,10 +143,47 @@ Context *SnapshotCreateRequest<I>::handle_append_op_event(int *result) {
                << dendl;
     return this->create_context_finisher(*result);
   }
+#ifdef WITH_GLOBAL_CACHE
+    send_allocate_fake_snap_id();
+#else
+    send_allocate_snap_id();
+#endif
+  return nullptr;
+}
+
+#ifdef WITH_GLOBAL_CACHE
+template <typename I>
+void SnapshotCreateRequest<I>::send_allocate_fake_snap_id() {
+  I &image_ctx = this->m_image_ctx;
+  CephContext *cct = image_ctx.cct;
+  ldout(cct, 5) << this << " " << __func__ << dendl;
+
+  librados::AioCompletion *rados_completion = create_rados_callback<
+    SnapshotCreateRequest<I>,
+    &SnapshotCreateRequest<I>::handle_allocate_fake_snap_id>(this);
+  image_ctx.data_ctx.aio_selfmanaged_snap_create(&m_fake_snap_id, rados_completion);
+  rados_completion->release();
+}
+
+template <typename I>
+Context *SnapshotCreateRequest<I>::handle_allocate_fake_snap_id(int *result) {
+  I &image_ctx = this->m_image_ctx;
+  CephContext *cct = image_ctx.cct;
+  ldout(cct, 5) << this << " " << __func__ << ": r=" << *result << ", "
+                << "fake snap_id=" << m_fake_snap_id << dendl;
+
+  if (*result < 0) {
+    save_result(result);
+    image_ctx.io_work_queue->unblock_writes();
+    lderr(cct) << "failed to allocate fake snapshot id: " << cpp_strerror(*result)
+               << dendl;
+    return this->create_context_finisher(*result);
+  }
 
   send_allocate_snap_id();
   return nullptr;
 }
+#endif
 
 template <typename I>
 void SnapshotCreateRequest<I>::send_allocate_snap_id() {
@@ -210,8 +266,50 @@ Context *SnapshotCreateRequest<I>::handle_create_snap(int *result) {
     return nullptr;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  if (image_ctx.md_ctx.check_acc() && !m_skip_gc) {
+    return send_add_snap_to_gc();
+  } else {
+    return send_create_object_map();
+  }
+#else
   return send_create_object_map();
+#endif
+}
+
+#ifdef WITH_GLOBAL_CACHE
+template <typename I>
+Context *SnapshotCreateRequest<I>::send_add_snap_to_gc() {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    int64_t md_pool_id = image_ctx.md_ctx.get_id();
+    int64_t data_pool_id = image_ctx.data_ctx.get_id();
+    std::string pool_name;
+    ldout(cct, 5) << this << " Client Adaptor: " << __func__
+                                                   << " md_pool_id: " << md_pool_id
+                                                   << " data_pool_id: " << data_pool_id
+                                                   << " image_id: " << image_ctx.id
+                                                   << " snap_id: " << m_snap_id
+                                                   << dendl;
+
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    int mgr_ret = -ELIBACC;
+
+    if (plugin && plugin->mgr_ref) {
+        mgr_ret = plugin->mgr_ref->add_snap_to_gc(md_pool_id, data_pool_id, image_ctx.id, m_snap_id);
+    }
+
+    if (mgr_ret < 0) {
+        lderr(cct) << __func__ << " Client Adaptor: " << "gc create snapshot failed" << dendl;
+        save_result(&mgr_ret);
+        send_release_snap();
+        return nullptr;
+    }
+    ldout(cct, 20) << __func__ << " Client Adaptor: " << "gc create snapshot success" << dendl;
+    return send_create_object_map();
 }
+#endif
 
 template <typename I>
 Context *SnapshotCreateRequest<I>::send_create_object_map() {
@@ -257,6 +355,43 @@ Context *SnapshotCreateRequest<I>::handle_create_object_map(int *result) {
   return this->create_context_finisher(0);
 }
 
+#ifdef WITH_GLOBAL_CACHE
+template <typename I>
+void SnapshotCreateRequest<I>::send_release_snap() {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << dendl;
+
+    librados::ObjectWriteOperation op;
+    if (image_ctx.old_format) {
+        cls_client::old_snapshot_remove(&op, m_snap_name);
+    } else {
+        cls_client::snapshot_remove(&op, m_snap_id);
+    }
+
+    librados::AioCompletion *rados_completion = create_rados_callback<
+        SnapshotCreateRequest<I>,
+        &SnapshotCreateRequest<I>::handle_release_snap>(this);
+    int r = image_ctx.md_ctx.aio_operate(image_ctx.header_oid,
+                                        rados_completion, &op);
+    ceph_assert(r == 0);
+    rados_completion->release();
+}
+
+template <typename I>
+Context *SnapshotCreateRequest<I>::handle_release_snap(int *result) {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << ": r=" << *result << dendl;
+
+    ceph_assert(m_ret_val < 0);
+    *result = m_ret_val;
+
+    send_release_snap_id();
+    return nullptr;
+}
+#endif
+
 template <typename I>
 void SnapshotCreateRequest<I>::send_release_snap_id() {
   I &image_ctx = this->m_image_ctx;
diff --git a/src/librbd/operation/SnapshotCreateRequest.h b/src/librbd/operation/SnapshotCreateRequest.h
index 406d2f0..ff04609 100644
--- a/src/librbd/operation/SnapshotCreateRequest.h
+++ b/src/librbd/operation/SnapshotCreateRequest.h
@@ -31,26 +31,32 @@ public:
    *           STATE_SUSPEND_REQUESTS
    *               |
    *               v
-   *           STATE_SUSPEND_AIO * * * * * * * * * * * * *
-   *               |                                     *
-   *               v                                     *
-   *           STATE_APPEND_OP_EVENT (skip if journal    *
-   *               |                  disabled)          *
-   *   (retry)     v                                     *
-   *   . . . > STATE_ALLOCATE_SNAP_ID                    *
-   *   .           |                                     *
-   *   .           v                                     *
-   *   . . . . STATE_CREATE_SNAP * * * * * * * * * *     *
-   *               |                               *     *
-   *               v                               *     *
-   *           STATE_CREATE_OBJECT_MAP (skip if    *     *
-   *               |                    disabled)  *     *
-   *               |                               *     *
-   *               |                               v     *
-   *               |              STATE_RELEASE_SNAP_ID  *
-   *               |                     |               *
-   *               |                     v               *
-   *               \----------------> <finish> < * * * * *
+   *           STATE_SUSPEND_AIO * * * * * * * * * * * * * * *
+   *               |                                         *
+   *               v                                         *
+   *           STATE_APPEND_OP_EVENT (skip if journal        *
+   *               |                  disabled)              *
+   *   (retry)     v                                         *
+   *   . . . > STATE_ALLOCATE_SNAP_ID                        *
+   *   .           |                                         *
+   *   .           v                                         *
+   *   . . . . STATE_CREATE_SNAP * * * * * * * * * * * *     *
+   *               |                                   *     *
+   *               v                                   *     *
+   *           STATE_ADD_SNAP_TO_GC  (skip if no       *     *
+   *               |                  gc)  * * * * *   *     *
+   *               v                               *   *     *
+   *           STATE_CREATE_OBJECT_MAP (skip if    *   *     *
+   *               |                    disabled)  *   *     *
+   *               |                               *   *     *
+   *               |                               v   *     *
+   *               |                STATE_RELEASE_SNAP *     *
+   *               |                               *   *     *
+   *               |                               v   v     *
+   *               |                   STATE_RELEASE_SNAP_ID *
+   *               |                     |                   *
+   *               |                     v                   *
+   *               \----------------> <finish> < * * * * * * *
    *
    * @endverbatim
    *
@@ -64,7 +70,11 @@ public:
 		        const std::string &snap_name,
 			uint64_t journal_op_tid,
                         bool skip_object_map);
-
+#ifdef WITH_GLOBAL_CACHE
+  void skip_send_to_gc() {
+    m_skip_gc = true;
+  }
+#endif
 protected:
   void send_op() override;
   bool should_complete(int r) override {
@@ -83,6 +93,10 @@ private:
   bool m_skip_object_map;
 
   int m_ret_val;
+#ifdef WITH_GLOBAL_CACHE
+  bool m_skip_gc = false;
+  uint64_t m_fake_snap_id;
+#endif
 
   uint64_t m_snap_id;
   uint64_t m_size;
@@ -97,15 +111,29 @@ private:
   void send_append_op_event();
   Context *handle_append_op_event(int *result);
 
+#ifdef WITH_GLOBAL_CACHE
+  void send_allocate_fake_snap_id();
+  Context *handle_allocate_fake_snap_id(int *result);
+#endif
+
   void send_allocate_snap_id();
   Context *handle_allocate_snap_id(int *result);
 
   void send_create_snap();
   Context *handle_create_snap(int *result);
 
+#ifdef WITH_GLOBAL_CACHE
+  Context *send_add_snap_to_gc();
+#endif
+
   Context *send_create_object_map();
   Context *handle_create_object_map(int *result);
 
+#ifdef WITH_GLOBAL_CACHE
+  void send_release_snap();
+  Context *handle_release_snap(int *result);
+#endif
+
   void send_release_snap_id();
   Context *handle_release_snap_id(int *result);
 
diff --git a/src/librbd/operation/SnapshotRollbackRequest.cc b/src/librbd/operation/SnapshotRollbackRequest.cc
index 596570a..56ee136 100644
--- a/src/librbd/operation/SnapshotRollbackRequest.cc
+++ b/src/librbd/operation/SnapshotRollbackRequest.cc
@@ -2,6 +2,8 @@
 // vim: ts=8 sw=2 smarttab
 
 #include "librbd/operation/SnapshotRollbackRequest.h"
+#include "librbd/operation/SnapshotCreateRequest.h"
+#include "librbd/operation/SnapshotRemoveRequest.h"
 #include "include/rados/librados.hpp"
 #include "common/dout.h"
 #include "common/errno.h"
@@ -15,6 +17,11 @@
 #include "osdc/Striper.h"
 #include <boost/lambda/bind.hpp>
 #include <boost/lambda/construct.hpp>
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#endif
 
 #define dout_subsys ceph_subsys_rbd
 #undef dout_prefix
@@ -252,7 +259,12 @@ void SnapshotRollbackRequest<I>::send_rollback_object_map() {
       return;
     }
   }
-
+#ifdef WITH_GLOBAL_CACHE
+    if (image_ctx.md_ctx.check_acc()) {
+        send_gc_snap_create(1);
+        return;
+    }
+#endif
   send_rollback_objects();
 }
 
@@ -270,11 +282,181 @@ Context *SnapshotRollbackRequest<I>::handle_rollback_object_map(int *result) {
     apply();
     return this->create_context_finisher(*result);
   }
-
+#ifdef WITH_GLOBAL_CACHE
+    if (image_ctx.md_ctx.check_acc()) {
+        send_gc_snap_create(1);
+        return nullptr;
+    }
+#endif
   send_rollback_objects();
   return nullptr;
 }
 
+#ifdef WITH_GLOBAL_CACHE
+template <typename I>
+void SnapshotRollbackRequest<I>::send_gc_snap_create(int num) {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << dendl;
+
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+
+    if (!plugin || !plugin->msg_ref) {
+      lderr(cct) << " " << __func__ << ": load plugin failed! plugin=" << plugin << dendl;
+      this->complete(-ELIBACC);
+      return;
+    }
+
+    string snap_name;
+    plugin->msg_ref->gen_random_gc_snap(m_snap_id, num, snap_name);
+    if (num == 1) {
+        m_gc_snap_name1 = snap_name;
+    } else {
+        m_gc_snap_name2 = snap_name;
+    }
+    ldout(cct, 20) << this << " : create snap " << snap_name << dendl;
+
+    RWLock::RLocker owner_locker(image_ctx.owner_lock);
+    Context *ctx = create_context_callback<
+        SnapshotRollbackRequest<I>,
+        &SnapshotRollbackRequest<I>::handle_send_gc_snap_create>(this);
+    SnapshotCreateRequest<I> *req = new SnapshotCreateRequest<I>(
+        image_ctx, ctx, m_snap_namespace, snap_name, 0, false);
+    req->skip_send_to_gc();
+    req->send();
+}
+
+template <typename I>
+Context *SnapshotRollbackRequest<I>::handle_send_gc_snap_create(int *result) {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << ": r=" << *result << dendl;
+    if (*result < 0) {
+        save_result(*result);
+        return send_gc_snap_remove();
+    }
+    if (m_gc_snap_name2.empty()) {
+        m_prog_ctx.update_progress(1, 100);
+        send_gc_snap_create(2);
+        return nullptr;
+    }
+    m_prog_ctx.update_progress(2, 100);
+    send_gc_rollback();
+    return nullptr;
+}
+template <typename I>
+Context *SnapshotRollbackRequest<I>::send_gc_snap_remove() {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << dendl;
+
+    string snap_name;
+    if (!m_gc_snap_name1.empty()) {
+        snap_name = m_gc_snap_name1;
+        m_gc_snap_name1 = "";
+    } else {
+        snap_name = m_gc_snap_name2;
+        m_gc_snap_name2 = "";
+    }
+
+    if (snap_name.empty()) {
+        lderr(cct) << this << " rollback failed, return " << m_ret_val << dendl;
+        this->complete(m_ret_val);
+        return nullptr;
+    }
+    RWLock::RLocker owner_locker(image_ctx.owner_lock);
+    image_ctx.snap_lock.get_read();
+    uint64_t snap_id = image_ctx.get_snap_id(m_snap_namespace, snap_name);
+    if (snap_id == CEPH_NOSNAP) {
+        ldout(cct, 20) << this << " " << __func__ << " "
+                        << snap_name << " not exists" << dendl;
+        image_ctx.snap_lock.put_read();
+        this->complete(m_ret_val);
+        return nullptr;
+    }
+    image_ctx.snap_lock.put_read();
+
+    Context *ctx = create_context_callback<
+        SnapshotRollbackRequest<I>,
+        &SnapshotRollbackRequest<I>::handle_send_gc_snap_remove>(this);
+    operation::SnapshotRemoveRequest<I> *req = new operation::SnapshotRemoveRequest<I>(
+        image_ctx, ctx, m_snap_namespace, snap_name, snap_id);
+    req->send();
+    return nullptr;
+}
+template <typename I>
+Context *SnapshotRollbackRequest<I>::handle_send_gc_snap_remove(int *result) {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << ": r=" << *result << dendl;
+    if (*result < 0) {
+        this->complete(m_ret_val);
+        return nullptr;
+    }
+
+    return send_gc_snap_remove();
+}
+
+template <typename I>
+void SnapshotRollbackRequest<I>::send_gc_rollback() {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << dendl;
+    this->m_image_ctx.op_work_queue->queue(create_context_callback<
+                                SnapshotRollbackRequest<I>,
+                                &SnapshotRollbackRequest<I>::handle_send_gc_rollback>(this), 0);
+}
+
+template <typename I>
+Context *SnapshotRollbackRequest<I>::handle_send_gc_rollback(int *result) {
+    I &image_ctx = this->m_image_ctx;
+    CephContext *cct = image_ctx.cct;
+    ldout(cct, 5) << this << " " << __func__ << dendl;
+    
+    int64_t pool_id = image_ctx.md_ctx.get_id();
+    int64_t data_pool_id = image_ctx.data_ctx.get_id();
+
+    image_ctx.snap_lock.get_read();
+    uint64_t tp_snap_id1 = image_ctx.get_snap_id(m_snap_namespace, m_gc_snap_name1);
+    uint64_t tp_snap_id2 = image_ctx.get_snap_id(m_snap_namespace, m_gc_snap_name2);
+    uint64_t num_objs = Striper::get_num_objects(image_ctx.layout, image_ctx.get_current_size());
+    image_ctx.snap_lock.put_read();
+
+    ceph_assert(tp_snap_id1 != CEPH_NOSNAP && tp_snap_id2 != CEPH_NOSNAP);
+    uint64_t snap_seq = 0;
+
+
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    int mgr_ret = -ELIBACC;
+    if (plugin && plugin->mgr_ref && plugin->msg_ref) {
+        ldout(cct, 3) << " send to gc rollback image " << pool_id << "-" << data_pool_id << "/" << image_ctx.id << dendl;
+        ldout(cct, 3) << " send to gc rollback " << m_snap_id << " => [" << tp_snap_id1 << "-" << tp_snap_id2
+                        << "]" << " seq=" << snap_seq << " num objs=" << num_objs << dendl;
+        mgr_ret = plugin->mgr_ref->rollback_gc_snap(pool_id, data_pool_id,
+                                                    image_ctx.id, num_objs, snap_seq,
+                                                    m_snap_id, tp_snap_id1, tp_snap_id2);
+        if (mgr_ret < 0) {
+            lderr(cct) << " " << __func__ << "rollback snap failed, agent return " << mgr_ret << dendl;
+        } else {
+            ldout(cct, 20) << " gc snap rollback successful! rollback " << pool_id << "/" << image_ctx.id << "@"
+                            << m_snap_id << dendl;
+        }
+    } else {
+        lderr(cct) << " " << __func__ << ": load plugin failed! plugin=" << plugin << dendl;
+    }
+    if (mgr_ret < 0) {
+        save_result(mgr_ret);
+        return send_gc_snap_remove();        
+    } else {
+        m_prog_ctx.update_progress(100, 100);
+        send_refresh_object_map();
+    }
+    return nullptr;
+}
+#endif
+
 template <typename I>
 void SnapshotRollbackRequest<I>::send_rollback_objects() {
   I &image_ctx = this->m_image_ctx;
diff --git a/src/librbd/operation/SnapshotRollbackRequest.h b/src/librbd/operation/SnapshotRollbackRequest.h
index e58a618..2638e11 100644
--- a/src/librbd/operation/SnapshotRollbackRequest.h
+++ b/src/librbd/operation/SnapshotRollbackRequest.h
@@ -83,6 +83,11 @@ private:
   uint64_t m_snap_size;
   uint64_t m_head_num_objects;
   ProgressContext &m_prog_ctx;
+#ifdef WITH_GLOBAL_CACHE
+  int m_ret_val = 0;
+  string m_gc_snap_name1;
+  string m_gc_snap_name2;
+#endif
 
   NoOpProgressContext m_no_op_prog_ctx;
 
@@ -110,7 +115,19 @@ private:
 
   Context *send_invalidate_cache();
   Context *handle_invalidate_cache(int *result);
-
+#ifdef WITH_GLOBAL_CACHE
+  void send_gc_snap_create(int num);
+  Context *handle_send_gc_snap_create(int *result);
+  Context *send_gc_snap_remove();
+  Context *handle_send_gc_snap_remove(int *result);
+  void send_gc_rollback();
+  Context *handle_send_gc_rollback(int *result);
+  void save_result(int result) {
+    if (m_ret_val == 0 && result < 0) {
+      m_ret_val = result;
+    }
+  }
+#endif
   void apply();
 };
 
diff --git a/src/msg/Message.cc b/src/msg/Message.cc
index d36a95e..e5384c3 100644
--- a/src/msg/Message.cc
+++ b/src/msg/Message.cc
@@ -202,6 +202,8 @@
 #include "messages/MOSDPGUpdateLogMissing.h"
 #include "messages/MOSDPGUpdateLogMissingReply.h"
 
+#include "msg/Messenger.h"
+
 #define DEBUGLVL  10    // debug level of output
 
 #define dout_subsys ceph_subsys_ms
@@ -932,12 +934,12 @@ void Message::decode_trace(bufferlist::const_iterator &p, bool create)
   const auto msgr = connection->get_messenger();
   const auto endpoint = msgr->get_trace_endpoint();
   if (info.trace_id) {
-    trace.init(get_type_name(), endpoint, &info, true);
+    trace.init(get_type_name().data(), endpoint, &info, true);
     trace.event("decoded trace");
   } else if (create || (msgr->get_myname().is_osd() &&
                         msgr->cct->_conf->osd_blkin_trace_all)) {
     // create a trace even if we didn't get one on the wire
-    trace.init(get_type_name(), endpoint);
+    trace.init(get_type_name().data(), endpoint);
     trace.event("created trace");
   }
   trace.keyval("tid", get_tid());
diff --git a/src/msg/async/AsyncConnection.cc b/src/msg/async/AsyncConnection.cc
index cab4145..98a69dd 100644
--- a/src/msg/async/AsyncConnection.cc
+++ b/src/msg/async/AsyncConnection.cc
@@ -360,8 +360,6 @@ void AsyncConnection::process() {
   last_active = ceph::coarse_mono_clock::now();
   recv_start_time = ceph::mono_clock::now();
 
-  ldout(async_msgr->cct, 20) << __func__ << dendl;
-
   switch (state) {
     case STATE_NONE: {
       ldout(async_msgr->cct, 20) << __func__ << " enter none state" << dendl;
@@ -758,12 +756,30 @@ void AsyncConnection::tick(uint64_t id)
   } else {
     auto idle_period = std::chrono::duration_cast<std::chrono::microseconds>
       (now - last_active).count();
+
     if (inactive_timeout_us < (uint64_t)idle_period) {
+#ifdef WITH_GLOBAL_CACHE
+      int con_port = target_addr.get_port();
+      if (con_port >= lower_port && con_port <= upper_port) {
+        ldout(async_msgr->cct, 1) << __func__ << " idle (" << idle_period
+                                << ") for more than " << inactive_timeout_us
+                                << " us, keep establish."
+                                << dendl;
+        last_tick_id = center->create_time_event(inactive_timeout_us, tick_handler);
+      } else {
+        ldout(async_msgr->cct, 1) << __func__ << " idle (" << idle_period
+                                << ") for more than " << inactive_timeout_us
+                                << " us, fault."
+                                << dendl;
+        protocol->fault();
+      }
+#else
       ldout(async_msgr->cct, 1) << __func__ << " idle (" << idle_period
                                 << ") for more than " << inactive_timeout_us
                                 << " us, fault."
                                 << dendl;
       protocol->fault();
+#endif
     } else {
       last_tick_id = center->create_time_event(inactive_timeout_us, tick_handler);
     }
diff --git a/src/msg/async/AsyncConnection.h b/src/msg/async/AsyncConnection.h
index 5b914cc..301cc89 100644
--- a/src/msg/async/AsyncConnection.h
+++ b/src/msg/async/AsyncConnection.h
@@ -136,6 +136,16 @@ class AsyncConnection : public Connection {
     return target_addr;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  uint32_t get_port_lower_boundary() const {
+    return lower_port;
+  }
+
+  uint32_t get_port_upper_boundary() const {
+    return upper_port;
+  }
+#endif
+
   int get_con_mode() const override;
 
  private:
@@ -202,6 +212,11 @@ class AsyncConnection : public Connection {
 
   entity_addr_t _infer_target_addr(const entity_addrvec_t& av);
 
+#ifdef WITH_GLOBAL_CACHE
+  const uint32_t lower_port = 7880;
+  const uint32_t upper_port = 7889;
+#endif
+
   // used only by "read_until"
   uint64_t state_offset;
   Worker *worker;
diff --git a/src/msg/async/ProtocolV2.cc b/src/msg/async/ProtocolV2.cc
index 4b03f5e..4b05d5c 100644
--- a/src/msg/async/ProtocolV2.cc
+++ b/src/msg/async/ProtocolV2.cc
@@ -13,6 +13,12 @@
 #include "auth/AuthClient.h"
 #include "auth/AuthServer.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#include <iomanip>
+#endif
+
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
 #define dout_prefix _conn_prefix(_dout)
@@ -360,7 +366,18 @@ CtPtr ProtocolV2::_fault() {
       if (backoff > cct->_conf->ms_max_backoff)
         backoff.set_from_double(cct->_conf->ms_max_backoff);
     }
-
+#ifdef WITH_GLOBAL_CACHE
+    entity_addr_t con_target_addr = connection->get_peer_socket_addr();
+    int con_port = con_target_addr.get_port();
+    uint32_t gc_lower_port = connection->get_port_lower_boundary();
+    uint32_t gc_upper_port = connection->get_port_upper_boundary();
+    if (backoff.sec() >= (__u32)trunc(cct->_conf->ms_max_backoff) && con_port >= gc_lower_port && con_port <= gc_upper_port) {
+      ldout(cct, 1) << __func__ << " reconnect timeout more reset connection con_port " << con_port << dendl;
+      stop();
+      connection->dispatch_queue->queue_reset(connection);
+      return nullptr;
+    }
+#endif
     if (server_cookie) {
       connect_seq++;
     }
@@ -1708,6 +1725,40 @@ CtPtr ProtocolV2::send_auth_request(std::vector<uint32_t> &allowed_methods) {
   vector<uint32_t> preferred_modes;
   auto am = auth_meta;
   connection->lock.unlock();
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  std::shared_lock<std::shared_mutex> rlock(plugin->msg_ref->connlock);
+  bool is_gc_conn = plugin->msg_ref->connections.find(connection) != plugin->msg_ref->connections.end();
+  rlock.unlock();
+  if (is_gc_conn) {
+    ldout(cct, 3) << __func__ << " Client Adaptor: dummy get_auth_request. " << dendl;
+    am->auth_method = CEPH_AUTH_NONE;
+    preferred_modes = { CEPH_CON_MODE_CRC };
+    connection->lock.lock();
+    if (state != AUTH_CONNECTING) {
+      ldout(cct, 1) << __func__ << " state changed!" << dendl;
+      return _fault();
+    }
+  } else {
+    int r = messenger->auth_client->get_auth_request(
+      connection, am.get(),
+      &am->auth_method, &preferred_modes, &bl);
+      connection->lock.lock();
+      if (state != AUTH_CONNECTING) {
+        ldout(cct, 1) << __func__ << " state changed!" << dendl;
+        return _fault();
+      }
+      if (r < 0) {
+        ldout(cct, 0) << __func__ << " get_initial_auth_request returned " << r
+          << dendl;
+        stop();
+        connection->dispatch_queue->queue_reset(connection);
+        return nullptr;
+      }
+  }
+#else
   int r = messenger->auth_client->get_auth_request(
     connection, am.get(),
     &am->auth_method, &preferred_modes, &bl);
@@ -1723,6 +1774,7 @@ CtPtr ProtocolV2::send_auth_request(std::vector<uint32_t> &allowed_methods) {
     connection->dispatch_queue->queue_reset(connection);
     return nullptr;
   }
+#endif
 
   INTERCEPT(9);
 
@@ -1811,6 +1863,40 @@ CtPtr ProtocolV2::handle_auth_done(ceph::bufferlist &payload)
   ceph_assert(messenger->auth_client);
   auto am = auth_meta;
   connection->lock.unlock();
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  std::shared_lock<std::shared_mutex> rlock(plugin->msg_ref->connlock);
+  bool is_gc_conn = plugin->msg_ref->connections.find(connection) != plugin->msg_ref->connections.end();
+  rlock.unlock();
+  if (is_gc_conn) {
+    ldout(cct, 3) << __func__ << " Client Adaptor: dummy handle_auth_done. " << dendl;
+    connection->lock.lock();
+    if (state != AUTH_CONNECTING) {
+      ldout(cct, 1) << __func__ << " state changed!" << dendl;
+      return _fault();
+    }
+  } else {
+    int r = messenger->auth_client->handle_auth_done(
+      connection,
+      am.get(),
+      auth_done.global_id(),
+      auth_done.con_mode(),
+      auth_done.auth_payload(),
+      &am->session_key,
+      &am->connection_secret);
+
+    connection->lock.lock();
+    if (state != AUTH_CONNECTING) {
+      ldout(cct, 1) << __func__ << " state changed!" << dendl;
+      return _fault();
+    }
+    if (r < 0) {
+      return _fault();
+    }
+  }
+#else
   int r = messenger->auth_client->handle_auth_done(
     connection,
     am.get(),
@@ -1819,6 +1905,7 @@ CtPtr ProtocolV2::handle_auth_done(ceph::bufferlist &payload)
     auth_done.auth_payload(),
     &am->session_key,
     &am->connection_secret);
+
   connection->lock.lock();
   if (state != AUTH_CONNECTING) {
     ldout(cct, 1) << __func__ << " state changed!" << dendl;
@@ -1827,6 +1914,7 @@ CtPtr ProtocolV2::handle_auth_done(ceph::bufferlist &payload)
   if (r < 0) {
     return _fault();
   }
+#endif
   auth_meta->con_mode = auth_done.con_mode();
   session_stream_handlers = \
     ceph::crypto::onwire::rxtx_t::create_handler_pair(cct, *auth_meta, false);
diff --git a/src/osdc/Objecter.cc b/src/osdc/Objecter.cc
index bc39114..c5caa96 100644
--- a/src/osdc/Objecter.cc
+++ b/src/osdc/Objecter.cc
@@ -51,6 +51,18 @@
 #include "common/errno.h"
 #include "common/EventTrace.h"
 
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#include "client_adaptor/ClientAdaptorPerf.h"
+#include "common/address_helper.h"
+#include <iomanip>
+
+#include <sys/syscall.h>
+#define gettid() syscall(__NR_gettid)
+#endif
+
 using ceph::real_time;
 using ceph::real_clock;
 
@@ -73,7 +85,11 @@ enum {
   l_osdc_op_send_bytes,
   l_osdc_op_resend,
   l_osdc_op_reply,
-
+#ifdef WITH_GLOBAL_CACHE
+  l_osdc_op_gc_retry,
+  l_osdc_op_gc_hangon,
+  l_osdc_op_gc_resend,
+#endif
   l_osdc_op,
   l_osdc_op_r,
   l_osdc_op_w,
@@ -236,6 +252,30 @@ void Objecter::init()
 {
   ceph_assert(!initialized);
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  int32_t ccm_ret = (static_cast<ClientAdaptorPlugin *>(plugin))->mgr_ref->init_mgr(this);
+  if (ccm_ret){
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Initiate manager failed ret " << ccm_ret << dendl;
+    (static_cast<ClientAdaptorPlugin *>(plugin))->mgr_ref->set_init_flag(false);
+    ceph_abort();
+  }
+  (static_cast<ClientAdaptorPlugin *>(plugin))->mgr_ref->set_init_flag(true);
+  if (cct->_conf.get_val<bool>("global_cache")) {
+    if ((static_cast<ClientAdaptorPlugin *>(plugin))->msg_ref->das_init(this)){
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " Initiate DAS failed, close prefetch" << dendl;
+    }
+  }
+  ldout(cct, 3) << __func__ << "Client Adaptor: PID: " << dec << getpid() << " TID: " << gettid() << dendl;
+  ldout(cct, 3) << __func__ << "Client Adaptor: Objecter pointer: " << hex << this << dendl;
+  if (cct->_conf.get_val<bool>("global_cache_tick")) {
+    plugin->perf_ref->start_record(plugin);
+  }
+  gc_perf = cct->_conf.get_val<bool>("gc_perf");
+#endif
+
   if (!logger) {
     PerfCountersBuilder pcb(cct, "objecter", l_osdc_first, l_osdc_last);
 
@@ -246,6 +286,14 @@ void Objecter::init()
     pcb.add_u64_counter(l_osdc_op_send_bytes, "op_send_bytes", "Sent data", NULL, 0, unit_t(UNIT_BYTES));
     pcb.add_u64_counter(l_osdc_op_resend, "op_resend", "Resent operations");
     pcb.add_u64_counter(l_osdc_op_reply, "op_reply", "Operation reply");
+#ifdef WITH_GLOBAL_CACHE
+    pcb.add_u64_counter(l_osdc_op_gc_resend, "op_gc_resend",
+            "Operation being resent to gc");
+    pcb.add_u64_counter(l_osdc_op_gc_retry, "op_gc_retry",
+            "Operation need to be retried to gc, because of error");
+    pcb.add_u64_counter(l_osdc_op_gc_hangon, "op_gc_hangon",
+            "Operation being hanging because of abnormal PT");
+#endif
 
     pcb.add_u64_counter(l_osdc_op, "op", "Operations");
     pcb.add_u64_counter(l_osdc_op_r, "op_r", "Read operations", "rd",
@@ -400,6 +448,38 @@ void Objecter::shutdown()
 {
   ceph_assert(initialized);
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  if (cct->_conf.get_val<bool>("global_cache_tick")) {
+    plugin->perf_ref->tick_done = true;
+    plugin->perf_ref->threads[0].join();
+    plugin->perf_ref->outfile.close();
+  }
+  if (plugin){
+    plugin->msg_ref->das_remove(this);
+    plugin->mgr_ref->ccm_deregister(this);
+    std::lock_guard l(reg->lock);
+    reg->remove("global_cache", "client_adaptor_plugin");
+  }
+
+  while(!retry_op.op_waiting_for_retry.empty()) {
+    map<uint32_t, std::queue<Op*>>::iterator i = retry_op.op_waiting_for_retry.begin();
+    retry_op.op_waiting_for_retry.erase(i->first);
+  }
+
+  while(!retry_op.reboot_retry_ops.empty()) {
+    map<ceph_tid_t, Op*>::iterator i = retry_op.reboot_retry_ops.begin();
+    retry_op.reboot_retry_ops.erase(i->first);
+  }
+
+  while(!retry_op.hangon_retry_submit_ops.empty()) {
+    retry_op.hangon_retry_submit_ops.pop_back();
+  }
+  ldout(cct, 3) << __func__ << "Client Adaptor: PID: " << dec << getpid() << " TID: " << gettid() << dendl;
+  ldout(cct, 3) << __func__ << "Client Adaptor: Objecter pointer: " << hex << this << dendl;
+#endif
   unique_lock wl(rwlock);
 
   initialized = false;
@@ -1062,6 +1142,15 @@ void Objecter::_scan_requests(
   while (p != s->ops.end()) {
     Op *op = p->second;
     ++p;   // check_op_pool_dne() may touch ops; prevent iterator invalidation
+#ifdef WITH_GLOBAL_CACHE
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    if (check_osd_value(s->osd) && plugin->msg_ref->filter_msg_by_op(op)) {
+        ldout(cct, 10) << " dont need to resend op here, tid=" << op->tid << ", waiting for nodeView event" << dendl;
+        continue;
+    }
+#endif
     ldout(cct, 10) << " checking op " << op->tid << dendl;
     _prune_snapc(osdmap->get_new_removed_snaps(), op);
     if (skipped_map) {
@@ -1229,17 +1318,41 @@ void Objecter::handle_osd_map(MOSDMap *m)
 	for (map<int,OSDSession*>::iterator p = osd_sessions.begin();
 	     p != osd_sessions.end(); ) {
 	  OSDSession *s = p->second;
-	  _scan_requests(s, skipped_map, cluster_full,
-			 &pool_full_map, need_resend,
-			 need_resend_linger, need_resend_command, sul,
-			 &m->gap_removed_snaps);
+#ifdef WITH_GLOBAL_CACHE
+    if (!check_osd_value(s->osd)) {
+      _scan_requests(s, skipped_map, cluster_full,
+              &pool_full_map, need_resend,
+              need_resend_linger, need_resend_command, sul,
+              &m->gap_removed_snaps);
+    }
+#else
+      _scan_requests(s, skipped_map, cluster_full,
+              &pool_full_map, need_resend,
+              need_resend_linger, need_resend_command, sul,
+              &m->gap_removed_snaps);
+
+#endif
 	  ++p;
+#ifdef WITH_GLOBAL_CACHE
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    if (check_osd_value(s->osd)) {
+      // osd means one Global Cache connection
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " bypass close osd session for 0x" << hex << s->osd << dendl;
+    } else if (!osdmap->is_up(s->osd) ||
+	      (s->con &&
+	       s->con->get_peer_addrs() != osdmap->get_addrs(s->osd))) {
+      close_session(s);
+    }
+#else
 	  // osd down or addr change?
 	  if (!osdmap->is_up(s->osd) ||
 	      (s->con &&
 	       s->con->get_peer_addrs() != osdmap->get_addrs(s->osd))) {
 	    close_session(s);
 	  }
+#endif
 	}
 
 	ceph_assert(e == osdmap->get_epoch());
@@ -1390,6 +1503,41 @@ void Objecter::consume_blacklist_events(std::set<entity_addr_t> *events)
   }
 }
 
+#ifdef WITH_GLOBAL_CACHE
+void Objecter::set_acc_pool_set(int64_t poolid, int32_t clusterId)
+{
+  acc_pool_set[poolid] = clusterId;
+}
+
+bool Objecter::get_acc_pool_set(int64_t poolid)
+{
+  return acc_pool_set.count(poolid) > 0;
+}
+
+int32_t Objecter::get_cluster_id(int64_t poolId)
+{
+  if (!acc_pool_set.count(poolId)) {
+    ldout(cct, 3) << __func__ << " incorrect call" << dendl;
+    ceph_abort();
+  }
+  return acc_pool_set[poolId];
+}
+
+bool Objecter::check_osd_value(int osd)
+{
+  if (!(osd == -1) && (osd >> 20)) {
+    return true;
+  }
+  return false;
+}
+
+int32_t Objecter::get_clusterId_from_osd(int osd)
+{
+  return (osd & 0xF0000) >> 16;
+}
+
+#endif
+
 void Objecter::emit_blacklist_events(const OSDMap::Incremental &inc)
 {
   if (!blacklist_events_enabled) {
@@ -1772,6 +1920,56 @@ int Objecter::_get_session(int osd, OSDSession **session, shunique_lock& sul)
     return 0;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  map<int,OSDSession*>::iterator p = osd_sessions.find(osd);
+
+  if (p != osd_sessions.end()) {
+    OSDSession *s = p->second;
+    s->get();
+    *session = s;
+    ldout(cct, 20) << __func__ << " s=" << s << " osd=" << osd << " "
+	        << s->get_nref() << dendl;
+    return 0;
+  }
+  if (!sul.owns_lock()) {
+    return -EAGAIN;
+  }
+
+  OSDSession *s = nullptr;
+  // osd_entry = osd;
+  if (check_osd_value(osd)) {
+    string node_ip = "";
+    int32_t clusterId = get_clusterId_from_osd(osd);
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " osd = 0x" << hex << osd << " clusterId " << clusterId << dendl;
+    uint32_t ret = plugin->msg_ref->get_node_ip(clusterId, osd, node_ip);
+    if (ret){
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " Get node ip failed, ret " << ret << dendl;
+      ceph_abort();
+    }
+
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " ip address = " << node_ip.c_str() << dendl;
+    entity_addr_t node_addr;
+    entity_addr_from_url(&node_addr, node_ip.c_str());
+    node_addr.set_type(entity_addr_t::TYPE_MSGR2);
+    entity_addrvec_t node_addrs(node_addr);
+    s = new OSDSession(cct, osd);
+    osd_sessions[osd] = s;
+    s->con = messenger->connect_to_osd(node_addrs);
+    std::unique_lock<std::shared_mutex> wlock(plugin->msg_ref->connlock);
+    plugin->msg_ref->connections.insert((void*)(s->con.get()));
+    for (auto it : plugin->msg_ref->connections) {
+      ldout(cct, 3) << "Client Adaptor: con = " << it << dendl;
+    }
+    wlock.unlock();
+  } else {
+    s = new OSDSession(cct, osd);
+    osd_sessions[osd] = s;
+    s->con = messenger->connect_to_osd(osdmap->get_addrs(osd));
+  }
+#else
   map<int,OSDSession*>::iterator p = osd_sessions.find(osd);
   if (p != osd_sessions.end()) {
     OSDSession *s = p->second;
@@ -1787,6 +1985,7 @@ int Objecter::_get_session(int osd, OSDSession **session, shunique_lock& sul)
   OSDSession *s = new OSDSession(cct, osd);
   osd_sessions[osd] = s;
   s->con = messenger->connect_to_osd(osdmap->get_addrs(osd));
+#endif
   s->con->set_priv(RefCountedPtr{s});
   logger->inc(l_osdc_osd_session_open);
   logger->set(l_osdc_osd_sessions, osd_sessions.size());
@@ -2180,7 +2379,12 @@ void Objecter::tick()
       (*i)->con->send_message(new MPing);
     }
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  RetryOp::unique_lock retry_op_rl(retry_op.retrylock);
+  ldout(cct, 2) << " tick hangon_retry_submit_ops size " <<retry_op.hangon_retry_submit_ops.size()<< dendl;
+  ldout(cct, 2) << " tick op_waiting_for_retry size " <<retry_op.op_waiting_for_retry.size()<< dendl;
+  retry_op_rl.unlock();
+#endif
   // Make sure we don't reschedule if we wake up after shutdown
   if (initialized) {
     tick_event = timer.reschedule_me(ceph::make_timespan(
@@ -2363,10 +2567,25 @@ void Objecter::_op_submit(Op *op, shunique_lock& sul, ceph_tid_t *ptid)
   // pick target
   ceph_assert(op->session == NULL);
   OSDSession *s = NULL;
+#ifdef WITH_GLOBAL_CACHE
+  bool pt_stat = true;
+  bool check_for_latest_map = _calc_pt_target(&op->target, nullptr, pt_stat)
+    == RECALC_OP_TARGET_POOL_DNE;
 
+  if (!pt_stat) {
+    unique_lock rl(retry_op.retrylock);
+    retry_op.hangon_retry_submit_ops.push_back(op);
+    rl.unlock();
+    if (gc_perf) {
+        logger->inc(l_osdc_op_gc_hangon);
+    }
+    ldout(cct, 1) << " this " << this << " " << __func__ << " op " << op << "pt unnormal, hang on IO waiting for retry by pt normal trigger " << dendl;
+    return;
+  }
+#else
   bool check_for_latest_map = _calc_target(&op->target, nullptr)
     == RECALC_OP_TARGET_POOL_DNE;
-
+#endif
   // Try to get a session, including a retry if we need to take write lock
   int r = _get_session(op->target.osd, &s, sul);
   if (r == -EAGAIN ||
@@ -2382,8 +2601,24 @@ void Objecter::_op_submit(Op *op, shunique_lock& sul, ceph_tid_t *ptid)
       // map changed; recalculate mapping
       ldout(cct, 10) << __func__ << " relock raced with osdmap, recalc target"
 		     << dendl;
+#ifdef WITH_GLOBAL_CACHE
+      check_for_latest_map = _calc_pt_target(&op->target, nullptr, pt_stat)
+        == RECALC_OP_TARGET_POOL_DNE;
+
+      if (!pt_stat) {
+        unique_lock rl(retry_op.retrylock);
+        retry_op.hangon_retry_submit_ops.push_back(op);
+        rl.unlock();
+        if (gc_perf) {
+            logger->inc(l_osdc_op_gc_hangon);
+        }
+        ldout(cct, 1) << __func__ << " op " << op << "pt unnormal, hang on IO waiting for retry by pt normal trigger " << dendl;
+        return;
+      }
+#else
       check_for_latest_map = _calc_target(&op->target, nullptr)
-	== RECALC_OP_TARGET_POOL_DNE;
+	      == RECALC_OP_TARGET_POOL_DNE;
+#endif
       if (s) {
 	put_session(s);
 	s = NULL;
@@ -2453,6 +2688,17 @@ void Objecter::_op_submit(Op *op, shunique_lock& sul, ceph_tid_t *ptid)
   _session_op_assign(s, op);
 
   if (need_send) {
+#ifdef WITH_GLOBAL_CACHE
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    if (check_osd_value(s->osd) && plugin->msg_ref->filter_msg_by_op(op)){
+      plugin->msg_ref->das_update_info(get_clusterId_from_osd(s->osd), this, op);
+      if (cct->_conf.get_val<bool>("global_cache_tick")) {
+        plugin->perf_ref->start_tick(op);
+      }
+    }
+#endif
     _send_op(op);
   }
 
@@ -2770,6 +3016,232 @@ void Objecter::_prune_snapc(
   }
 }
 
+#ifdef WITH_GLOBAL_CACHE
+int Objecter::_calc_pt_target(op_target_t *t, Connection *con, bool &pt_status, bool any_change)
+{
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+  if (get_acc_pool_set(t->base_oloc.pool) && plugin->msg_ref->filter_msg(t)){
+    t->target_oid = t->base_oid;
+    t->target_oloc = t->base_oloc;
+    ldout(cct, 3) << " Client Adaptor: " << __func__ << " msg filter pass to Global Cache" << dendl;
+    int64_t pool_id = t->base_oloc.pool;
+    int32_t clusterId = get_cluster_id(pool_id);
+    uint32_t pt_id = 0;
+    int32_t node_id = plugin->msg_ref->get_node_id(clusterId, t->base_oid.name, pool_id, pt_id);
+    if (node_id < 0) {
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " Get node id failed ret " << node_id << dendl;
+      ceph_abort();
+    }
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Send to PT " << pt_id << dendl;
+    t->actual_pgid.pgid.set_pool(t->base_oloc.pool);
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Pool ID = " << t->actual_pgid.pgid.m_pool << dendl;
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Seed = " << t->actual_pgid.pgid.m_seed << dendl;
+    t->actual_pgid.pgid.set_ps(pt_id);
+    t->osd = node_id;
+    pt_status = plugin->mgr_ref->get_pt_status(clusterId, pt_id);
+    return RECALC_OP_TARGET_NO_ACTION;
+  } else {
+    // rwlock is locked
+    bool is_read = t->flags & CEPH_OSD_FLAG_READ;
+    bool is_write = t->flags & CEPH_OSD_FLAG_WRITE;
+    t->epoch = osdmap->get_epoch();
+    ldout(cct,20) << __func__ << " epoch " << t->epoch
+      << " base " << t->base_oid << " " << t->base_oloc
+      << " precalc_pgid " << (int)t->precalc_pgid
+      << " pgid " << t->base_pgid
+      << (is_read ? " is_read" : "")
+      << (is_write ? " is_write" : "")
+      << dendl;
+
+    const pg_pool_t *pi = osdmap->get_pg_pool(t->base_oloc.pool);
+    if (!pi) {
+      t->osd = -1;
+      return RECALC_OP_TARGET_POOL_DNE;
+    }
+    ldout(cct,30) << __func__ << " base pi " << pi
+      << " pg_num " << pi->get_pg_num() << dendl;
+
+    bool force_resend = false;
+    if (osdmap->get_epoch() == pi->last_force_op_resend) {
+      if (t->last_force_resend < pi->last_force_op_resend) {
+        t->last_force_resend = pi->last_force_op_resend;
+	force_resend = true;
+      } else if (t->last_force_resend == 0) {
+        force_resend = true;
+      }
+    }
+
+    //apply tiering
+    t->target_oid = t->base_oid;
+    t->target_oloc = t->base_oloc;
+    if ((t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) {
+      if (is_read && pi->has_read_tier())
+	t->target_oloc.pool = pi->read_tier;
+      if (is_write && pi->has_write_tier())
+	t->target_oloc.pool = pi->write_tier;
+      pi = osdmap->get_pg_pool(t->target_oloc.pool);
+      if (!pi) {
+        t->osd = -1;
+	return RECALC_OP_TARGET_POOL_DNE;
+      }
+    }
+
+    pg_t pgid;
+    if (t->precalc_pgid) {
+      ceph_assert(t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY);
+      ceph_assert(t->base_oid.name.empty()); //make sure this is a pg op 
+      ceph_assert(t->base_oloc.pool == (int64_t)t->base_pgid.pool());
+      pgid = t->base_pgid;
+    } else {
+      int ret = osdmap->object_locator_to_pg(t->target_oid, t->target_oloc,
+	      pgid);
+      if (ret == -ENOENT) {
+        t->osd = -1;
+	return RECALC_OP_TARGET_POOL_DNE;
+      }
+    }
+    ldout(cct,20) << __func__ << " target " << t->target_oid << " "
+      << t->target_oloc << " -> pgid " << pgid << dendl;
+    ldout(cct,30) << __func__ << " target pi " << pi
+      <<" pg_num " << pi->get_pg_num() << dendl;
+    t->pool_ever_existed = true;
+
+    int size = pi->size;
+    int min_size = pi->min_size;
+    unsigned pg_num = pi->get_pg_num();
+    unsigned pg_num_pending = pi->get_pg_num_pending();
+    int up_primary, acting_primary;
+    vector<int> up, acting;
+    osdmap->pg_to_up_acting_osds(pgid, &up, &up_primary,
+	      &acting, &acting_primary);
+    bool sort_bitwise = osdmap->test_flag(CEPH_OSDMAP_SORTBITWISE);
+    bool recovery_deletes = osdmap->test_flag(CEPH_OSDMAP_RECOVERY_DELETES);
+    unsigned prev_seed = ceph_stable_mod(pgid.ps(), t->pg_num, t->pg_num_mask);
+    pg_t prev_pgid(prev_seed, pgid.pool());
+    if (any_change && PastIntervals::is_new_interval(
+    t->acting_primary,
+    acting_primary,
+    t->acting,
+    acting,
+    t->up_primary,
+    up_primary,
+    t->up,
+    up,
+    t->size,
+    size,
+    t->min_size,
+    min_size,
+    t->pg_num,
+    pg_num,
+    t->pg_num_pending,
+    pg_num_pending,
+    t->sort_bitwise,
+    sort_bitwise,
+    t->recovery_deletes,
+    recovery_deletes,
+    prev_pgid)) {
+      force_resend = true;
+    }
+
+    bool unpaused = false;
+    bool should_be_paused = target_should_be_paused(t);
+    if (t->paused && !should_be_paused) {
+      unpaused = true;
+    }
+    t->paused = should_be_paused;
+
+    bool legacy_change = 
+      t->pgid != pgid ||
+        is_pg_changed(
+    t->acting_primary, t->acting, acting_primary, acting,
+    t->used_replica || any_change);
+    bool split_or_merge = false;
+    if (t->pg_num) {
+      split_or_merge = 
+	prev_pgid.is_split(t->pg_num, pg_num, nullptr) ||
+	prev_pgid.is_merge_source(t->pg_num, pg_num, nullptr) ||
+	prev_pgid.is_merge_target(t->pg_num, pg_num);
+    }
+
+    if (legacy_change || split_or_merge || force_resend) {
+      t->pgid = pgid;
+      t->acting = acting;
+      t->acting_primary = acting_primary;
+      t->up_primary = up_primary;
+      t->up = up;
+      t->size = size;
+      t->min_size = min_size;
+      t->pg_num = pg_num;
+      t->pg_num_mask = pi->get_pg_num_mask();
+      t->pg_num_pending = pg_num_pending;
+      osdmap->get_primary_shard(
+	pg_t(ceph_stable_mod(pgid.ps(), t->pg_num, t->pg_num_mask), pgid.pool()),
+	&t->actual_pgid);
+      t->sort_bitwise = sort_bitwise;
+      t->recovery_deletes = recovery_deletes;
+      ldout(cct, 10) << __func__ << " "
+	<< " raw pgid " << pgid << " -> actual " << t->actual_pgid
+        << " acting " << acting
+        << " primary " << acting_primary << dendl;
+      t->used_replica = false;
+      if (acting_primary == -1) {
+        t->osd = -1;
+      } else {
+        int osd;
+	bool read = is_read && !is_write;
+	if (read && (t->flags & CEPH_OSD_FLAG_BALANCE_READS)) {
+    int p = rand() % acting.size();
+    if (p)
+      t->used_replica = true;
+    osd = acting[p];
+    ldout(cct, 10) << " chose random osd." << osd << " of " << acting
+	    << dendl;
+	} else if (read && (t->flags & CEPH_OSD_FLAG_LOCALIZE_READS) &&
+      acting.size() > 1) {
+    
+
+    int best = -1;
+    int best_locality = 0;
+    for (unsigned i = 0; i < acting.size(); ++i) {
+      int locality = osdmap->crush->get_common_ancestor_distance(
+      cct, acting[i], crush_location);
+      ldout(cct, 20) << __func__ << " localize: rank " << i
+	<< " osd." << acting[i]
+	<< " locality " << locality << dendl;
+      if (i == 0 ||
+          (locality >= 0 && best_locality >= 0 &&
+	   locality < best_locality) ||
+	   (best_locality < 0 && locality >= 0)) {
+        best = i;
+        best_locality = locality;
+        if (i)
+          t->used_replica = true;
+      }
+    }
+    ceph_assert(best >= 0);
+    osd = acting[best];
+	} else {
+    osd = acting_primary;
+	}
+	t->osd = osd;
+      }      
+    }
+    if (legacy_change || unpaused || force_resend) {
+      return RECALC_OP_TARGET_NEED_RESEND;
+    }
+    if (split_or_merge &&
+	(osdmap->require_osd_release >= CEPH_RELEASE_LUMINOUS ||
+	 HAVE_FEATURE(osdmap->get_xinfo(acting_primary).features,
+	   RESEND_ON_SPLIT))) {
+      return RECALC_OP_TARGET_NEED_RESEND;
+    }
+    return RECALC_OP_TARGET_NO_ACTION;
+  }
+}
+#endif
+
 int Objecter::_calc_target(op_target_t *t, Connection *con, bool any_change)
 {
   // rwlock is locked
@@ -2969,6 +3441,7 @@ int Objecter::_calc_target(op_target_t *t, Connection *con, bool any_change)
   return RECALC_OP_TARGET_NO_ACTION;
 }
 
+
 int Objecter::_map_session(op_target_t *target, OSDSession **s,
 			   shunique_lock& sul)
 {
@@ -3139,7 +3612,6 @@ void Objecter::_finish_op(Op *op, int r)
   }
 
   logger->dec(l_osdc_op_active);
-
   ceph_assert(check_latest_map_ops.find(op->tid) == check_latest_map_ops.end());
 
   inflight_ops--;
@@ -3271,6 +3743,38 @@ void Objecter::_send_op(Op *op)
   if (op->trace.valid()) {
     m->trace.init("op msg", nullptr, &op->trace);
   }
+
+#ifdef WITH_GLOBAL_CACHE
+  ldout(cct, 3) << "Client Adaptor: " << __func__ << " msg-type = 0x" << hex << m->get_type() << dendl;
+  if (m->get_type() == CEPH_MSG_OSD_OP) {
+    MOSDOp *mosdop = static_cast<MOSDOp *>(m);
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " MOSDOp object name = " << mosdop->get_oid().name << dendl;
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " Request ID = " << mosdop->get_reqid().tid << dendl;
+    uint32_t pt_index = mosdop->get_pg().m_seed;
+    uint64_t pool_id = mosdop->get_pg().m_pool;
+    ldout(cct, 3) << "Client Adaptor: " << __func__ << " PT ID = " << pt_index << " Pool ID = " << pool_id << dendl;
+    int index = 0;
+    for (auto op : mosdop->ops) {
+      ldout(cct, 3) << "Client Adaptor: " << __func__ << " index = " << index
+	      << " op-code = 0x" << hex << op.op.op << dendl;
+
+      index++;
+      if (op.op.op == CEPH_OSD_OP_READ || op.op.op == CEPH_OSD_OP_WRITE || op.op.op == CEPH_OSD_OP_SPARSE_READ ||
+	     op.op.op == CEPH_OSD_OP_WRITEFULL || op.op.op == CEPH_OSD_OP_SYNC_READ) {
+        ldout(cct, 3) << "Client Adaptor: " << __func__ << " offset = 0x" << hex << op.op.extent.offset
+               << " length = 0x" << hex << op.op.extent.length << dendl;
+      }
+      if (op.op.op == CEPH_OSD_OP_CALL) {
+        string cname, mname;
+        auto bp = op.indata.cbegin();
+	      bp.copy(op.op.cls.class_len, cname);
+	      bp.copy(op.op.cls.method_len, mname);
+	      ldout(cct, 3) << "Client Adaptor: " << __func__ << " op call class: " << cname << " method: " << mname << dendl;
+      }
+    }
+  }
+#endif
+
   op->session->con->send_message(m);
 }
 
@@ -3326,6 +3830,144 @@ int Objecter::take_linger_budget(LingerOp *info)
   return 1;
 }
 
+#ifdef WITH_GLOBAL_CACHE
+void Objecter::nodeview_change_retry_op_submit(int32_t clusterId, set<uint32_t> available_nodes)
+{
+    ldout(cct, 3) << "Enter NodeView Change Retry OP Submit. clusterId=" << clusterId
+      << "available_nodes=" << available_nodes << dendl;
+    map<ceph_tid_t, Op *> need_resend;
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    unique_lock sul(rwlock);
+    for (map<int, OSDSession *>::iterator p = osd_sessions.begin(); p != osd_sessions.end();) {
+        OSDSession *s = p->second;
+        int osd = p->first;
+        ldout(cct, 3) << "NodeView Change osd=" << osd << " ops=" << s->ops << dendl;
+        if (!check_osd_value(osd) ||
+            get_clusterId_from_osd(osd) != clusterId ||
+            available_nodes.find((osd & 0xFFFF) >> 4) != available_nodes.end()) {
+            ++p;
+            continue;
+        }
+        OSDSession::unique_lock sl(s->lock);
+        map<ceph_tid_t, Op *>::iterator it = s->ops.begin();
+        while (it != s->ops.end()) {
+            _session_op_remove(it->second->session, it->second);
+            need_resend[it->second->tid] = it->second;
+            it = s->ops.begin();
+        }
+        if (s->con) {
+            std::unique_lock<std::shared_mutex> wlock(plugin->msg_ref->connlock);
+            auto connection = s->con.get();
+            plugin->msg_ref->connections.erase((void *)connection);
+            wlock.unlock();
+            ldout(cct, 3) << "NodeView Change need remove connection :" << connection << dendl;
+        }
+        ++p;
+        sl.unlock();
+        close_session(s);
+    }
+
+    OSDSession *s = homeless_session;
+    OSDSession::unique_lock sl(s->lock);
+    map<ceph_tid_t, Op *>::iterator p = s->ops.begin();
+    while (p != s->ops.end()) {
+        Op *op = p->second;
+        ++p;
+        if (plugin->msg_ref->filter_msg_by_op(op)) {
+            ldout(cct, 3) << "NodeView Change: need resend homeless op: " << op->tid << dendl;
+            _session_op_remove(op->session, op);
+            need_resend[op->tid] = op;
+        }
+    }
+    sl.unlock();
+
+    sul.unlock();
+    ldout(cct, 3) << "NodeView Change Retry OP Submit: need_resend size: " << need_resend.size() << dendl;
+    for (map<ceph_tid_t, Op *>::iterator it = need_resend.begin(); it != need_resend.end(); it++) {
+        shunique_lock sul(rwlock, ceph::acquire_shared);
+        ldout(cct, 3) << "NodeView Change Retry OP Submit. TID: " << it->second->tid << dendl;
+        logger->dec(l_osdc_op_active);
+        _op_submit(it->second, sul, &(it->second->tid));
+    }
+}
+void Objecter::retry_op_submit(vector<uint32_t> ready_pt_id)
+{
+  if (!ready_pt_id.size()) {
+    ldout(cct, 3) << __func__ << "ccm pt change notify, normal pt size 0 " << dendl;
+    return;
+  }
+  std::queue<std::queue<Op*>> deal_op;
+  map<uint32_t, std::queue<Op*>>::iterator iter;
+  unique_lock rl(retry_op.retrylock);
+  for (uint32_t i = 0; i < ready_pt_id.size(); i++) {
+    iter = retry_op.op_waiting_for_retry.find(ready_pt_id[i]);
+    if (iter != retry_op.op_waiting_for_retry.end()) {
+      deal_op.push(iter->second);
+      retry_op.op_waiting_for_retry.erase(iter);
+    }
+  }
+  rl.unlock();
+  ldout(cct, 3) << __func__ << "resend op queue size " << deal_op.size() << " remain resend size " << retry_op.op_waiting_for_retry.size() << dendl;
+
+  uint32_t resend_errort_op_num = 0;
+  while(!deal_op.empty()) {
+    std::queue<Op*> op_queue = deal_op.front();
+    deal_op.pop();
+    while(!op_queue.empty()) {
+      Op* op = op_queue.front();
+      op_queue.pop();
+      if (gc_perf) {
+        logger->dec(l_osdc_op_gc_retry);
+      }
+      shunique_lock sul(rwlock, ceph::acquire_shared);
+      _op_submit(op, sul, NULL);
+      resend_errort_op_num++;
+    }
+  }
+  ldout(cct, 3) << __func__ << "resend errort op number " << resend_errort_op_num << dendl;
+
+
+  // reboot inflight ops
+
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+
+  rl.lock();
+  
+  std::queue<Op*> op_queue;
+  // hang on io retry
+  uint32_t resend_hangon_op_num = 0;
+  for (auto iter = retry_op.hangon_retry_submit_ops.begin(); iter != retry_op.hangon_retry_submit_ops.end(); ) {
+    Op *op = static_cast<Op*>(*iter);
+    uint32_t pt_id = op->target.actual_pgid.pgid.m_seed;
+    int32_t clusterId = get_cluster_id(op->target.base_oloc.pool);
+    if (plugin->mgr_ref->get_pt_status(clusterId, pt_id)) {
+      op_queue.push(op);
+      resend_hangon_op_num++;
+      retry_op.hangon_retry_submit_ops.erase(iter);
+    } else {
+      ldout(cct, 1) << __func__ << " resend hangon_retry_submit_ops pt " << pt_id << " unnormal, resend failed"<< dendl;
+      ++iter;
+    }
+  }
+  ldout(cct, 3) << __func__ << " should resend hangon op number " << resend_hangon_op_num << " left hangon size " << retry_op.hangon_retry_submit_ops.size() << dendl;
+  rl.unlock();
+
+  while (!op_queue.empty()) {
+    Op* op = op_queue.front();
+    op_queue.pop();
+    if (gc_perf) {
+        logger->dec(l_osdc_op_gc_hangon);
+    }
+    shunique_lock sul(rwlock, ceph::acquire_shared);
+    _op_submit(op, sul, NULL);
+  }
+}
+#endif
+
 /* This function DOES put the passed message before returning */
 void Objecter::handle_osd_op_reply(MOSDOpReply *m)
 {
@@ -3348,7 +3990,11 @@ void Objecter::handle_osd_op_reply(MOSDOpReply *m)
     m->put();
     return;
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ceph_assert(plugin);
+#endif
   OSDSession::unique_lock sl(s->lock);
 
   map<ceph_tid_t, Op *>::iterator iter = s->ops.find(tid);
@@ -3406,7 +4052,6 @@ void Objecter::handle_osd_op_reply(MOSDOpReply *m)
   Context *onfinish = 0;
 
   int rc = m->get_result();
-
   if (m->is_redirect_reply()) {
     ldout(cct, 5) << " got redirect reply; redirecting" << dendl;
     if (op->onfinish)
@@ -3438,13 +4083,58 @@ void Objecter::handle_osd_op_reply(MOSDOpReply *m)
     op->target.flags &= ~(CEPH_OSD_FLAG_BALANCE_READS |
 			  CEPH_OSD_FLAG_LOCALIZE_READS);
     op->target.pgid = pg_t();
+#ifdef WITH_GLOBAL_CACHE
+  if (get_acc_pool_set(op->target.target_oloc.pool)) {
+    logger->dec(l_osdc_op_active);
+  }
+#endif
     _op_submit(op, sul, NULL);
     m->put();
     return;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  if (get_acc_pool_set(op->target.target_oloc.pool) && errort_filter(rc) && m->get_oid().name.find("rbd_data") != string::npos) {
+       ldout(cct,1) << " op " << op << " error code " << rc << ", client resubmitting" << dendl;
+       if (op->onfinish)
+         num_in_flight--;
+       _session_op_remove(s, op);
+       sl.unlock();
+
+       op->tid = 0;
+       op->target.flags &= ~(CEPH_OSD_FLAG_BALANCE_READS |
+                         CEPH_OSD_FLAG_LOCALIZE_READS);
+       op->target.pgid = pg_t();
+       unique_lock rl(retry_op.retrylock);
+       std::queue<Op*> insert_op;
+       uint32_t pt_id = m->get_pg().m_seed;
+       map<uint32_t, std::queue<Op*>>::iterator iter = retry_op.op_waiting_for_retry.find(pt_id);
+       if (iter != retry_op.op_waiting_for_retry.end()) {
+               iter->second.push(op);
+       } else {
+               insert_op.push(op);
+               retry_op.op_waiting_for_retry[pt_id] = insert_op;
+       }
+       rl.unlock();
+       if (gc_perf) {
+                logger->inc(l_osdc_op_gc_retry);
+       }
+       m->put();
+       return;
+  }
+#endif
   sul.unlock();
 
+#ifdef WITH_GLOBAL_CACHE
+  if (cct->_conf.get_val<bool>("global_cache_tick") && get_acc_pool_set(op->target.target_oloc.pool)) {
+    if (plugin->msg_ref->filter_msg_by_op(op)){
+      plugin->perf_ref->end_tick(op);
+      plugin->perf_ref->record_op(op);
+      plugin->perf_ref->total_in_flight += num_in_flight;
+    }
+  }
+#endif
+
   if (op->objver)
     *op->objver = m->get_user_version();
   if (op->reply_epoch)
@@ -4399,6 +5089,53 @@ bool Objecter::ms_handle_reset(Connection *con)
     if (session) {
       ldout(cct, 1) << "ms_handle_reset " << con << " session " << session
 		    << " osd." << session->osd << dendl;
+#ifdef WITH_GLOBAL_CACHE
+  if (check_osd_value(session->osd)) {
+    PluginRegistry *reg = cct->get_plugin_registry();
+    auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+    ceph_assert(plugin);
+    OSDSession::unique_lock sl(session->lock);
+    if (session->con) {
+        std::unique_lock<std::shared_mutex> wlock(plugin->msg_ref->connlock);
+        plugin->msg_ref->connections.erase((void *)(session->con.get()));
+        wlock.unlock();
+    }
+    map<ceph_tid_t,Op*> resend;
+    for (map<ceph_tid_t,Op*>::iterator p = session->ops.begin(); p != session->ops.end(); ++p) {
+        Op *op = p->second;
+        resend[op->tid] = op;
+    }
+
+    ldout(cct, 3) << __func__ << " resend size " << resend.size()  << dendl;
+    for (map<ceph_tid_t,Op*>::iterator p = resend.begin(); p!= resend.end(); ++p ) {
+      Op *op = p->second;
+      if (op->onfinish) {
+        num_in_flight--;
+      }
+      _session_op_remove(session, op);
+    }
+    sl.unlock();
+    close_session(session);
+
+    if (!(initialized && osdmap->is_up(session->osd))) {
+      ldout(cct, 1) << "ms_handle_reset aborted,initialized=" << initialized << dendl;
+      wl.unlock();
+
+      for (map<ceph_tid_t,Op*>::iterator p = resend.begin(); p!= resend.end(); ++p ) {
+        Op *op = p->second;
+        shunique_lock sul(rwlock, ceph::acquire_shared);
+        ldout(cct, 3) << __func__ << " resend submit op " << op << " tid=" << op->tid << dendl;
+        if (gc_perf && plugin->msg_ref->filter_msg_by_op(op)) {
+          logger->dec(l_osdc_op_active);
+          logger->inc(l_osdc_op_gc_resend);
+        }
+        _op_submit(op, sul, NULL);
+      }
+
+      return false;
+    }
+  }
+#endif
       // the session maybe had been closed if new osdmap just handled
       // says the osd down
       if (!(initialized && osdmap->is_up(session->osd))) {
diff --git a/src/osdc/Objecter.h b/src/osdc/Objecter.h
index ca8d85f..ec671ad 100644
--- a/src/osdc/Objecter.h
+++ b/src/osdc/Objecter.h
@@ -258,7 +258,14 @@ struct ObjectOperation {
     }
   };
   void stat(uint64_t *psize, ceph::real_time *pmtime, int *prval) {
+#ifdef WITH_GLOBAL_CACHE
+    OSDOp& osd_op = add_op(CEPH_OSD_OP_STAT);
+    if (!psize) {
+      osd_op.op.flags = 1;
+    }
+#else
     add_op(CEPH_OSD_OP_STAT);
+#endif
     unsigned p = ops.size() - 1;
     C_ObjectOperation_stat *h = new C_ObjectOperation_stat(psize, pmtime, NULL, NULL,
 							   prval);
@@ -267,7 +274,14 @@ struct ObjectOperation {
     out_rval[p] = prval;
   }
   void stat(uint64_t *psize, time_t *ptime, int *prval) {
+#ifdef WITH_GLOBAL_CACHE
+    OSDOp& osd_op = add_op(CEPH_OSD_OP_STAT);
+    if (!psize) {
+      osd_op.op.flags = 1;
+    }
+#else
     add_op(CEPH_OSD_OP_STAT);
+#endif
     unsigned p = ops.size() - 1;
     C_ObjectOperation_stat *h = new C_ObjectOperation_stat(psize, NULL, ptime, NULL,
 							   prval);
@@ -276,7 +290,14 @@ struct ObjectOperation {
     out_rval[p] = prval;
   }
   void stat(uint64_t *psize, struct timespec *pts, int *prval) {
+#ifdef WITH_GLOBAL_CACHE
+    OSDOp& osd_op = add_op(CEPH_OSD_OP_STAT);
+    if (!psize) {
+      osd_op.op.flags = 1;
+    }
+#else
     add_op(CEPH_OSD_OP_STAT);
+#endif
     unsigned p = ops.size() - 1;
     C_ObjectOperation_stat *h = new C_ObjectOperation_stat(psize, NULL, NULL, pts,
 							   prval);
@@ -1232,7 +1253,17 @@ public:
   void maybe_request_map();
 
   void enable_blacklist_events();
+#ifdef WITH_GLOBAL_CACHE
+  void set_acc_pool_set(int64_t poolid, int32_t clusterId);
+  bool get_acc_pool_set(int64_t poolid);
+  int32_t get_cluster_id(int64_t poolId);
+  bool check_osd_value(int osd);
+  int32_t get_clusterId_from_osd(int osd);
+#endif
 private:
+#ifdef WITH_GLOBAL_CACHE
+  std::map<int64_t, int32_t> acc_pool_set;
+#endif
 
   void _maybe_request_map();
 
@@ -1341,7 +1372,13 @@ public:
     int incarnation;
 
     op_target_t target;
-
+#ifdef WITH_GLOBAL_CACHE
+    struct perf_tick_t {
+      struct timeval start;
+      struct timeval end;
+    };
+    perf_tick_t perf_tick;
+#endif
     ConnectionRef con;  // for rx buffer only
     uint64_t features;  // explicitly specified op features
 
@@ -1863,9 +1900,32 @@ public:
 
   bool osdmap_full_flag() const;
   bool osdmap_pool_full(const int64_t pool_id) const;
+#ifdef WITH_GLOBAL_CACHE
+
+
+  struct RetryOp {
+    map<uint32_t, std::queue<Op*>>op_waiting_for_retry; //error code retry
+    map<ceph_tid_t, Op*> reboot_retry_ops; //reboot retry ops
+    std::vector<Op*> hangon_retry_submit_ops; //reboot hang on ops
+    mutable std::shared_mutex retrylock;
+    using unique_lock = std::unique_lock<decltype(retrylock)>;
+  };
+
+  RetryOp retry_op;
+
+  void retry_op_submit(vector<uint32_t> ready_pt_id);
 
+  void nodeview_change_retry_op_submit(int32_t clusterId, set<uint32_t> available_nodes);
+
+  bool gc_perf;
+#endif
  private:
 
+#ifdef WITH_GLOBAL_CACHE
+  int _calc_pt_target(op_target_t *t, Connection *con,
+            bool &pt_status, bool any_change = false);
+
+#endif
   /**
    * Test pg_pool_t::FLAG_FULL on a pool
    *
@@ -2057,6 +2117,34 @@ private:
     return std::forward<Callback>(cb)(*osdmap, std::forward<Args>(args)...);
   }
 
+  bool errort_filter(errorcode32_t returnCode)
+  {
+    switch (returnCode) {
+      case -EINTR:
+      case -EBUSY:
+      case -ETXTBSY:
+      case -ENOSPC:
+      case -EDEADLK:
+      case -EWOULDBLOCK:
+      case -ETIME:
+      case -ECOMM:
+      case -ERESTART:
+      case -ENETDOWN:
+      case -ENETRESET:
+      case -ECONNRESET:
+      case -ENOBUFS:
+      case -ETIMEDOUT:
+      case -EHOSTDOWN:
+      case -EALREADY:
+      case -ENOMEDIUM:
+      case -ECANCELED:
+        return true;
+      default:
+        return false;
+    }
+    return false;
+  }
+
 
   /**
    * Tell the objecter to throttle outgoing ops according to its
@@ -2395,6 +2483,11 @@ public:
     vector<OSDOp> ops;
     int i = init_ops(ops, 1, extra_ops);
     ops[i].op.op = CEPH_OSD_OP_STAT;
+#ifdef WITH_GLOBAL_CACHE
+    if (!psize) {
+        ops[i].op.flags = 1;
+    }
+#endif
     C_Stat *fin = new C_Stat(psize, pmtime, onfinish);
     Op *o = new Op(oid, oloc, ops, flags | global_op_flags |
 		   CEPH_OSD_FLAG_READ, fin, objver);
diff --git a/src/test/CMakeLists.txt b/src/test/CMakeLists.txt
index 5dcee16..fc342cc 100644
--- a/src/test/CMakeLists.txt
+++ b/src/test/CMakeLists.txt
@@ -31,6 +31,13 @@ add_subdirectory(fs)
 add_subdirectory(journal)
 add_subdirectory(libcephfs)
 add_subdirectory(librados)
+
+# Client adaptor
+if(WITH_GLOBAL_CACHE)
+	add_subdirectory(ClientAdaptorTest)
+	add_subdirectory(ServerAdaptorSimulate)
+endif()
+
 add_subdirectory(librados_test_stub)
 if(WITH_LIBRADOSSTRIPER)
   add_subdirectory(libradosstriper)
diff --git a/src/test/ClientAdaptorTest/CMakeLists.txt b/src/test/ClientAdaptorTest/CMakeLists.txt
new file mode 100644
index 0000000..2eb494c
--- /dev/null
+++ b/src/test/ClientAdaptorTest/CMakeLists.txt
@@ -0,0 +1,11 @@
+# unittest_client_adaptor
+
+add_executable(client_adaptor_plugin_test
+  ClientAdaptorTest.cc
+  $<TARGET_OBJECTS:unit-main>
+  )
+target_link_libraries(client_adaptor_plugin_test global ${UNITTEST_LIBS} ceph_client_adaptor_plugin)
+
+# add_ceph_unittest(client_adaptor_test)
+
+message(STATUS "Client adaptor test cmake executing...")
diff --git a/src/test/ClientAdaptorTest/ClientAdaptorTest.cc b/src/test/ClientAdaptorTest/ClientAdaptorTest.cc
new file mode 100644
index 0000000..2661c62
--- /dev/null
+++ b/src/test/ClientAdaptorTest/ClientAdaptorTest.cc
@@ -0,0 +1,412 @@
+/* License:LGPL-2.1
+*
+* Copyright (c) 2021 Huawei Technologies Co., Ltd All rights reserved.
+*
+*/
+
+#include <iostream>
+#include <string.h>
+#include <iomanip>
+
+#include "gtest/gtest.h"
+#include "global/global_context.h"
+
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#include "client_adaptor/ClientAdaptorMgr.h"
+#include "client_adaptor/ClientAdaptorPerf.h"
+#include "client_adaptor/open_ccm.h"
+#include "osdc/Objecter.h"
+
+class ClientAdaptorCcmMock : public ClientAdaptorMgr{
+public:
+  ClientAdaptorCcmMock(){}
+  ~ClientAdaptorCcmMock() override {}
+  int32_t init_mgr(Objecter *obj) override {
+    std::cout << "Client Adaptor: Init CCM Mock successfully" << std::endl;
+    return 0;
+  }
+
+  int32_t get_pt_num(int32_t clusterid, uint32_t& num) override {
+    num = 32;
+    std::cout << "Client Adaptor: Get total PT number is" << num << std::endl;
+
+    return 0;
+  }
+
+  int32_t get_pt_entry(int32_t clusterid, uint32_t pt_index, PTViewPtEntry* entry) override {
+    std::cout << "Client Adaptor: PT index is" << pt_index << std::endl;
+    entry->curNodeInfo.nodeId = pt_index % 3;
+    std::cout << "Client Adaptor: PT entry master node id is" << entry->curNodeInfo.nodeId << std::endl;
+    return 0;
+  }
+
+  int32_t get_node_info(int32_t clusterid, uint32_t node_id, NodeInfo* node_info) override {
+    strcpy(node_info->publicAddrStr, "172.0.0.1");
+    node_info->ports[0] = 68;
+    node_info->portNum = 1;
+    return 0 ;
+  }
+
+    int32_t add_snap_to_gc(int64_t md_pool_id, int64_t data_pool_id, const std::string &image_id, uint64_t snap_id) {
+    return 0;
+  }
+
+  int32_t remove_snap_from_gc(int64_t data_pool_id, const std::string &name_space, 
+                              const std::string &image_id, uint64_t snap_id) {
+    return 0;
+  }
+
+  int32_t remove_gc_image_resource(const int64_t pool_id, const std::string image_id) {
+    return 0;
+  }
+
+  int32_t get_node_from_ma(const int64_t pool_id, int32_t *nodeId) {
+    return 0;
+  }
+
+  int32_t rollback_gc_snap(int64_t pool_id, int64_t data_pool_id,
+                            const std::string image_id, uint64_t num_objs, uint64_t snap_seq,
+                            uint64_t rb_snap_id, uint64_t tp_snap_id1, uint64_t tp_snap_id2) {
+    return 0;
+  }
+
+  int32_t gc_is_rollbacking(int64_t md_pool_id, int64_t data_pool_id, const std::string image_id) {
+    return 0;
+  }
+
+  int32_t gc_snap_is_rollbacking(int64_t data_pool_id, const std::string image_id, int64_t snap_id) {
+    return 0;
+  }
+
+
+  const string name() override {
+      return "ClientAdaptorCcmMock";
+  }
+  bool get_pt_status(int32_t clusterId, uint32_t pt_id) {
+    return true;
+  }
+  void ccm_deregister(Objecter *obj) {}
+};
+
+class ClientAdaptorTest : public ::testing::Test,
+	public ::testing::WithParamInterface<const char*> {
+public:
+  string plugin;
+
+  ClientAdaptorTest(){
+  }
+  ~ClientAdaptorTest() override {
+  }
+
+  void SetUp() override {
+    std::cout << "Client Adaptor: test setup" << std::endl;
+
+    return;
+  }
+  void TearDown() override {
+    std::cout << "Client Adaptor: test teardown" << std::endl;
+    PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+    lock_guard l(reg->lock);
+    reg->remove("global_cache", "client_adaptor_plugin");
+
+    return;
+  }
+};
+
+TEST_P(ClientAdaptorTest, PluginTest)
+{
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+
+  auto dirName = g_ceph_context->_conf.get_val<string>("plugin_dir");
+  std::cout << "Client Adaptor: plugin diractory name = " << dirName << std::endl;
+
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  std::cout << "Plugin address = " << plugin << std::endl;
+  
+
+  std::cout << "Client Adaptor: plugin name = " << plugin->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorPlugin", plugin->name().c_str());
+
+  auto msg=plugin->msg_ref;
+  std::cout << "Client Adaptor: msg name = " << msg->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorMsg", msg->name().c_str());
+
+  auto mgr=plugin->mgr_ref;
+  std::cout << "Client Adaptor: mgr name = " << mgr->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorCcm", mgr->name().c_str());
+
+  auto perf=plugin->perf_ref;
+  std::cout << "Client Adaptor: perf name = " << mgr->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorPerf", perf->name().c_str());
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+
+  return;
+}
+
+TEST_P(ClientAdaptorTest, PluginRegistryTest)
+{
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+
+  std::cout << "Client Adaptor: Plugin registry map before global cache insert:" << std::endl;
+  for(auto it : reg->plugins) {
+  std::cout << it.first << "---" << it.second << std::endl;
+  }
+
+  std::cout << "Client Adaptor: Plugin registry map after global cache insert:" << std::endl;
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  std::cout << "Client Adaptor: Plugin address = " << plugin << std::endl;
+  
+  for(auto it : reg->plugins) {
+  std::cout << "Client Adaptor: " << it.first << "---" << it.second << std::endl;
+  }
+
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  return;
+}
+
+TEST_P(ClientAdaptorTest, CalNodeIpNormalTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  
+  ClientAdaptorCcmMock* ccm_mock = new ClientAdaptorCcmMock();
+  ClientAdaptorMgr* mgr = plugin->msg_ref->get_mgr();
+  std::cout << "Client Adaptor: Before DI Mgr subclass is " << mgr->name() << std::endl;
+  plugin->msg_ref->set_mgr(ccm_mock);
+  mgr = plugin->msg_ref->get_mgr();
+  std::cout << "Client Adaptor: After DI Mgr subclass is " << mgr->name() << std::endl;
+  EXPECT_STREQ("ClientAdaptorCcmMock", mgr->name().c_str());
+
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  uint64_t pool_id = 3;
+  uint32_t pt_index;
+  int32_t clusterId = 0;
+  uint32_t node_id = plugin->msg_ref->get_node_id(clusterId, obj_name, pool_id, pt_index);
+  std::cout << "Client Adaptor: PT id  is " << pt_index << std::endl;
+  std::cout << "Client Adaptor: Hashed node id is 0x " << hex << node_id << std::endl;
+  EXPECT_EQ((uint32_t)0x100010, node_id);
+
+  string node_ip = "";
+  EXPECT_EQ(0, plugin->msg_ref->get_node_ip(clusterId, node_id, node_ip));
+  std::cout << "Client Adaptor: Node IP =  " << node_ip << std::endl;
+  EXPECT_STREQ("tcp://172.0.0.1:68", node_ip.c_str());
+
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  delete ccm_mock;
+  return;
+}
+
+TEST_P(ClientAdaptorTest, DasUpdateInfoTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  
+  Objecter tObj(g_ceph_context, NULL, NULL, NULL, 0, 0);
+  plugin->msg_ref->das_init(&tObj);
+  const char *cls = "hello";
+  const char *method = "say_hello";
+  string obj_name = "70ac98a7:::7a3ead19-df4f-4019-a4c2-38c52299e348.4389.1_test5";
+  bufferlist indata;
+  vector<OSDOp> nops(1);
+  OSDOp &op = nops[0];
+  int32_t clusterId = 0;
+
+  op.op.op = CEPH_OSD_OP_CALL;
+  op.op.cls.class_len = strlen(cls);
+  op.op.cls.method_len = strlen(method);
+  op.op.cls.indata_len = indata.length();
+  op.indata.append(cls, op.op.cls.class_len);
+  op.indata.append(method, op.op.cls.method_len);
+  op.indata.append(indata);
+  Objecter::Op *objecter_op =
+	  new Objecter::Op(object_t(obj_name), object_locator_t(), nops, CEPH_OSD_FLAG_EXEC, NULL, NULL ,NULL, nullptr);
+
+  plugin->msg_ref->das_update_info(clusterId, &tObj, objecter_op);
+  plugin->msg_ref->das_update_info(clusterId, &tObj, objecter_op);
+  objecter_op->target.flags = CEPH_OSD_FLAG_WRITE;
+  plugin->msg_ref->das_update_info(clusterId, &tObj, objecter_op);
+  objecter_op->target.flags = CEPH_OSD_FLAG_READ;
+  plugin->msg_ref->das_update_info(clusterId, &tObj, objecter_op);
+  objecter_op->target.base_oid.name = "rbd_data-135421846e0f-0000000000000056";
+  plugin->msg_ref->das_update_info(clusterId, &tObj, objecter_op);
+  objecter_op->target.base_oid.name = "rbd_data.135421846e0f.0000000000000056";
+  plugin->msg_ref->das_update_info(clusterId, &tObj, objecter_op);
+  OSDOp rop;
+  rop.op.op = CEPH_OSD_OP_READ;
+  rop.op.extent.offset = 10;
+  rop.op.extent.length = 4096;
+  objecter_op->ops.push_back(rop);
+  plugin->msg_ref->das_update_info(clusterId, &tObj, objecter_op);
+  objecter_op->put();
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  return;
+}
+
+extern void das_req_prefetch(DasKvParam *params);
+TEST_P(ClientAdaptorTest, DasPushTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  ClientAdaptorCcmMock* ccm_mock = new ClientAdaptorCcmMock();
+  ClientAdaptorMgr* mgr = plugin->msg_ref->get_mgr();
+
+  std::cout << "Client Adaptor: Before DI Mgr subclass is " << mgr->name() << std::endl;
+  plugin->msg_ref->set_mgr(ccm_mock);
+  mgr = plugin->msg_ref->get_mgr();
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  Objecter tObj(g_ceph_context, NULL, NULL, NULL, 0, 0);
+  DasKvParam *params = reinterpret_cast<DasKvParam*>(new char[sizeof(DasKvParam) + 22 + 1]);
+  params->offset = 2048;
+  params->len = 4194512;
+  params->opcode = 0;
+  params->timeStamp = ceph_clock_now().to_nsec();
+  params->cephPoolId = 2;
+  params->algType = DAS_ALG_SEQ;
+  params->objId = 3;
+  params->imageIdLen = 22;
+  memcpy(params->imageIdBuf, obj_name.c_str(), params->imageIdLen);
+  params->handle = plugin->msg_ref;
+  params->ctx = reinterpret_cast<Objecter *>(&tObj);
+  das_req_prefetch(params);
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  delete ccm_mock;
+  return;
+}
+TEST_P(ClientAdaptorTest, DasExitPushTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  Objecter tObj(g_ceph_context, NULL, NULL, NULL, 0, 0);
+  DasKvParam *params = reinterpret_cast<DasKvParam*>(new char[sizeof(DasKvParam) + 22 + 1]);
+  params->offset = 2048;
+  params->len = 4194512;
+  params->opcode = 0;
+  params->timeStamp = ceph_clock_now().to_nsec();
+  params->cephPoolId = 2;
+  params->algType = DAS_ALG_SEQ;
+  params->objId = 3;
+  params->imageIdLen = 22;
+  memcpy(params->imageIdBuf, obj_name.c_str(), params->imageIdLen);
+  params->handle = plugin->msg_ref;
+  params->ctx = reinterpret_cast<Objecter *>(&tObj);
+  das_req_prefetch(params);
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  das_req_prefetch(params);
+  return;
+}
+
+TEST_P(ClientAdaptorTest, MgrInitTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  ClientAdaptorCcmMock* ccm_mock = new ClientAdaptorCcmMock();
+  ClientAdaptorMgr* mgr = plugin->msg_ref->get_mgr();
+  std::cout << "Client Adaptor: Before DI Mgr subclass is " << mgr->name() << std::endl;
+  plugin->msg_ref->set_mgr(ccm_mock);
+  mgr = plugin->msg_ref->get_mgr();
+  int32_t res = mgr->init_mgr(nullptr);
+  bool init_flag = (res==0?true:false);
+  mgr->set_init_flag(init_flag);
+  bool is_succeed = mgr->is_init_succeed();
+  std::cout << "Mgr init result: " << is_succeed << std::endl;
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  delete ccm_mock;
+  return;
+}
+
+TEST_P(ClientAdaptorTest, MgrInfoTest)
+{
+
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+
+  uint32_t num = 0;
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  uint64_t pool_id = 3;
+  uint32_t pt_index;
+  int32_t clusterId = 0;
+  plugin->msg_ref->get_node_id(clusterId, obj_name, pool_id, pt_index);
+  PTViewPtEntry *entry = new PTViewPtEntry();
+  NodeInfo *node_info = new NodeInfo();
+  strcpy(node_info->ipv4AddrStr, "172.0.0.1");
+  node_info->ports[0] = 68;
+  node_info->portNum = 1;
+  plugin->mgr_ref->get_pt_num(clusterId, num);
+  plugin->mgr_ref->get_pt_entry(clusterId, pt_index, entry);
+  delete node_info;
+  delete entry;
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+}
+
+TEST_P(ClientAdaptorTest, MgrInfoTest2)
+{
+  PluginRegistry *reg = g_ceph_context->get_plugin_registry();
+  ASSERT_TRUE(reg);
+  auto plugin = dynamic_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+  ASSERT_TRUE(plugin);
+  ClientAdaptorCcm* ccm = new ClientAdaptorCcm();
+  string obj_name = "rbd_data.135421846e0f.0000000000000056";
+  uint32_t num = 0;
+  uint64_t pool_id = 3;
+  uint32_t pt_index;
+  uint32_t node_id = 2;
+  int32_t clusterId = 0;
+  plugin->msg_ref->get_node_id(clusterId, obj_name, pool_id, pt_index);
+  PTViewPtEntry *entry = new PTViewPtEntry();
+  NodeInfo *node_info = new NodeInfo();
+  int32_t res = ccm->init_mgr(nullptr);
+  int32_t res1 = ccm->get_pt_num(clusterId, num);
+  int32_t res2 = ccm->get_pt_entry(clusterId, pt_index,entry);
+  int32_t res3 = ccm->get_node_info(clusterId, node_id,node_info);
+
+  EXPECT_EQ(0, res);
+  std::cout << "Mgr init result: " << res << std::endl;
+  std::cout << "Mgr init result: " << res1 << std::endl;
+  std::cout << "Mgr init result: " << res2 << std::endl;
+  std::cout << "Mgr init result: " << res3 << std::endl;
+  delete node_info;
+  delete entry;
+  std::lock_guard l(reg->lock);
+  reg->remove("global_cache", "client_adaptor_plugin");
+  delete ccm;
+  return;
+}
+
+
+INSTANTIATE_TEST_CASE_P(
+  ClientAdaptor,
+  ClientAdaptorTest,
+  ::testing::Values("client-adaptor")
+);  
diff --git a/src/test/ServerAdaptorSimulate/CMakeLists.txt b/src/test/ServerAdaptorSimulate/CMakeLists.txt
new file mode 100644
index 0000000..547e1dd
--- /dev/null
+++ b/src/test/ServerAdaptorSimulate/CMakeLists.txt
@@ -0,0 +1,9 @@
+add_executable(global_cache_server
+  global_cache_server.cc
+  global_cache_dispatcher.cc
+  )
+target_link_libraries(global_cache_server
+   global ceph-common
+   ${EXTRALIBS} 
+   ${CMAKE_DL_LIBS} 
+)
diff --git a/src/test/ServerAdaptorSimulate/global_cache_dispatcher.cc b/src/test/ServerAdaptorSimulate/global_cache_dispatcher.cc
new file mode 100644
index 0000000..baa2d88
--- /dev/null
+++ b/src/test/ServerAdaptorSimulate/global_cache_dispatcher.cc
@@ -0,0 +1,150 @@
+#include <string>
+#include <iostream>
+#include <iomanip>
+
+#include "include/compat.h"
+#include "global_cache_dispatcher.h"
+#include "messages/MPing.h"
+#include "messages/MDataPing.h"
+#include "messages/MOSDOpReply.h"
+#include "messages/MOSDOp.h"
+
+using namespace std;
+
+GlobalCacheDispatcher::GlobalCacheDispatcher(Messenger *msgr, int32_t rval, int32_t result):
+    Dispatcher(msgr->cct),
+    active(false),
+    messenger(msgr),
+    dcount(0),
+    rval(rval),
+    result(result)
+{
+    out_data = std::make_unique<string>(8192, 'c');
+}
+
+GlobalCacheDispatcher::~GlobalCacheDispatcher() {
+    // nothing
+}
+
+bool GlobalCacheDispatcher::ms_dispatch(Message *m)
+{
+    uint64_t dc = 0;
+
+    dc = dcount++;
+
+    ConnectionRef con = m->get_connection();
+    Messenger* msgr = con->get_messenger();
+
+    switch (m->get_type()) {
+        case CEPH_MSG_PING:
+	{
+	    cout << "Client Adaptor: " << __func__ << " msg ping " << std::endl;
+	    cout << "Client Adaptor: " << __func__ << "connection = " << con << "peer_addr = "
+		 << con->get_peer_addr() << std::endl;
+	    break;
+	}
+	case MSG_DATA_PING:
+	{
+	    MDataPing* mdp __attribute__((unused)) = static_cast<MDataPing*>(m);
+	    cout << "Client Adaptor: " << __func__ << "msg data ping" << std::endl;
+	    ConnectionRef con = m->get_connection();
+	    con->send_message(m);
+	}
+	break;
+	case CEPH_MSG_OSD_OP:
+	{
+	    cout << "Client Adaptor: " << __func__ << " osd op msg" << std::endl;
+	    cout << "Client Adaptor: " << __func__ << "connection = " << con << std::endl;
+	    cout << "Client Adaptor: " << __func__ << "peer_addr = " << con->get_peer_addr() << std::endl;
+	    MOSDOp *mosd_op = static_cast<MOSDOp *>(m);
+	    mosd_op->finish_decode();
+	    cout << "Client Adaptor: " << __func__ << " MOSDOp " << *mosd_op << std::endl;
+	    cout << "Client Adaptor: " << __func__ << " pool id = " << mosd_op->get_pg().pool() << std::endl;
+	    cout << "Client Adaptor: " << __func__ << " pt id = " << mosd_op->get_pg().m_seed << std::endl;
+
+	    uint32_t index = 0;
+	    vector<OSDOp> &ops = mosd_op->ops;
+	    for (vector<OSDOp>::iterator op = ops.begin(); op != ops.end();index++, ++op) {
+	        cout << "Client Adaptor: " << __func__ << "index = " << index
+		     << " op-code = 0x" << hex << op->op.op << std::endl;
+		string cname, mname;
+		switch (op->op.op) {
+		    case CEPH_OSD_OP_READ:
+		    case CEPH_OSD_OP_SYNC_READ:
+		    case CEPH_OSD_OP_SPARSE_READ: {
+		        cout << "Client Adaptor: " << __func__ << " offset = 0x" << hex << op->op.extent.offset
+			     << "length = 0x" << hex << op->op.extent.length << std::endl;
+			if (unlikely(op->op.op == CEPH_OSD_OP_SPARSE_READ)) {
+			    std::map<uint64_t, uint64_t> extents;
+			    extents[op->op.extent.offset] = op->op.extent.length;
+			    encode(extents, op->outdata);
+			    encode(std::string_view(out_data->c_str(), op->op.extent.length), op->outdata);
+			} else {
+			    string error;
+			    cout << "Client Adaptor: " << __func__ << "read op" << std::endl;
+			    cout << "Client Adaptor: " << __func__ << "before length = 0x" << hex << op->outdata.length() << std::endl;
+			    op->outdata.read_file("/home/ceph-14.2.8/build/writefile.txt", &error);
+			    cout << "Client Adaptor: " << __func__ << " after length = 0x" << hex << op->outdata.length() << std::endl;
+			}
+			op->rval = rval;
+			cout << "Client Adaptor: " << __func__ << " op rval " << op->rval << std::endl;
+		    break;}
+		case CEPH_OSD_OP_WRITE:
+		case CEPH_OSD_OP_WRITEFULL: {
+		    cout << "Client Adaptor: " << __func__ << "offset = 0x" << hex << op->op.extent.offset
+				<< "length = 0x" << hex << op->op.extent.length << std::endl;
+		    op->indata.write_file("/home/ceph-14.2.8/build/writefile.txt");
+
+		    op->rval = rval;
+		    cout << "Client Adaptor: " << __func__ << " op rval " << op->rval << std::endl;
+		    break;}
+		case CEPH_OSD_OP_CALL:{
+		    auto bp = op->indata.cbegin();
+		    bp.copy(op->op.cls.class_len, cname);
+		    bp.copy(op->op.cls.method_len, mname);
+		    cout << "Client Adaptor: " << __func__ << " op call class: " << cname << " method: " << mname << std::endl;
+		    break;}
+		case CEPH_OSD_OP_STAT: {
+		    uint64_t psize = 0x400000;
+		    time_t ptime = 0;
+		    encode(psize, op->outdata);
+		    encode(ptime, op->outdata);
+		    break;}
+		default:
+		    break;
+		}
+	    }
+	    MOSDOpReply *reply = new MOSDOpReply(mosd_op, 0, 0, CEPH_OSD_FLAG_ACK|CEPH_OSD_FLAG_ONDISK, false);
+	    reply->claim_op_out_data(mosd_op->ops);
+	    reply->set_result(result);
+	    cout << "Client Adaptor: " << __func__ << "Return result " << reply->get_result() << std::endl;
+	    cout << "Client Adaptor: " << __func__ << "Retry time " << mosd_op->get_retry_attempt() << std::endl;
+	    cout << "Client Adaptor: " << __func__ << " MOSDOpReply " << *reply << std::endl;
+	    mosd_op->get_connection()->send_message(reply);
+	    mosd_op->put();
+	    break;
+	}
+	default:
+	    ceph_abort();
+    }
+
+    if (unlikely(msgr->get_magic() & MSG_MAGIC_TRACE_CTR)) {
+        if (unlikely(dc % 65536) == 0) {
+	    struct timespec ts;
+	    clock_gettime(CLOCK_REALTIME_COARSE, &ts);
+	    cout << "Client Adaptor: " << __func__ << " ping " << dc << "nanos: " <<
+		    ts.tv_nsec + (ts.tv_sec * 1000000000) << std::endl;
+	}
+    }
+
+    return true;
+}
+
+bool GlobalCacheDispatcher::ms_handle_reset(Connection *con)
+{
+    return true;
+}
+
+void GlobalCacheDispatcher::ms_handle_remote_reset(Connection *con)
+{
+}    // nothing
diff --git a/src/test/ServerAdaptorSimulate/global_cache_dispatcher.h b/src/test/ServerAdaptorSimulate/global_cache_dispatcher.h
new file mode 100644
index 0000000..b2ce439
--- /dev/null
+++ b/src/test/ServerAdaptorSimulate/global_cache_dispatcher.h
@@ -0,0 +1,108 @@
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#ifndef GLOBAL_CACHE_DISPATCHER_H_
+#define GLOBAL_CACHE_DISPATCHER_H_
+
+#include "msg/Dispatcher.h"
+#include "msg/Messenger.h"
+
+class GlobalCacheDispatcher: public Dispatcher {
+private:
+  bool active;
+  Messenger *messenger;
+  uint64_t dcount;
+  int32_t rval;
+  int32_t result;
+  unique_ptr<string> out_data;
+
+public:
+  GlobalCacheDispatcher(Messenger *msgr, int32_t rval, int32_t result);
+  ~GlobalCacheDispatcher() override;
+  uint64_t get_dcount() { return dcount; }
+
+  void set_active() {
+    active = true;
+  };
+
+
+  bool ms_dispatch(Message *m) override;
+
+
+
+
+
+
+
+
+  void ms_handle_connect(Connection *con) override { };
+
+
+
+
+
+
+  void ms_handle_accept(Connection *con) override { };
+
+
+
+
+
+
+
+
+
+
+
+  bool ms_handle_reset(Connection *con) override;
+
+
+
+
+
+
+
+
+
+
+  void ms_handle_remote_reset(Connection *con) override;
+
+  bool ms_handle_refused(Connection *con) override { return false; }
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+  
+  bool ms_get_authorizer(int dest_type, AuthAuthorizer **a) override {
+    return false;
+  };
+
+  int ms_handle_authentication(Connection *con) override {
+    return 1;
+  }
+};
+
+#endif
+
diff --git a/src/test/ServerAdaptorSimulate/global_cache_server.cc b/src/test/ServerAdaptorSimulate/global_cache_server.cc
new file mode 100644
index 0000000..486825a
--- /dev/null
+++ b/src/test/ServerAdaptorSimulate/global_cache_server.cc
@@ -0,0 +1,131 @@
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+#include <sys/types.h>
+
+#include <iostream>
+#include <string>
+
+using namespace std;
+
+#include "common/config.h"
+#include "msg/Messenger.h"
+#include "common/Timer.h"
+#include "common/ceph_argparse.h"
+#include "global/global_init.h"
+#include "global/signal_handler.h"
+#include "perfglue/heap_profiler.h"
+#include "common/address_helper.h"
+#include "global_cache_dispatcher.h"
+#include "auth/DummyAuth.h"
+#include "msg/async/AsyncMessenger.h"
+
+#define dout_subsys ceph_subsys_global_cache
+
+void usage(ostream& out)
+{
+  out << "usage: global_cache_server [options]\n"
+    "options:\n"
+    "--rval -ErrorCode\n"
+        "--result -ErrorCode\n"
+	;
+}
+
+int main(int argc, const char **argv)
+{
+    vector<const char*> args;
+    Messenger *messenger;
+    Dispatcher *dispatcher;
+    vector<const char*>::iterator arg_iter;
+    string val;
+    int32_t rval = 0;
+    int32_t result = 0;
+    entity_addr_t bind_addr; 
+
+    string addr = "localhost";
+    string port = "1234";
+
+    cout << "Client Adaptor: " << __func__ << " Global Cache Server starting..." << std::endl;
+
+    argv_to_vec(argc, argv, args);
+
+    auto cct = global_init(NULL, args, CEPH_ENTITY_TYPE_ANY,
+		    CODE_ENVIRONMENT_DAEMON,
+		    CINIT_FLAG_NO_DEFAULT_CONFIG_FILE);
+
+    for (arg_iter = args.begin(); arg_iter != args.end();) {
+      if (ceph_argparse_witharg(args, arg_iter, &val, "--rval", (char*) NULL)){
+        rval = atoi(val.c_str());
+      } else if (ceph_argparse_witharg(args, arg_iter, &val, "--result", (char*) NULL)){
+        result = atoi(val.c_str());
+      } else {
+      ++arg_iter;
+      }
+    };
+
+    if (!args.empty()) {
+      cerr << "What is this? -- " << args[0] << std::endl;
+      usage(cerr);
+      exit(1);
+    }
+
+    string dest_str = "tcp://";
+    dest_str += addr;
+    dest_str += ":";
+    dest_str += port;
+    entity_addr_from_url(&bind_addr, dest_str.c_str());
+  entity_addrvec_t bind_addrs(bind_addr);
+
+  string ms_type = g_conf().get_val<std::string>("ms_type");
+  cout << "Client Adaptor: " << __func__ << " ms_type: " << ms_type << std::endl;
+    messenger = Messenger::create(g_ceph_context, ms_type,
+		    entity_name_t::OSD(-1),
+		    "global_cache_server",
+		    0 /* nonce */,
+		    0 /* flags */);
+
+    DummyAuthClientServer dummy_auth(g_ceph_context);
+  dummy_auth.auth_registry.refresh_config();
+  messenger->set_auth_client(&dummy_auth);
+    messenger->set_auth_server(&dummy_auth);
+    messenger->set_magic(MSG_MAGIC_TRACE_CTR);
+    messenger->set_default_policy(Messenger::Policy::stateless_server(0));
+
+
+  bind_addr.set_type(entity_addr_t::TYPE_MSGR2);
+    int32_t r = messenger->bind(bind_addr);
+    if(r < 0)
+       goto out;
+
+
+
+    common_init_finish(g_ceph_context);
+
+    dispatcher = new GlobalCacheDispatcher(messenger, rval, result);
+    dispatcher->ms_set_require_authorizer(false);
+
+    messenger->add_dispatcher_head(dispatcher);
+    messenger->start();
+
+  cout << "Client Adaptor: " << __func__ << " server conn = "
+	    << static_cast<AsyncMessenger *>(messenger)->lookup_conn(bind_addrs) << std::endl;
+    messenger->wait();
+
+
+    delete messenger;
+
+out:
+    cout << "Client Adaptor: " << __func__ << " Simple Server exit" << std::endl;
+    return r;
+}
diff --git a/src/test/librbd/io/test_mock_ImageRequestWQ.cc b/src/test/librbd/io/test_mock_ImageRequestWQ.cc
index 50daa83..209740a 100644
--- a/src/test/librbd/io/test_mock_ImageRequestWQ.cc
+++ b/src/test/librbd/io/test_mock_ImageRequestWQ.cc
@@ -58,6 +58,11 @@ struct ImageDispatchSpec<librbd::MockTestImageCtx> {
     return s_instance;
   }
 
+#ifdef WITH_GLOBAL_CACHE
+  AioCompletion* get_completion() {
+    return aio_comp;
+  }
+#endif
   MOCK_CONST_METHOD0(is_write_op, bool());
   MOCK_CONST_METHOD0(start_op, void());
   MOCK_CONST_METHOD0(send, void());
diff --git a/src/test/librbd/operation/test_mock_SnapshotRollbackRequest.cc b/src/test/librbd/operation/test_mock_SnapshotRollbackRequest.cc
index 1727180..ec72706 100644
--- a/src/test/librbd/operation/test_mock_SnapshotRollbackRequest.cc
+++ b/src/test/librbd/operation/test_mock_SnapshotRollbackRequest.cc
@@ -11,6 +11,8 @@
 #include "librbd/ImageState.h"
 #include "librbd/internal.h"
 #include "librbd/operation/SnapshotRollbackRequest.h"
+#include "librbd/operation/SnapshotCreateRequest.h"
+#include "librbd/operation/SnapshotRemoveRequest.h"
 #include "gmock/gmock.h"
 #include "gtest/gtest.h"
 
@@ -52,6 +54,60 @@ struct ResizeRequest<MockOperationImageCtx> {
 
 ResizeRequest<MockOperationImageCtx> *ResizeRequest<MockOperationImageCtx>::s_instance = nullptr;
 
+template <>
+struct SnapshotCreateRequest<MockOperationImageCtx> {
+  static SnapshotCreateRequest *sc_instance;
+  Context *on_finish = nullptr;
+
+  static SnapshotCreateRequest* create(MockOperationImageCtx &image_ctx,
+                               Context *on_finish,
+                               const cls::rbd::SnapshotNamespace &snap_namespace,
+                               const std::string &snap_name,
+                               uint64_t journal_op_tid,
+                               bool skip_object_map) {
+    sc_instance->on_finish = on_finish;
+    return sc_instance;
+  }
+
+  SnapshotCreateRequest(MockOperationImageCtx &image_ctx,
+                               Context *on_finish,
+                               const cls::rbd::SnapshotNamespace &snap_namespace,
+                               const std::string &snap_name,
+                               uint64_t journal_op_tid,
+                               bool skip_object_map) {
+    sc_instance = this;
+  }
+
+  void skip_send_to_gc() {}
+
+  MOCK_METHOD0(send, void());
+};
+
+SnapshotCreateRequest<MockOperationImageCtx> *SnapshotCreateRequest<MockOperationImageCtx>::sc_instance = nullptr;
+
+template <>
+struct SnapshotRemoveRequest<MockOperationImageCtx> {
+  static SnapshotRemoveRequest *sr_instance;
+  Context *on_finish = nullptr;
+
+  static SnapshotRemoveRequest* create(MockOperationImageCtx &image_ctx, Context *on_finish,
+                               const cls::rbd::SnapshotNamespace &snap_namespace,
+                               const std::string &snap_name, uint64_t snap_id) {
+    sr_instance->on_finish = on_finish;
+    return sr_instance;
+  }
+
+  SnapshotRemoveRequest(MockOperationImageCtx &image_ctx, Context *on_finish,
+                               const cls::rbd::SnapshotNamespace &snap_namespace,
+                               const std::string &snap_name, uint64_t snap_id) {
+    sr_instance = this;
+  }
+
+  MOCK_METHOD0(send, void());
+};
+
+SnapshotRemoveRequest<MockOperationImageCtx> *SnapshotRemoveRequest<MockOperationImageCtx>::sr_instance = nullptr;
+
 } // namespace operation
 
 template <>
@@ -83,6 +139,8 @@ class TestMockOperationSnapshotRollbackRequest : public TestMockFixture {
 public:
   typedef SnapshotRollbackRequest<MockOperationImageCtx> MockSnapshotRollbackRequest;
   typedef ResizeRequest<MockOperationImageCtx> MockResizeRequest;
+  typedef SnapshotCreateRequest<MockOperationImageCtx> MockSnapshotCreateRequest;
+  typedef SnapshotRemoveRequest<MockOperationImageCtx> MockSnapshotRemoveRequest;
 
   void expect_block_writes(MockOperationImageCtx &mock_image_ctx, int r) {
     EXPECT_CALL(*mock_image_ctx.io_work_queue, block_writes(_))
diff --git a/src/test/messenger/simple_client.cc b/src/test/messenger/simple_client.cc
index ba7ed2b..8b1eee7 100644
--- a/src/test/messenger/simple_client.cc
+++ b/src/test/messenger/simple_client.cc
@@ -30,6 +30,7 @@ using namespace std;
 #include "common/address_helper.h"
 #include "message_helper.h"
 #include "simple_dispatcher.h"
+#include "auth/DummyAuth.h"
 
 #define dout_subsys ceph_subsys_simple_client
 
@@ -59,7 +60,7 @@ int main(int argc, const char **argv)
 	std::string addr = "localhost";
 	std::string port = "1234";
 
-	int n_msgs = 50;
+	int n_msgs = 1;
 	int n_dsize = 0;
 
 	struct timespec ts;
@@ -101,12 +102,20 @@ int main(int argc, const char **argv)
 	  "dest port " << port << " " <<
 	  "initial msgs (pipe depth) " << n_msgs << " " <<
 	  "data buffer size " << n_dsize << std::endl;
-
-	messenger = Messenger::create(g_ceph_context, g_conf().get_val<std::string>("ms_type"),
-				      entity_name_t::MON(-1),
+             g_ceph_context->_conf.set_val("auth_cluster_required", "none");
+             g_ceph_context->_conf.set_val("auth_service_required", "none");
+             g_ceph_context->_conf.set_val("auth_client_required", "none");
+	     string ms_type = g_conf().get_val<std::string>("ms_type");
+	     cout << "Client Adaptor: ms_type: " << ms_type << std::endl;
+	messenger = Messenger::create(g_ceph_context, ms_type,
+ 				      entity_name_t::CLIENT(-1),
 				      "client",
 				      getpid(), 0);
 
+	DummyAuthClientServer dummy_auth(g_ceph_context);
+        	dummy_auth.auth_registry.refresh_config();
+	messenger->set_auth_client(&dummy_auth);
+		messenger->set_auth_server(&dummy_auth);
 	// enable timing prints
 	messenger->set_magic(MSG_MAGIC_TRACE_CTR);
 	messenger->set_default_policy(Messenger::Policy::lossy_client(0));
@@ -115,10 +124,16 @@ int main(int argc, const char **argv)
 	dest_str += addr;
 	dest_str += ":";
 	dest_str += port;
+		cout << "Client Adaptor: address = " << dest_str << std::endl;
 	entity_addr_from_url(&dest_addr, dest_str.c_str());
+	//dest_addr.set_type(entity_addr_t::TYPE_LEGACY);
+	dest_addr.set_type(entity_addr_t::TYPE_MSGR2);
+		cout << "Client Adaptor: legacy address = " << dest_addr.get_legacy_str() << std::endl;
 	entity_addrvec_t dest_addrs(dest_addr);
+		cout << "Client Adaptor: vec legacy address = " << dest_addrs.get_legacy_str() << std::endl;
 
 	dispatcher = new SimpleDispatcher(messenger);
+	dispatcher->ms_set_require_authorizer(false);
 	messenger->add_dispatcher_head(dispatcher);
 
 	dispatcher->set_active(); // this side is the pinger
@@ -127,7 +142,12 @@ int main(int argc, const char **argv)
 	if (r < 0)
 		goto out;
 
-	conn = messenger->connect_to_mon(dest_addrs);
+	conn = messenger->connect_to_osd(dest_addrs);
+		std::cout << "Client Adaptor: conn = " << conn << std::endl;
+
+		while (!conn->is_connected()) {
+			nanosleep(&ts, NULL);
+		}
 
 	// do stuff
 	time_t t1, t2;
@@ -144,6 +164,12 @@ int main(int argc, const char **argv)
 	    m = new_simple_ping_with_data("simple_client", n_dsize);
 	  }
 	  conn->send_message(m);
+		std::cout << "Client Adaptor: client message conn = " << m->get_connection() << std::endl;
+		std::cout << "Client Adaptor: peer_addr = " << conn->get_peer_addr() << std::endl;
+		entity_addrvec_t peer_addr_vec = *(m->get_connection()->peer_addrs);
+		for (auto it : peer_addr_vec.v) {
+			std::cout << "Client Adaptor: peer_addr = " << it.get_legacy_str() << std::endl;
+		}	
 	}
 
 	// do stuff
diff --git a/src/test/messenger/simple_dispatcher.cc b/src/test/messenger/simple_dispatcher.cc
index b13958d..7e1011b 100644
--- a/src/test/messenger/simple_dispatcher.cc
+++ b/src/test/messenger/simple_dispatcher.cc
@@ -17,6 +17,8 @@
 #include "simple_dispatcher.h"
 #include "messages/MPing.h"
 #include "messages/MDataPing.h"
+#include "messages/MOSDOpReply.h"
+#include "messages/MOSDOp.h"
 
 SimpleDispatcher::SimpleDispatcher(Messenger *msgr) :
   Dispatcher(msgr->cct),
@@ -42,16 +44,37 @@ bool SimpleDispatcher::ms_dispatch(Message *m)
 
   switch (m->get_type()) {
   case CEPH_MSG_PING:
+  {
+    std::cout << "Client Adaptor: msg ping " << std::endl;  
+    std::cout << "Client Adaptor: conn =  " << con << "peer_addr = " << con->get_peer_addr() <<  std::endl;  
+
+
+
+
     break;
+  }
   case MSG_DATA_PING:
   {
     MDataPing* mdp __attribute__((unused)) = static_cast<MDataPing*>(m);
+    std::cout << "Client Adaptor: msg data ping " << std::endl;  
     //cout << "MDataPing " << mdp->tag << " " << mdp->counter << std::endl;
     //mdp->get_data().hexdump(cout);
     ConnectionRef con = m->get_connection();
     con->send_message(m);
   }
     break;
+  case CEPH_MSG_OSD_OP:
+  {
+    std::cout << "Client Adaptor: osd op msg " << std::endl;  
+    std::cout << "Client Adaptor: conn = " << con << std::endl;  
+    std::cout << "Client Adaptor: peer_addr = " << con->get_peer_addr() << std::endl;  
+    MOSDOp *osd_op = static_cast<MOSDOp*>(m);
+    MOSDOpReply *reply = new MOSDOpReply(osd_op, 0, 0, 0, false);
+    std::cout << "Client Adaptor: connection " << m->get_connection() << std::endl;  
+     m->get_connection()->send_message(reply);
+     m->put();
+     break;
+   }
   default:
     ceph_abort();
   }
@@ -66,7 +89,7 @@ bool SimpleDispatcher::ms_dispatch(Message *m)
   } /* trace ctr */
 
 
-  con->send_message(m);
+  //con->send_message(m);
 
   //m->put();
 
diff --git a/src/test/messenger/simple_server.cc b/src/test/messenger/simple_server.cc
index 8b85f3a..34feff1 100644
--- a/src/test/messenger/simple_server.cc
+++ b/src/test/messenger/simple_server.cc
@@ -28,6 +28,8 @@ using namespace std;
 #include "perfglue/heap_profiler.h"
 #include "common/address_helper.h"
 #include "simple_dispatcher.h"
+#include "auth/DummyAuth.h"
+#include "msg/async/AsyncMessenger.h"
 
 #define dout_subsys ceph_subsys_simple_server
 
@@ -72,17 +74,26 @@ int main(int argc, const char **argv)
 	dest_str += ":";
 	dest_str += port;
 	entity_addr_from_url(&bind_addr, dest_str.c_str());
+		entity_addrvec_t bind_addrs(bind_addr);
 
-	messenger = Messenger::create(g_ceph_context, g_conf().get_val<std::string>("ms_type"),
-				      entity_name_t::MON(-1),
+		string ms_type = g_conf().get_val<std::string>("ms_type");
+		cout << "Client Adaptor: ms_type: " << ms_type << std::endl;
+	messenger = Messenger::create(g_ceph_context, ms_type,
+				      entity_name_t::OSD(-1),
 				      "simple_server",
 				      0 /* nonce */,
 				      0 /* flags */);
 	// enable timing prints
+	DummyAuthClientServer dummy_auth(g_ceph_context);
+        	dummy_auth.auth_registry.refresh_config();
+		messenger->set_auth_client(&dummy_auth);
+	messenger->set_auth_server(&dummy_auth);
 	messenger->set_magic(MSG_MAGIC_TRACE_CTR);
 	messenger->set_default_policy(
 	  Messenger::Policy::stateless_server(0));
 
+
+		bind_addr.set_type(entity_addr_t::TYPE_MSGR2);
 	r = messenger->bind(bind_addr);
 	if (r < 0)
 		goto out;
@@ -92,9 +103,12 @@ int main(int argc, const char **argv)
 	common_init_finish(g_ceph_context);
 
 	dispatcher = new SimpleDispatcher(messenger);
+	dispatcher->ms_set_require_authorizer(false);
 
 	messenger->add_dispatcher_head(dispatcher); // should reach ready()
 	messenger->start();
+
+	std::cout << "Client Adaptor: server conn = " << static_cast<AsyncMessenger *>(messenger)->lookup_conn(bind_addrs) << std::endl;
 	messenger->wait(); // can't be called until ready()
 
 	// done
diff --git a/src/tools/rbd/action/Snap.cc b/src/tools/rbd/action/Snap.cc
index 70cf62d..efec5f8 100644
--- a/src/tools/rbd/action/Snap.cc
+++ b/src/tools/rbd/action/Snap.cc
@@ -12,6 +12,10 @@
 #include <iostream>
 #include <boost/program_options.hpp>
 #include <boost/bind.hpp>
+#ifdef WITH_GLOBAL_CACHE
+#include "client_adaptor/ClientAdaptorPlugin.h"
+#include "client_adaptor/ClientAdaptorMsg.h"
+#endif
 
 namespace rbd {
 namespace action {
@@ -21,8 +25,11 @@ static const std::string ALL_NAME("all");
 
 namespace at = argument_types;
 namespace po = boost::program_options;
-
+#ifdef WITH_GLOBAL_CACHE
+int do_list_snaps(librados::IoCtx& io_ctx, librbd::Image& image, Formatter *f, bool all_snaps, librados::Rados& rados)
+#else
 int do_list_snaps(librbd::Image& image, Formatter *f, bool all_snaps, librados::Rados& rados)
+#endif
 {
   std::vector<librbd::snap_info_t> snaps;
   TextTable t;
@@ -58,8 +65,24 @@ int do_list_snaps(librbd::Image& image, Formatter *f, bool all_snaps, librados::
   rados.pool_list2(pool_list);
   std::map<int64_t, std::string> pool_map(pool_list.begin(), pool_list.end());
 
+#ifdef WITH_GLOBAL_CACHE
+  auto cct = reinterpret_cast<CephContext*>(rados.cct());
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+
+  if (!plugin || !plugin->msg_ref) {
+    std::cerr << "global_cache plugin not loaded, plugin=" << plugin << std::endl;
+    return -EINVAL;
+  }
+#endif
+
   for (std::vector<librbd::snap_info_t>::iterator s = snaps.begin();
        s != snaps.end(); ++s) {
+#ifdef WITH_GLOBAL_CACHE
+    if (!all_snaps && plugin->msg_ref->is_gc_snap(s->name) && io_ctx.check_acc()) {
+      continue;
+    }
+#endif
     struct timespec timestamp;
     bool snap_protected = false;
     image.snap_get_timestamp(s->id, &timestamp);
@@ -204,12 +227,25 @@ int do_rollback_snap(librbd::Image& image, const char *snapname,
   pc.finish();
   return 0;
 }
-
+#ifdef WITH_GLOBAL_CACHE
+int do_purge_snaps(librados::IoCtx& io_ctx, librbd::Image& image, bool no_progress)
+#else
 int do_purge_snaps(librbd::Image& image, bool no_progress)
+#endif
 {
   utils::ProgressContext pc("Removing all snapshots", no_progress);
   std::vector<librbd::snap_info_t> snaps;
   bool is_protected = false;
+#ifdef WITH_GLOBAL_CACHE
+  auto cct = reinterpret_cast<CephContext*>(io_ctx.cct());
+  PluginRegistry *reg = cct->get_plugin_registry();
+  auto plugin = static_cast<ClientAdaptorPlugin *>(reg->get_with_load("global_cache", "client_adaptor_plugin"));
+
+  if (!plugin || !plugin->msg_ref) {
+    std::cerr << "global_cache plugin not loaded, plugin=" << plugin << std::endl;
+    return -EINVAL;
+  }
+#endif
   int r = image.snap_list(snaps);
   if (r < 0) {
     pc.fail();
@@ -230,6 +266,10 @@ int do_purge_snaps(librbd::Image& image, bool no_progress)
       } else if (is_protected == true) {
         protect.push_back(it->name.c_str());
         snaps.erase(it);
+#ifdef WITH_GLOBAL_CACHE
+      } else if (plugin->msg_ref->is_gc_snap(it->name) && io_ctx.check_acc()) {
+        snaps.erase(it);
+#endif
       } else {
         ++it;
       }
@@ -339,7 +379,11 @@ int execute_list(const po::variables_map &vm,
   }
 
   bool all_snaps = vm[ALL_NAME].as<bool>();
+#ifdef WITH_GLOBAL_CACHE
+  r = do_list_snaps(io_ctx, image, formatter.get(), all_snaps, rados);
+#else
   r = do_list_snaps(image, formatter.get(), all_snaps, rados);
+#endif
   if (r < 0) {
     cerr << "rbd: failed to list snapshots: " << cpp_strerror(r)
          << std::endl;
@@ -530,8 +574,11 @@ int execute_purge(const po::variables_map &vm,
   if (r < 0) {
     return r;
   }
-
+#ifdef WITH_GLOBAL_CACHE
+  r = do_purge_snaps(io_ctx, image, vm[at::NO_PROGRESS].as<bool>());
+#else
   r = do_purge_snaps(image, vm[at::NO_PROGRESS].as<bool>());
+#endif
   if (r < 0) {
     if (r != -EBUSY) {
       std::cerr << "rbd: removing snaps failed: " << cpp_strerror(r)
diff --git a/src/vstart.sh b/src/vstart.sh
index 37aa28b..a146de3 100644
--- a/src/vstart.sh
+++ b/src/vstart.sh
@@ -752,6 +752,8 @@ EOF
             local uuid=`uuidgen`
             echo "add osd$osd $uuid"
 	    OSD_SECRET=$($CEPH_BIN/ceph-authtool --gen-print-key)
+
+	    OSD_SECRET=`echo ${OSD_SECRET} | awk '{print $NF}'`
 	    echo "{\"cephx_secret\": \"$OSD_SECRET\"}" > $CEPH_DEV_DIR/osd$osd/new.json
             ceph_adm osd new $uuid -i $CEPH_DEV_DIR/osd$osd/new.json
 	    rm $CEPH_DEV_DIR/osd$osd/new.json
